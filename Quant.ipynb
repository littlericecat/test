{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtYoUODod0VEi0DgHU8Q9i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/littlericecat/test/blob/main/Quant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ccxt pandas\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyYuOVoJ0Pa_",
        "outputId": "06f6b63f-1591-4965-a794-cce253b89d3c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ccxt in /usr/local/lib/python3.11/dist-packages (4.4.86)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: setuptools>=60.9.0 in /usr/local/lib/python3.11/dist-packages (from ccxt) (75.2.0)\n",
            "Requirement already satisfied: certifi>=2018.1.18 in /usr/local/lib/python3.11/dist-packages (from ccxt) (2025.4.26)\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.11/dist-packages (from ccxt) (2.32.3)\n",
            "Requirement already satisfied: cryptography>=2.6.1 in /usr/local/lib/python3.11/dist-packages (from ccxt) (43.0.3)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from ccxt) (4.13.2)\n",
            "Requirement already satisfied: aiohttp>=3.10.11 in /usr/local/lib/python3.11/dist-packages (from ccxt) (3.11.15)\n",
            "Requirement already satisfied: aiodns>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from ccxt) (3.4.0)\n",
            "Requirement already satisfied: yarl>=1.7.2 in /usr/local/lib/python3.11/dist-packages (from ccxt) (1.20.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: pycares>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from aiodns>=1.1.1->ccxt) (4.8.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->ccxt) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->ccxt) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->ccxt) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->ccxt) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->ccxt) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->ccxt) (0.3.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=2.6.1->ccxt) (1.17.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.18.4->ccxt) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.18.4->ccxt) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.18.4->ccxt) (2.4.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=2.6.1->ccxt) (2.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install backtesting"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTVCjGWjgf0V",
        "outputId": "7535b2e7-6f60-4d33-c3a8-f09d02309f76"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: backtesting in /usr/local/lib/python3.11/dist-packages (0.6.4)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from backtesting) (2.0.2)\n",
            "Requirement already satisfied: pandas!=0.25.0,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from backtesting) (2.2.2)\n",
            "Requirement already satisfied: bokeh!=3.0.*,!=3.2.*,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from backtesting) (3.7.3)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.11/dist-packages (from bokeh!=3.0.*,!=3.2.*,>=1.4.0->backtesting) (3.1.6)\n",
            "Requirement already satisfied: contourpy>=1.2 in /usr/local/lib/python3.11/dist-packages (from bokeh!=3.0.*,!=3.2.*,>=1.4.0->backtesting) (1.3.2)\n",
            "Requirement already satisfied: narwhals>=1.13 in /usr/local/lib/python3.11/dist-packages (from bokeh!=3.0.*,!=3.2.*,>=1.4.0->backtesting) (1.40.0)\n",
            "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.11/dist-packages (from bokeh!=3.0.*,!=3.2.*,>=1.4.0->backtesting) (24.2)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from bokeh!=3.0.*,!=3.2.*,>=1.4.0->backtesting) (11.2.1)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.11/dist-packages (from bokeh!=3.0.*,!=3.2.*,>=1.4.0->backtesting) (6.0.2)\n",
            "Requirement already satisfied: tornado>=6.2 in /usr/local/lib/python3.11/dist-packages (from bokeh!=3.0.*,!=3.2.*,>=1.4.0->backtesting) (6.4.2)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.11/dist-packages (from bokeh!=3.0.*,!=3.2.*,>=1.4.0->backtesting) (2025.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas!=0.25.0,>=0.25.0->backtesting) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas!=0.25.0,>=0.25.0->backtesting) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas!=0.25.0,>=0.25.0->backtesting) (2025.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=2.9->bokeh!=3.0.*,!=3.2.*,>=1.4.0->backtesting) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas!=0.25.0,>=0.25.0->backtesting) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ccxt\n",
        "import pandas as pd\n",
        "import time\n",
        "import datetime\n",
        "import os\n",
        "\n",
        "# --- Configuration ---\n",
        "EXCHANGE_ID = 'binanceus'  # Example: 'binance', 'kraken', 'coinbasepro', 'bybit'\n",
        "SYMBOLS_TO_FETCH = ['BTC/USDT', 'ETH/USDT', 'SOL/USDT']\n",
        "TIMEFRAMES_TO_FETCH = ['1d', '1h', '15m'] # Daily, Hourly, 15-minute\n",
        "\n",
        "# Calculate date range (4 years from today, May 14, 2025)\n",
        "# Current date is May 14, 2025\n",
        "end_date_dt = datetime.datetime(2025, 5, 14, 0, 0, 0, tzinfo=datetime.timezone.utc)\n",
        "start_date_dt = end_date_dt - datetime.timedelta(days=4*365)\n",
        "\n",
        "# Convert to milliseconds timestamps as required by CCXT\n",
        "since_timestamp = int(start_date_dt.timestamp() * 1000)\n",
        "to_timestamp = int(end_date_dt.timestamp() * 1000)\n",
        "\n",
        "# --- MODIFICATION FOR COLAB: Set Output Directory to Google Drive ---\n",
        "# Ensure you have mounted your Google Drive in the previous step\n",
        "# Create a folder named 'CryptoDataCCXT' in your Google Drive's root if it doesn't exist\n",
        "DRIVE_OUTPUT_PATH = '/content/drive/MyDrive/CryptoDataCCXT' # Change if you want a different folder\n",
        "if not os.path.exists(DRIVE_OUTPUT_PATH):\n",
        "    os.makedirs(DRIVE_OUTPUT_PATH)\n",
        "OUTPUT_DIRECTORY = DRIVE_OUTPUT_PATH\n",
        "# If you don't want to use Google Drive (data will be temporary):\n",
        "# OUTPUT_DIRECTORY = \"crypto_ohlcv_data\"\n",
        "# if not os.path.exists(OUTPUT_DIRECTORY):\n",
        "#     os.makedirs(OUTPUT_DIRECTORY)\n",
        "\n",
        "print(f\"Data will be saved to: {OUTPUT_DIRECTORY}\")\n",
        "\n",
        "# --- Initialize Exchange ---\n",
        "exchange_options = {\n",
        "    'options': {\n",
        "        'adjustForTimeDifference': True,\n",
        "    },\n",
        "    # 'enableRateLimit': True, # Consider enabling for CCXT's own rate limiting\n",
        "}\n",
        "exchange = getattr(ccxt, EXCHANGE_ID)(exchange_options)\n",
        "\n",
        "# --- Helper Function to Fetch All OHLCV Data ---\n",
        "def fetch_all_ohlcv(symbol, timeframe, since, limit=1000):\n",
        "    all_candles = []\n",
        "    current_since = since\n",
        "    now = exchange.milliseconds() # To avoid fetching beyond current time if not needed\n",
        "                                 # Though we are using a fixed 'to_timestamp' for this 4-year historical pull\n",
        "\n",
        "    print(f\"Starting fetch for {symbol} ({timeframe}) from {exchange.iso8601(current_since)} up to {exchange.iso8601(to_timestamp)}\")\n",
        "\n",
        "    while current_since < to_timestamp: # Ensure we don't fetch beyond our defined 4-year end period\n",
        "        try:\n",
        "            # Conservative delay before each request\n",
        "            # For 15m data over 4 years, this delay is CRUCIAL.\n",
        "            # Binance limit is ~1200 weight/min. fetchOHLCV might be 1-10 weight.\n",
        "            # 0.5s to 1s is a common starting point for non-critical scripts.\n",
        "            # Increase if you hit rate limit errors (429, 418).\n",
        "            time.sleep(exchange.rateLimit / 1000 + 0.75) # Adjusted delay\n",
        "\n",
        "            candles = exchange.fetch_ohlcv(symbol, timeframe, since=current_since, limit=limit)\n",
        "\n",
        "            if candles and len(candles) > 0:\n",
        "                all_candles.extend(candles)\n",
        "                last_timestamp_fetched = candles[-1][0]\n",
        "                print(f\"Fetched {len(candles)} candles for {symbol} ({timeframe}) up to {exchange.iso8601(last_timestamp_fetched)}\")\n",
        "\n",
        "                if last_timestamp_fetched >= current_since:\n",
        "                     # Advance 'since' to the timestamp of the candle AFTER the last one received\n",
        "                    current_since = last_timestamp_fetched + (exchange.parse_timeframe(timeframe) * 1000)\n",
        "                else:\n",
        "                    print(f\"Warning: Timestamp did not advance for {symbol} ({timeframe}). Current 'since': {exchange.iso8601(current_since)}, last fetched: {exchange.iso8601(last_timestamp_fetched)}. Breaking.\")\n",
        "                    break\n",
        "\n",
        "                # Stop if the next 'since' would be past our overall desired end period\n",
        "                if current_since > to_timestamp:\n",
        "                    print(f\"Next fetch for {symbol} ({timeframe}) would start after target end date {exchange.iso8601(to_timestamp)}. Current 'since': {exchange.iso8601(current_since)}. Stopping current timeframe fetch.\")\n",
        "                    break\n",
        "            else:\n",
        "                print(f\"No more data for {symbol} ({timeframe}) after {exchange.iso8601(current_since)}. Moving to next timeframe or symbol.\")\n",
        "                break # No more data for this specific symbol/timeframe starting from current_since\n",
        "\n",
        "        except ccxt.NetworkError as e:\n",
        "            print(f\"NetworkError fetching {symbol} ({timeframe}): {e}. Retrying in 30s...\")\n",
        "            time.sleep(30)\n",
        "        except ccxt.DDoSProtection as e: # Specifically handle DDoS protection errors\n",
        "            print(f\"DDoSProtection/RateLimit fetching {symbol} ({timeframe}): {e}. Retrying in 120s...\")\n",
        "            time.sleep(120)\n",
        "        except ccxt.RateLimitExceeded as e: # Specifically handle RateLimitExceeded errors\n",
        "            print(f\"RateLimitExceeded fetching {symbol} ({timeframe}): {e}. Retrying in 120s...\")\n",
        "            time.sleep(120)\n",
        "        except ccxt.ExchangeError as e: # Other exchange errors\n",
        "            print(f\"ExchangeError fetching {symbol} ({timeframe}): {e}. Retrying in 60s...\")\n",
        "            time.sleep(60)\n",
        "        except Exception as e: # Other unexpected errors\n",
        "            print(f\"An unexpected error occurred fetching {symbol} ({timeframe}): {e}. Retrying in 60s...\")\n",
        "            time.sleep(60)\n",
        "\n",
        "    # Post-loop processing for collected candles for the current symbol/timeframe\n",
        "    if all_candles:\n",
        "        df_temp = pd.DataFrame(all_candles, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])\n",
        "        df_temp.drop_duplicates(subset=['timestamp'], keep='first', inplace=True)\n",
        "        # Filter out any data that might have been fetched beyond our strict 'to_timestamp'\n",
        "        # This can happen if the last chunk fetched candles that crossed the 'to_timestamp'\n",
        "        df_temp = df_temp[df_temp['timestamp'] < to_timestamp] # strictly less than, as 'to_timestamp' is the start of the next interval\n",
        "        all_candles = df_temp.values.tolist()\n",
        "\n",
        "    return all_candles\n",
        "\n",
        "# --- Main Loop ---\n",
        "for symbol in SYMBOLS_TO_FETCH:\n",
        "    print(f\"\\n--- Processing Symbol: {symbol} ---\")\n",
        "    for timeframe in TIMEFRAMES_TO_FETCH:\n",
        "        print(f\"  Fetching timeframe: {timeframe}\")\n",
        "\n",
        "        safe_symbol_name = symbol.replace(\"/\", \"_\")\n",
        "        filename = os.path.join(OUTPUT_DIRECTORY, f\"{safe_symbol_name}_{timeframe}_4years.csv\")\n",
        "\n",
        "        if os.path.exists(filename):\n",
        "            print(f\"  Data for {symbol} ({timeframe}) already exists: {filename}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        fetch_limit = 1000 # Default limit\n",
        "        # (You can adjust limit per timeframe/exchange if needed, but 1000 is common for Binance)\n",
        "\n",
        "        ohlcv_data = fetch_all_ohlcv(symbol, timeframe, since_timestamp, limit=fetch_limit)\n",
        "\n",
        "        if ohlcv_data:\n",
        "            df = pd.DataFrame(ohlcv_data, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])\n",
        "            df['datetime_utc'] = pd.to_datetime(df['timestamp'], unit='ms', utc=True)\n",
        "            # Ensure the datetime_utc is also before the 'to_timestamp'\n",
        "            df = df[df['timestamp'] < to_timestamp]\n",
        "            df = df[['datetime_utc', 'open', 'high', 'low', 'close', 'volume', 'timestamp']]\n",
        "\n",
        "            if not df.empty:\n",
        "                df.to_csv(filename, index=False)\n",
        "                print(f\"  Successfully saved data to {filename}\")\n",
        "                print(f\"  Fetched {len(df)} records for {symbol} ({timeframe}).\")\n",
        "                if not df.empty:\n",
        "                    print(f\"  Data ranges from {df['datetime_utc'].min()} to {df['datetime_utc'].max()}\")\n",
        "            else:\n",
        "                 print(f\"  No data within the specified date range for {symbol} ({timeframe}) after filtering. File not saved.\")\n",
        "        else:\n",
        "            print(f\"  No data fetched for {symbol} ({timeframe}).\")\n",
        "\n",
        "print(\"\\n--- All Done! ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EOk3Kr_I0pNA",
        "outputId": "83f2f569-7aa3-4ca2-8921-3548044ef6a9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data will be saved to: /content/drive/MyDrive/CryptoDataCCXT\n",
            "\n",
            "--- Processing Symbol: BTC/USDT ---\n",
            "  Fetching timeframe: 1d\n",
            "Starting fetch for BTC/USDT (1d) from 2021-05-15T00:00:00.000Z up to 2025-05-14T00:00:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (1d) up to 2024-02-08T00:00:00.000Z\n",
            "Fetched 476 candles for BTC/USDT (1d) up to 2025-05-29T00:00:00.000Z\n",
            "Next fetch for BTC/USDT (1d) would start after target end date 2025-05-14T00:00:00.000Z. Current 'since': 2025-05-30T00:00:00.000Z. Stopping current timeframe fetch.\n",
            "  Successfully saved data to /content/drive/MyDrive/CryptoDataCCXT/BTC_USDT_1d_4years.csv\n",
            "  Fetched 1460 records for BTC/USDT (1d).\n",
            "  Data ranges from 2021-05-15 00:00:00+00:00 to 2025-05-13 00:00:00+00:00\n",
            "  Fetching timeframe: 1h\n",
            "Starting fetch for BTC/USDT (1h) from 2021-05-15T00:00:00.000Z up to 2025-05-14T00:00:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (1h) up to 2021-06-25T22:00:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (1h) up to 2021-08-06T14:00:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (1h) up to 2021-09-17T06:00:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (1h) up to 2021-10-28T22:00:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (1h) up to 2021-12-09T14:00:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (1h) up to 2022-01-20T06:00:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (1h) up to 2022-03-02T22:00:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (1h) up to 2022-04-13T14:00:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (1h) up to 2022-05-25T06:00:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (1h) up to 2022-07-05T22:00:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (1h) up to 2022-08-16T14:00:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (1h) up to 2022-09-27T06:00:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (1h) up to 2022-11-07T22:00:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (1h) up to 2022-12-19T14:00:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (1h) up to 2023-01-30T06:00:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (1h) up to 2023-03-13T05:00:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (1h) up to 2023-04-23T21:00:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (1h) up to 2023-06-04T13:00:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (1h) up to 2023-07-16T05:00:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (1h) up to 2023-08-26T21:00:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (1h) up to 2023-10-07T13:00:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (1h) up to 2023-11-18T05:00:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (1h) up to 2023-12-29T21:00:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (1h) up to 2024-02-09T13:00:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (1h) up to 2024-03-22T05:00:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (1h) up to 2024-05-02T21:00:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (1h) up to 2024-06-13T13:00:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (1h) up to 2024-07-25T05:00:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (1h) up to 2024-09-04T21:00:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (1h) up to 2024-10-16T13:00:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (1h) up to 2024-11-27T05:00:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (1h) up to 2025-01-07T21:00:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (1h) up to 2025-02-18T13:00:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (1h) up to 2025-04-01T05:00:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (1h) up to 2025-05-12T21:00:00.000Z\n",
            "Fetched 392 candles for BTC/USDT (1h) up to 2025-05-29T05:00:00.000Z\n",
            "Next fetch for BTC/USDT (1h) would start after target end date 2025-05-14T00:00:00.000Z. Current 'since': 2025-05-29T06:00:00.000Z. Stopping current timeframe fetch.\n",
            "  Successfully saved data to /content/drive/MyDrive/CryptoDataCCXT/BTC_USDT_1h_4years.csv\n",
            "  Fetched 35026 records for BTC/USDT (1h).\n",
            "  Data ranges from 2021-05-15 00:00:00+00:00 to 2025-05-13 23:00:00+00:00\n",
            "  Fetching timeframe: 15m\n",
            "Starting fetch for BTC/USDT (15m) from 2021-05-15T00:00:00.000Z up to 2025-05-14T00:00:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2021-05-25T09:45:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2021-06-04T19:45:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2021-06-15T05:45:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2021-06-25T23:30:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2021-07-06T09:30:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2021-07-16T19:30:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2021-07-27T05:30:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2021-08-06T15:30:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2021-08-17T01:30:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2021-08-27T11:30:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2021-09-06T21:30:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2021-09-17T07:30:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2021-09-27T17:30:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2021-10-08T03:30:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2021-10-18T13:30:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2021-10-28T23:30:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2021-11-08T09:30:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2021-11-18T19:30:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2021-11-29T05:30:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2021-12-09T15:30:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2021-12-20T01:30:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2021-12-30T11:30:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2022-01-09T21:30:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2022-01-20T07:30:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2022-01-30T17:30:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2022-02-10T03:30:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2022-02-20T13:30:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2022-03-02T23:45:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2022-03-13T09:45:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2022-03-23T19:45:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2022-04-03T05:45:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2022-04-13T15:45:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2022-04-24T01:45:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2022-05-04T11:45:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2022-05-14T21:45:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2022-05-25T07:45:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2022-06-04T17:45:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2022-06-15T03:45:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2022-06-25T13:45:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2022-07-05T23:45:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2022-07-16T09:45:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2022-07-26T19:45:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2022-08-06T05:45:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2022-08-16T15:45:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2022-08-27T01:45:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2022-09-06T11:45:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2022-09-16T21:45:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2022-09-27T07:45:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2022-10-07T17:45:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2022-10-18T03:45:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2022-10-28T13:45:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2022-11-07T23:45:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2022-11-18T09:45:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2022-11-28T19:45:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2022-12-09T05:45:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2022-12-19T15:45:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2022-12-30T01:45:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2023-01-09T11:45:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2023-01-19T21:45:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2023-01-30T07:45:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2023-02-10T00:45:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2023-02-20T10:45:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2023-03-02T20:45:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2023-03-13T06:45:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2023-03-23T16:45:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2023-04-03T02:45:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2023-04-13T12:45:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2023-04-23T22:45:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2023-05-04T08:45:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2023-05-14T18:45:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2023-05-25T04:45:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2023-06-04T14:45:00.000Z\n",
            "Fetched 1000 candles for BTC/USDT (15m) up to 2023-06-15T00:45:00.000Z\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-5b59aad226d9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# (You can adjust limit per timeframe/exchange if needed, but 1000 is common for Binance)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0mohlcv_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_all_ohlcv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msince_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfetch_limit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mohlcv_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-5b59aad226d9>\u001b[0m in \u001b[0;36mfetch_all_ohlcv\u001b[0;34m(symbol, timeframe, since, limit)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;31m# 0.5s to 1s is a common starting point for non-critical scripts.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;31m# Increase if you hit rate limit errors (429, 418).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexchange\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrateLimit\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Adjusted delay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mcandles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexchange\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch_ohlcv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msince\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurrent_since\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# --- Configuration ---\n",
        "\n",
        "# Option 1: Where are the files you want to check?\n",
        "# \"colab_session\" - for files in the current Colab session's temporary storage (likely in a subfolder)\n",
        "# \"google_drive\" - for files already saved to your Google Drive\n",
        "FILE_LOCATION_TYPE = \"google_drive\" # Or \"colab_session\"\n",
        "\n",
        "# Define the symbols and timeframes from your CCXT script\n",
        "SYMBOLS = ['BTC_USDT', 'ETH_USDT', 'SOL_USDT']\n",
        "TIMEFRAMES = ['1d', '1h', '15m'] # CCXT format\n",
        "DURATION_TAG = \"4years\" # From your CCXT script's naming\n",
        "\n",
        "# Generate the list of expected CSV file names\n",
        "EXPECTED_CSV_FILES = []\n",
        "for symbol_base in SYMBOLS:\n",
        "    for tf_ccxt in TIMEFRAMES:\n",
        "        # Convert CCXT timeframe to a more descriptive tag if needed, or use as is\n",
        "        # For simplicity, using CCXT tf_ccxt directly in filename part\n",
        "        filename = f\"{symbol_base}_{tf_ccxt}_{DURATION_TAG}.csv\"\n",
        "        EXPECTED_CSV_FILES.append(filename)\n",
        "\n",
        "print(\"Expected files to check:\")\n",
        "for f_name in EXPECTED_CSV_FILES:\n",
        "    print(f\"- {f_name}\")\n",
        "\n",
        "\n",
        "# If checking from Google Drive, specify the base path (should match your CCXT script's output)\n",
        "GDRIVE_BASE_PATH = \"/content/drive/MyDrive/CryptoDataCCXT/\" # Adjust if you used a different folder\n",
        "\n",
        "# If checking from Colab session, specify the subfolder used by the CCXT script\n",
        "DEFAULT_SESSION_SUBFOLDER = \"crypto_ohlcv_data\"\n",
        "\n",
        "# --- Mount Google Drive if needed ---\n",
        "if FILE_LOCATION_TYPE == \"google_drive\":\n",
        "    from google.colab import drive\n",
        "    try:\n",
        "        drive.mount('/content/drive', force_remount=False) # force_remount=False is gentler\n",
        "        print(f\"Google Drive mounted. Checking files in: {GDRIVE_BASE_PATH}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not mount Google Drive: {e}. Cannot check files from Drive.\")\n",
        "        # Clear the list to prevent errors trying to access GDrive paths if mount fails\n",
        "        EXPECTED_CSV_FILES = []\n",
        "\n",
        "# --- Loop Through Files and Check Format ---\n",
        "print(\"\\n--- Starting Format Check for All Specified CSV Files ---\")\n",
        "\n",
        "all_files_summary = {} # To store summary status\n",
        "\n",
        "for file_name_to_check in EXPECTED_CSV_FILES:\n",
        "    path_to_file = None\n",
        "    if FILE_LOCATION_TYPE == \"google_drive\":\n",
        "        path_to_file = os.path.join(GDRIVE_BASE_PATH, file_name_to_check)\n",
        "    else: # colab_session\n",
        "        # Assumes file is in the specified subfolder or /content/subfolder\n",
        "        session_file_path = os.path.join(DEFAULT_SESSION_SUBFOLDER, file_name_to_check)\n",
        "        if os.path.exists(session_file_path):\n",
        "             path_to_file = session_file_path\n",
        "        elif os.path.exists(f\"/content/{session_file_path}\"): # Check relative to /content/\n",
        "             path_to_file = f\"/content/{session_file_path}\"\n",
        "        elif os.path.exists(file_name_to_check): # Check current working directory (less likely for Colab)\n",
        "            path_to_file = file_name_to_check\n",
        "\n",
        "    print(f\"\\n\\n======================================================================\")\n",
        "    print(f\"--- Checking Format for: {file_name_to_check} ---\")\n",
        "    print(f\"Attempting to locate at: {path_to_file if path_to_file else 'N/A (Path could not be determined for session check)'}\")\n",
        "    print(f\"======================================================================\")\n",
        "\n",
        "    file_status = \"ERROR: File not found or path undetermined.\"\n",
        "\n",
        "    if path_to_file and os.path.exists(path_to_file):\n",
        "        try:\n",
        "            # Load the CSV. The CCXT script saved 'datetime_utc' as a string.\n",
        "            # 'timestamp' (ms) was also saved.\n",
        "            # We'll use 'datetime_utc' as the index.\n",
        "            df = pd.read_csv(path_to_file, index_col='datetime_utc', parse_dates=True)\n",
        "            file_status = \"OK\" # Initial status\n",
        "\n",
        "            if df.empty:\n",
        "                print(\"WARNING: The file is empty.\")\n",
        "                file_status = \"WARNING: Empty file\"\n",
        "            else:\n",
        "                print(\"\\n1. First 5 rows:\")\n",
        "                print(df.head())\n",
        "\n",
        "                print(\"\\n2. Data types and non-null values:\")\n",
        "                df.info()\n",
        "\n",
        "                print(\"\\n3. Basic statistics for numerical columns:\")\n",
        "                print(df.describe())\n",
        "\n",
        "                print(\"\\n4. Index details:\")\n",
        "                print(f\"   Index name: {df.index.name}\")\n",
        "                print(f\"   Index type: {type(df.index)}\")\n",
        "                print(f\"   Index data type: {df.index.dtype}\") # Should be datetime64[ns, UTC]\n",
        "                if not df.empty:\n",
        "                    is_monotonic = df.index.is_monotonic_increasing or df.index.is_monotonic_decreasing # allow for either strictly\n",
        "                    print(f\"   Is index monotonic? {is_monotonic}\")\n",
        "                    if not is_monotonic:\n",
        "                        file_status = \"WARNING: Index not monotonic\"\n",
        "                    print(f\"   Date range: {df.index.min()} to {df.index.max()}\")\n",
        "                print(f\"   Number of rows: {len(df)}\")\n",
        "\n",
        "                # Basic Sanity Checks\n",
        "                print(\"\\n5. Column and Value Sanity Checks:\")\n",
        "                required_cols = ['open', 'high', 'low', 'close', 'volume']\n",
        "                missing_cols = [col for col in required_cols if col not in df.columns]\n",
        "                if missing_cols:\n",
        "                    print(f\"   WARNING: Missing expected OHLCV columns (lowercase): {', '.join(missing_cols)}\")\n",
        "                    file_status = f\"WARNING: Missing columns ({', '.join(missing_cols)})\"\n",
        "\n",
        "                for col in ['open', 'high', 'low', 'close', 'volume']:\n",
        "                    if col in df.columns and (df[col] < 0).any():\n",
        "                        print(f\"   WARNING: Negative values found in '{col}' column.\")\n",
        "                        file_status = f\"WARNING: Negative values in {col}\"\n",
        "\n",
        "                # OHLC integrity checks\n",
        "                if 'open' in df.columns and 'high' in df.columns and \\\n",
        "                   'low' in df.columns and 'close' in df.columns:\n",
        "                    if not (df['high'] >= df['low']).all():\n",
        "                        print(\"   WARNING: Not all 'high' values are >= 'low' values.\")\n",
        "                        file_status = \"WARNING: High < Low inconsistency\"\n",
        "                    if not (df['high'] >= df['open']).all():\n",
        "                        print(\"   WARNING: Not all 'high' values are >= 'open' values.\")\n",
        "                        file_status = \"WARNING: High < Open inconsistency\"\n",
        "                    if not (df['high'] >= df['close']).all():\n",
        "                        print(\"   WARNING: Not all 'high' values are >= 'close' values.\")\n",
        "                        file_status = \"WARNING: High < Close inconsistency\"\n",
        "                    if not (df['low'] <= df['open']).all():\n",
        "                        print(\"   WARNING: Not all 'low' values are <= 'open' values.\")\n",
        "                        file_status = \"WARNING: Low > Open inconsistency\"\n",
        "                    if not (df['low'] <= df['close']).all():\n",
        "                        print(\"   WARNING: Not all 'low' values are <= 'close' values.\")\n",
        "                        file_status = \"WARNING: Low > Close inconsistency\"\n",
        "                else:\n",
        "                    print(\"   Skipping OHLC integrity checks due to missing columns.\")\n",
        "\n",
        "        except pd.errors.EmptyDataError:\n",
        "            error_message = f\"ERROR: The file '{path_to_file}' is empty and cannot be parsed by Pandas.\"\n",
        "            print(error_message)\n",
        "            file_status = error_message\n",
        "        except Exception as e:\n",
        "            error_message = f\"ERROR: An error occurred while reading or processing '{path_to_file}': {e}\"\n",
        "            print(error_message)\n",
        "            file_status = error_message\n",
        "    else:\n",
        "        print(f\"ERROR: File not found at the determined path: '{path_to_file if path_to_file else file_name_to_check}'. Skipping.\")\n",
        "        # file_status remains \"ERROR: File not found...\"\n",
        "\n",
        "    all_files_summary[file_name_to_check] = file_status\n",
        "\n",
        "print(\"\\n\\n--- Overall Summary of File Checks ---\")\n",
        "for file_name, status in all_files_summary.items():\n",
        "    print(f\"- {file_name}: {status}\")\n",
        "print(\"\\n--- Format Check for All Files Complete ---\")"
      ],
      "metadata": {
        "id": "jdvmseKSe394"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# --- Master Configuration for Indicator Parameters (Coin-Specific) ---\n",
        "# ==============================================================================\n",
        "# Define parameters for each coin. You can have different settings for BTC, ETH, SOL.\n",
        "# If a coin is not listed here, or an indicator for a coin is not listed,\n",
        "# you could fall back to a 'default' configuration or skip it.\n",
        "\n",
        "master_indicator_configs = {\n",
        "    \"BTC\": {\n",
        "        \"sma_short\":    {\"window\": 10, \"column\": \"Close\"},\n",
        "        \"sma_long\":     {\"window\": 50, \"column\": \"Close\"},\n",
        "        \"ema_short\":    {\"span\": 12, \"column\": \"Close\"},\n",
        "        \"ema_long\":     {\"span\": 26, \"column\": \"Close\"},\n",
        "        \"rsi\":          {\"window\": 14, \"column\": \"Close\"},\n",
        "        \"bollinger\":    {\"window\": 20, \"num_std_dev\": 2, \"column\": \"Close\"},\n",
        "        \"macd\":         {\"short_window\": 12, \"long_window\": 26, \"signal_window\": 9, \"column\": \"Close\"},\n",
        "        \"atr\":          {\"window\": 14},\n",
        "        \"stochastic\":   {\"k_window\": 14, \"d_window\": 3},\n",
        "        \"mfi\":          {\"window\": 14},\n",
        "        \"roc\":          {\"window\": 10, \"column\": \"Close\"},\n",
        "        \"keltner\":      {\"ema_window\": 20, \"atr_window\": 10, \"atr_multiplier\": 2},\n",
        "        \"volume_sma\":   {\"window\": 20},\n",
        "        \"support_resistance\": {\"window\": 15}\n",
        "    },\n",
        "    \"ETH\": {\n",
        "        \"sma_short\":    {\"window\": 12, \"column\": \"Close\"}, # Slightly different SMA for ETH\n",
        "        \"sma_long\":     {\"window\": 55, \"column\": \"Close\"},\n",
        "        \"ema_short\":    {\"span\": 10, \"column\": \"Close\"},\n",
        "        \"ema_long\":     {\"span\": 30, \"column\": \"Close\"},\n",
        "        \"rsi\":          {\"window\": 14, \"column\": \"Close\"}, # Same RSI\n",
        "        \"bollinger\":    {\"window\": 20, \"num_std_dev\": 2.1, \"column\": \"Close\"}, # Slightly wider BB\n",
        "        \"macd\":         {\"short_window\": 12, \"long_window\": 26, \"signal_window\": 9, \"column\": \"Close\"},\n",
        "        \"atr\":          {\"window\": 10},\n",
        "        \"stochastic\":   {\"k_window\": 10, \"d_window\": 3},\n",
        "        \"mfi\":          {\"window\": 12},\n",
        "        \"roc\":          {\"window\": 12, \"column\": \"Close\"},\n",
        "        \"keltner\":      {\"ema_window\": 22, \"atr_window\": 11, \"atr_multiplier\": 2},\n",
        "        \"volume_sma\":   {\"window\": 25},\n",
        "        \"support_resistance\": {\"window\": 20}\n",
        "    },\n",
        "    \"SOL\": {\n",
        "        # Using mostly similar params to BTC for SOL as a starting point\n",
        "        # You can fine-tune these specifically for SOL\n",
        "        \"sma_short\":    {\"window\": 9, \"column\": \"Close\"},\n",
        "        \"sma_long\":     {\"window\": 45, \"column\": \"Close\"},\n",
        "        \"ema_short\":    {\"span\": 12, \"column\": \"Close\"},\n",
        "        \"ema_long\":     {\"span\": 26, \"column\": \"Close\"},\n",
        "        \"rsi\":          {\"window\": 14, \"column\": \"Close\"},\n",
        "        \"bollinger\":    {\"window\": 20, \"num_std_dev\": 2, \"column\": \"Close\"},\n",
        "        \"macd\":         {\"short_window\": 12, \"long_window\": 26, \"signal_window\": 9, \"column\": \"Close\"},\n",
        "        \"atr\":          {\"window\": 14},\n",
        "        \"stochastic\":   {\"k_window\": 14, \"d_window\": 3},\n",
        "        \"mfi\":          {\"window\": 14},\n",
        "        \"roc\":          {\"window\": 10, \"column\": \"Close\"},\n",
        "        \"keltner\":      {\"ema_window\": 20, \"atr_window\": 10, \"atr_multiplier\": 1.8}, # Tighter Keltner for SOL\n",
        "        \"volume_sma\":   {\"window\": 20},\n",
        "        \"support_resistance\": {\"window\": 15}\n",
        "    }\n",
        "    # You can add a \"DEFAULT\" key here if you want to process coins not explicitly listed\n",
        "}\n"
      ],
      "metadata": {
        "id": "bkRDbn_jh_RW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://drive.google.com/drive/u/0/search?q=parent:1sbKi08-dsL5C4FRpHgmM_RQbx9ldG0Ie"
      ],
      "metadata": {
        "id": "O99rOlGc-_DX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# --- Technical Indicator Functions (Paste the functions from previous response here) ---\n",
        "# e.g., add_sma, add_ema, add_rsi, add_bollinger_bands, add_macd,\n",
        "#       add_atr, add_stochastic_oscillator, add_mfi, add_roc,\n",
        "#       add_keltner_channels, add_volume_sma, add_basic_support_resistance\n",
        "# ==============================================================================\n",
        "# --- (Assuming all indicator functions like add_sma, add_ema, etc., are defined above this line) ---\n",
        "# --- (For brevity, I'm not re-pasting all of them here, but you should have them in your script) ---\n",
        "\n",
        "# --- PASTE ALL YOUR INDICATOR CALCULATION FUNCTIONS (add_sma, add_ema, ...) HERE ---\n",
        "# For example:\n",
        "def add_sma(df, window=20, column='Close', new_column_name=None):\n",
        "    if new_column_name is None: new_column_name = f'SMA_{window}'\n",
        "    df[new_column_name] = df[column].rolling(window=window, min_periods=1).mean()\n",
        "    return df\n",
        "\n",
        "def add_ema(df, span=20, column='Close', new_column_name=None):\n",
        "    if new_column_name is None: new_column_name = f'EMA_{span}'\n",
        "    df[new_column_name] = df[column].ewm(span=span, adjust=False, min_periods=1).mean()\n",
        "    return df\n",
        "\n",
        "def add_rsi(df, window=14, column='Close', new_column_name=None):\n",
        "    if new_column_name is None: new_column_name = f'RSI_{window}'\n",
        "    delta = df[column].diff(1)\n",
        "    gain = delta.where(delta > 0, 0).fillna(0) # fillna for first diff\n",
        "    loss = -delta.where(delta < 0, 0).fillna(0) # fillna for first diff\n",
        "    avg_gain = gain.ewm(alpha=1/window, adjust=False, min_periods=1).mean() # Use ewm for standard RSI\n",
        "    avg_loss = loss.ewm(alpha=1/window, adjust=False, min_periods=1).mean() # Use ewm for standard RSI\n",
        "    rs = avg_gain / avg_loss\n",
        "    rs = rs.replace([np.inf, -np.inf], np.nan).fillna(0) # Handle division by zero or NaNs if avg_loss is 0\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "    # If avg_loss is consistently 0 (all gains), RSI should be 100. If avg_gain is 0 (all losses), RSI is 0.\n",
        "    rsi = np.where(avg_loss == 0, 100, rsi)\n",
        "    rsi = np.where(avg_gain == 0, 0, rsi)\n",
        "    df[new_column_name] = rsi\n",
        "    return df\n",
        "\n",
        "def add_bollinger_bands(df, window=20, num_std_dev=2, column='Close', sma_col_name=None, upper_band_col_name=None, lower_band_col_name=None):\n",
        "    if sma_col_name is None: sma_col_name = f'BB_SMA_{window}'\n",
        "    if upper_band_col_name is None: upper_band_col_name = f'BB_Upper_{window}'\n",
        "    if lower_band_col_name is None: lower_band_col_name = f'BB_Lower_{window}'\n",
        "    df[sma_col_name] = df[column].rolling(window=window, min_periods=1).mean()\n",
        "    std_dev = df[column].rolling(window=window, min_periods=1).std()\n",
        "    df[upper_band_col_name] = df[sma_col_name] + (std_dev * num_std_dev)\n",
        "    df[lower_band_col_name] = df[sma_col_name] - (std_dev * num_std_dev)\n",
        "    return df\n",
        "\n",
        "def add_macd(df, short_window=12, long_window=26, signal_window=9, column='Close', macd_line_col_name=None, signal_line_col_name=None, macd_hist_col_name=None):\n",
        "    if macd_line_col_name is None: macd_line_col_name = f'MACD_{short_window}_{long_window}'\n",
        "    if signal_line_col_name is None: signal_line_col_name = f'MACD_Signal_{signal_window}'\n",
        "    if macd_hist_col_name is None: macd_hist_col_name = f'MACD_Hist_{signal_window}'\n",
        "    ema_short = df[column].ewm(span=short_window, adjust=False, min_periods=1).mean()\n",
        "    ema_long = df[column].ewm(span=long_window, adjust=False, min_periods=1).mean()\n",
        "    df[macd_line_col_name] = ema_short - ema_long\n",
        "    df[signal_line_col_name] = df[macd_line_col_name].ewm(span=signal_window, adjust=False, min_periods=1).mean()\n",
        "    df[macd_hist_col_name] = df[macd_line_col_name] - df[signal_line_col_name]\n",
        "    return df\n",
        "\n",
        "def add_atr(df, window=14, high_col='High', low_col='Low', close_col='Close', atr_col_name=None):\n",
        "    if atr_col_name is None: atr_col_name = f'ATR_{window}'\n",
        "    df['_prev_close'] = df[close_col].shift(1)\n",
        "    df['_h_minus_l'] = df[high_col] - df[low_col]\n",
        "    df['_h_minus_pc'] = abs(df[high_col] - df['_prev_close'])\n",
        "    df['_l_minus_pc'] = abs(df[low_col] - df['_prev_close'])\n",
        "    df['_true_range'] = df[['_h_minus_l', '_h_minus_pc', '_l_minus_pc']].max(axis=1)\n",
        "    df[atr_col_name] = df['_true_range'].ewm(alpha=1/window, adjust=False, min_periods=1).mean()\n",
        "    df.drop(columns=['_prev_close', '_h_minus_l', '_h_minus_pc', '_l_minus_pc', '_true_range'], inplace=True)\n",
        "    return df\n",
        "\n",
        "def add_stochastic_oscillator(df, k_window=14, d_window=3,\n",
        "                              high_col='High', low_col='Low', close_col='Close',\n",
        "                              stoch_k_col_name=None, stoch_d_col_name=None):\n",
        "    \"\"\"Adds Stochastic Oscillator (%K and %D) to the DataFrame.\"\"\"\n",
        "    if stoch_k_col_name is None: stoch_k_col_name = f'Stoch_%K_{k_window}'\n",
        "    if stoch_d_col_name is None: stoch_d_col_name = f'Stoch_%D_{d_window}'\n",
        "\n",
        "    lowest_low = df[low_col].rolling(window=k_window, min_periods=1).min()\n",
        "    highest_high = df[high_col].rolling(window=k_window, min_periods=1).max()\n",
        "\n",
        "    # Avoid division by zero if highest_high == lowest_low by adding a small epsilon\n",
        "    denominator = highest_high - lowest_low\n",
        "    df[stoch_k_col_name] = ((df[close_col] - lowest_low) / (denominator + 1e-9)) * 100\n",
        "\n",
        "    # Handle potential NaN/inf values from division and clip to 0-100 range\n",
        "    df[stoch_k_col_name] = df[stoch_k_col_name].replace([np.inf, -np.inf], np.nan)\n",
        "    # --- UPDATED LINE TO ADDRESS FutureWarning ---\n",
        "    df[stoch_k_col_name] = df[stoch_k_col_name].ffill().bfill() # Use .ffill() and .bfill()\n",
        "    # --- END OF UPDATED LINE ---\n",
        "    df[stoch_k_col_name] = df[stoch_k_col_name].clip(0, 100) # Ensure values are strictly within 0-100\n",
        "\n",
        "    df[stoch_d_col_name] = df[stoch_k_col_name].rolling(window=d_window, min_periods=1).mean()\n",
        "    return df\n",
        "\n",
        "def add_mfi(df, window=14, high_col='High', low_col='Low', close_col='Close', volume_col='Volume', mfi_col_name=None):\n",
        "    if mfi_col_name is None: mfi_col_name = f'MFI_{window}'\n",
        "    df['_typical_price'] = (df[high_col] + df[low_col] + df[close_col]) / 3\n",
        "    df['_raw_money_flow'] = df['_typical_price'] * df[volume_col]\n",
        "    price_diff = df['_typical_price'].diff(1)\n",
        "    df['_positive_money_flow'] = df['_raw_money_flow'].where(price_diff > 0, 0).fillna(0)\n",
        "    df['_negative_money_flow'] = df['_raw_money_flow'].where(price_diff < 0, 0).fillna(0)\n",
        "    positive_mf_sum = df['_positive_money_flow'].rolling(window=window, min_periods=1).sum()\n",
        "    negative_mf_sum = df['_negative_money_flow'].rolling(window=window, min_periods=1).sum()\n",
        "    money_flow_ratio = positive_mf_sum / (negative_mf_sum + 1e-9) # Add small epsilon to avoid division by zero\n",
        "    mfi = 100 - (100 / (1 + money_flow_ratio))\n",
        "    mfi = mfi.replace([np.inf, -np.inf], 100) # If negative_mf_sum was essentially 0\n",
        "    df[mfi_col_name] = mfi\n",
        "    df.drop(columns=['_typical_price', '_raw_money_flow', '_positive_money_flow', '_negative_money_flow'], inplace=True)\n",
        "    return df\n",
        "\n",
        "def add_roc(df, window=12, column='Close', new_column_name=None):\n",
        "    if new_column_name is None: new_column_name = f'ROC_{window}'\n",
        "    df[new_column_name] = df[column].pct_change(periods=window) * 100\n",
        "    return df\n",
        "\n",
        "def add_keltner_channels(df, ema_window=20, atr_window=10, atr_multiplier=2, high_col='High', low_col='Low', close_col='Close', middle_col_name=None, upper_col_name=None, lower_col_name=None):\n",
        "    if middle_col_name is None: middle_col_name = f'KC_Middle_{ema_window}'\n",
        "    if upper_col_name is None: upper_col_name = f'KC_Upper_{ema_window}'\n",
        "    if lower_col_name is None: lower_col_name = f'KC_Lower_{ema_window}'\n",
        "    df[middle_col_name] = df[close_col].ewm(span=ema_window, adjust=False, min_periods=1).mean()\n",
        "    temp_atr_col = f'_temp_atr_for_kc_{atr_window}'\n",
        "    df_with_atr = add_atr(df.copy(), window=atr_window, high_col=high_col, low_col=low_col, close_col=close_col, atr_col_name=temp_atr_col) # Use a copy to avoid modifying df inplace during ATR calc\n",
        "    df[upper_col_name] = df[middle_col_name] + (df_with_atr[temp_atr_col] * atr_multiplier)\n",
        "    df[lower_col_name] = df[middle_col_name] - (df_with_atr[temp_atr_col] * atr_multiplier)\n",
        "    return df\n",
        "\n",
        "def add_volume_sma(df, window=20, volume_col='Volume', new_column_name=None):\n",
        "    if new_column_name is None: new_column_name = f'Volume_SMA_{window}'\n",
        "    df[new_column_name] = df[volume_col].rolling(window=window, min_periods=1).mean()\n",
        "    return df\n",
        "\n",
        "def add_basic_support_resistance(df, window=20, high_col='High', low_col='Low', support_col_name=None, resistance_col_name=None):\n",
        "    if support_col_name is None: support_col_name = f'Support_{window}'\n",
        "    if resistance_col_name is None: resistance_col_name = f'Resistance_{window}'\n",
        "    df[resistance_col_name] = df[high_col].rolling(window=window, min_periods=1).max().shift(1)\n",
        "    df[support_col_name] = df[low_col].rolling(window=window, min_periods=1).min().shift(1)\n",
        "    # print(\"Note: Basic Support/Resistance added using rolling min/max of prior periods.\")\n",
        "    return df\n",
        "\n",
        "# END OF INDICATOR FUNCTIONS - PASTE THEM ABOVE THIS LINE"
      ],
      "metadata": {
        "id": "ptSCoKDUiTxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from datetime import datetime, timedelta # If you're using these elsewhere\n",
        "# from google.colab import drive # If you're using drive"
      ],
      "metadata": {
        "id": "ODaTHBlrlNXM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd # Ensure pandas is imported\n",
        "import os # Ensure os is imported\n",
        "import numpy as np # For np.nan\n",
        "\n",
        "# This script assumes:\n",
        "# 1. You have run the Google Drive mount cell.\n",
        "# 2. Your indicator functions (add_sma, add_rsi, etc.) are defined above this block.\n",
        "# 3. Your master_indicator_configs dictionary is defined above this block.\n",
        "\n",
        "# --- Configuration for File Paths on Google Drive ---\n",
        "# Define the pairs, timeframes, and duration from your CCXT script's output\n",
        "TARGET_PAIRS = [\"BTC_USDT\", \"ETH_USDT\", \"SOL_USDT\"] # e.g., BTC_USDT, ETH_USDT\n",
        "TIMEFRAMES_TO_PROCESS = [\"1d\", \"1h\", \"15m\"] # CCXT timeframe codes, e.g., \"1d\", \"1h\", \"15m\"\n",
        "DURATION_TAG = \"4years\" # From your CCXT script's naming\n",
        "\n",
        "GDRIVE_BASE_DATA_PATH = \"/content/drive/MyDrive/CryptoDataCCXT/\" # Source of CCXT data\n",
        "GDRIVE_OUTPUT_PATH = \"/content/drive/MyDrive/CryptoDataCCXT/\" # Output for this script\n",
        "\n",
        "if not os.path.exists(GDRIVE_OUTPUT_PATH):\n",
        "    os.makedirs(GDRIVE_OUTPUT_PATH)\n",
        "    print(f\"Created output directory in Google Drive: {GDRIVE_OUTPUT_PATH}\")\n",
        "else:\n",
        "    print(f\"Output directory in Google Drive already exists or is accessible: {GDRIVE_OUTPUT_PATH}\")\n",
        "\n",
        "for pair_symbol in TARGET_PAIRS:\n",
        "    # Extract base coin symbol for looking up in master_indicator_configs (e.g., \"BTC\" from \"BTC_USDT\")\n",
        "    base_coin_symbol = pair_symbol.split('_')[0] # Assumes format like COIN_QUOTE (e.g., BTC_USDT)\n",
        "                                               # Adjust if your pair_symbol format is different (e.g. BTCUSD)\n",
        "\n",
        "    for timeframe_ccxt in TIMEFRAMES_TO_PROCESS:\n",
        "        print(f\"\\n\\nProcessing data for: {pair_symbol} ({timeframe_ccxt})\")\n",
        "        print(\"==================================================\")\n",
        "\n",
        "        data_file_name = f\"{pair_symbol}_{timeframe_ccxt}_{DURATION_TAG}.csv\"\n",
        "        full_data_path = os.path.join(GDRIVE_BASE_DATA_PATH, data_file_name)\n",
        "\n",
        "        try:\n",
        "            # The CCXT script saved 'datetime_utc' as the human-readable timestamp column\n",
        "            df_original = pd.read_csv(full_data_path, index_col='datetime_utc', parse_dates=True)\n",
        "            print(f\"Successfully loaded data for {pair_symbol} ({timeframe_ccxt}) from: {full_data_path}\")\n",
        "\n",
        "            print(f\"DEBUG: Original columns found: {df_original.columns.tolist()}\")\n",
        "\n",
        "            df = df_original.copy()\n",
        "\n",
        "            # --- FIX 1: Rename columns to capitalized versions for indicator functions ---\n",
        "            # CCXT output columns are: 'open', 'high', 'low', 'close', 'volume', 'timestamp' (ms)\n",
        "            rename_map = {\n",
        "                'open': 'Open',\n",
        "                'high': 'High',\n",
        "                'low': 'Low',\n",
        "                'close': 'Close',\n",
        "                'volume': 'Volume' # CCXT data should have 'volume'\n",
        "            }\n",
        "            # Only rename columns that actually exist in the DataFrame with lowercase names\n",
        "            columns_to_rename_now = {k: v for k, v in rename_map.items() if k in df.columns}\n",
        "            if columns_to_rename_now:\n",
        "                df.rename(columns=columns_to_rename_now, inplace=True)\n",
        "                print(f\"DEBUG: Columns after renaming: {df.columns.tolist()}\")\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"ERROR: Data file not found for {pair_symbol} ({timeframe_ccxt}) at '{full_data_path}'. Skipping.\")\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: Could not load data for {pair_symbol} ({timeframe_ccxt}) from '{full_data_path}': {e}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # --- FIX 2: Define required_cols (after renaming) ---\n",
        "        # For CCXT data, 'Volume' should be present after renaming.\n",
        "        required_cols_for_this_file = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
        "\n",
        "        missing_required_cols = [col for col in required_cols_for_this_file if col not in df.columns]\n",
        "        if missing_required_cols:\n",
        "            print(f\"ERROR: DataFrame for {pair_symbol} ({timeframe_ccxt}) is missing one or more required OHLCV columns: {missing_required_cols}. Actual columns: {df.columns.tolist()}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # --- Get Coin-Specific Indicator Configuration ---\n",
        "        # Using base_coin_symbol (e.g., \"BTC\") for lookup\n",
        "        if base_coin_symbol in master_indicator_configs:\n",
        "            current_config = master_indicator_configs[base_coin_symbol]\n",
        "            print(f\"Using specific indicator configurations for {base_coin_symbol} (from {pair_symbol}).\")\n",
        "        else:\n",
        "            print(f\"WARNING: No specific indicator configuration found for {base_coin_symbol} (from {pair_symbol}) in 'master_indicator_configs'. Skipping indicators for this file.\")\n",
        "            # Decide if you want to save the file without indicators or skip saving\n",
        "            # For now, let's skip applying indicators and thus not save a new file\n",
        "            continue\n",
        "\n",
        "        print(f\"\\nApplying technical indicators for {pair_symbol} ({timeframe_ccxt})...\")\n",
        "\n",
        "        # --- Apply Indicators (with checks for 'Volume' where needed) ---\n",
        "        # Assumes indicator functions like add_sma, add_rsi, add_mfi are defined elsewhere\n",
        "        # and expect uppercase column names ('Open', 'Close', 'Volume', etc.)\n",
        "\n",
        "        # --- Moving Averages (don't need Volume) ---\n",
        "        if \"sma_short\" in current_config:\n",
        "            params = current_config[\"sma_short\"]\n",
        "            df = add_sma(df, **params, new_column_name=f'{base_coin_symbol}_SMA_{params.get(\"window\", \"short\")}')\n",
        "        if \"sma_long\" in current_config:\n",
        "            params = current_config[\"sma_long\"]\n",
        "            df = add_sma(df, **params, new_column_name=f'{base_coin_symbol}_SMA_{params.get(\"window\", \"long\")}')\n",
        "        if \"ema_short\" in current_config:\n",
        "            params = current_config[\"ema_short\"]\n",
        "            df = add_ema(df, **params, new_column_name=f'{base_coin_symbol}_EMA_{params.get(\"span\", \"short\")}')\n",
        "        if \"ema_long\" in current_config:\n",
        "            params = current_config[\"ema_long\"]\n",
        "            df = add_ema(df, **params, new_column_name=f'{base_coin_symbol}_EMA_{params.get(\"span\", \"long\")}')\n",
        "\n",
        "        # --- Other Non-Volume Dependent Indicators ---\n",
        "        if \"rsi\" in current_config: df = add_rsi(df, **current_config[\"rsi\"])\n",
        "        if \"bollinger\" in current_config: df = add_bollinger_bands(df, **current_config[\"bollinger\"])\n",
        "        if \"macd\" in current_config: df = add_macd(df, **current_config[\"macd\"])\n",
        "        if \"atr\" in current_config: df = add_atr(df, **current_config[\"atr\"])\n",
        "        if \"stochastic\" in current_config: df = add_stochastic_oscillator(df, **current_config[\"stochastic\"])\n",
        "        if \"roc\" in current_config: df = add_roc(df, **current_config[\"roc\"])\n",
        "        if \"keltner\" in current_config: df = add_keltner_channels(df, **current_config[\"keltner\"])\n",
        "        if \"support_resistance\" in current_config: df = add_basic_support_resistance(df, **current_config[\"support_resistance\"])\n",
        "\n",
        "        # --- Indicators REQUIRING Volume (MFI, Volume SMA) ---\n",
        "        if 'Volume' in df.columns: # This check is robust\n",
        "            print(f\"DEBUG: 'Volume' column available for {pair_symbol}. Proceeding with volume-based indicators.\")\n",
        "            if \"mfi\" in current_config:\n",
        "                df = add_mfi(df, **current_config[\"mfi\"])\n",
        "            if \"volume_sma\" in current_config:\n",
        "                df = add_volume_sma(df, **current_config[\"volume_sma\"])\n",
        "        else:\n",
        "            # This block might be less likely to hit if using CCXT data which includes volume\n",
        "            print(f\"WARNING: 'Volume' column NOT found for {pair_symbol} even after renaming. Skipping MFI and Volume SMA.\")\n",
        "            if \"mfi\" in current_config:\n",
        "                mfi_window = current_config[\"mfi\"].get(\"window\", 14)\n",
        "                df[f'MFI_{mfi_window}'] = np.nan\n",
        "            if \"volume_sma\" in current_config:\n",
        "                vol_sma_window = current_config[\"volume_sma\"].get(\"window\", 20)\n",
        "                df[f'Volume_SMA_{vol_sma_window}'] = np.nan\n",
        "\n",
        "        # --- Output and Save Results ---\n",
        "        print(f\"\\n--- {pair_symbol} ({timeframe_ccxt}): DataFrame with Technical Indicators (First 3 rows) ---\")\n",
        "        pd.set_option('display.max_columns', None) # Show all columns for head()\n",
        "        print(df.head(3))\n",
        "        pd.reset_option('display.max_columns')\n",
        "\n",
        "        print(f\"\\n--- {pair_symbol} ({timeframe_ccxt}): DataFrame Info ---\")\n",
        "        df.info()\n",
        "\n",
        "        # Make output filename more descriptive\n",
        "        output_file_name = f\"{pair_symbol}_{timeframe_ccxt}_with_indicators.csv\"\n",
        "        full_output_path = os.path.join(GDRIVE_OUTPUT_PATH, output_file_name)\n",
        "        try:\n",
        "            df.to_csv(full_output_path)\n",
        "            print(f\"Successfully saved DataFrame for {pair_symbol} ({timeframe_ccxt}) with indicators to: {full_output_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: Could not save DataFrame for {pair_symbol} ({timeframe_ccxt}) to '{full_output_path}': {e}\")\n",
        "\n",
        "print(\"\\n\\n--- All Coin Processing Complete ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "8eAPYFYGkQaJ",
        "outputId": "655f511f-2594-4645-835b-5c792a377cb5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output directory in Google Drive already exists or is accessible: /content/drive/MyDrive/CryptoDataCCXT/\n",
            "\n",
            "\n",
            "Processing data for: BTC_USDT (1d)\n",
            "==================================================\n",
            "Successfully loaded data for BTC_USDT (1d) from: /content/drive/MyDrive/CryptoDataCCXT/BTC_USDT_1d_4years.csv\n",
            "DEBUG: Original columns found: ['open', 'high', 'low', 'close', 'volume', 'timestamp']\n",
            "DEBUG: Columns after renaming: ['Open', 'High', 'Low', 'Close', 'Volume', 'timestamp']\n",
            "Using specific indicator configurations for BTC (from BTC_USDT).\n",
            "\n",
            "Applying technical indicators for BTC_USDT (1d)...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'add_sma' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-d337ca0a6c8f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"sma_short\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcurrent_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sma_short\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_sma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_column_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'{base_coin_symbol}_SMA_{params.get(\"window\", \"short\")}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"sma_long\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcurrent_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sma_long\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'add_sma' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wuKdrvL3MGw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uSemXyjaMIDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from google.colab import drive # For Google Drive access\n",
        "\n",
        "\n",
        "# --- Configuration for Verification ---\n",
        "# This should be the EXACT path where your INDICATOR SCRIPT (processing individual timeframes) saves its output.\n",
        "GDRIVE_INDICATORS_OUTPUT_PATH = \"/content/drive/MyDrive/CryptoDataCCXT/\" # As per your log\n",
        "\n",
        "# Define the files your indicator script is expected to generate\n",
        "TARGET_PAIRS_FOR_VERIFICATION = [\"BTC_USDT\", \"ETH_USDT\", \"SOL_USDT\"]\n",
        "TIMEFRAMES_FOR_VERIFICATION = [\"1d\", \"1h\", \"15m\"] # CCXT timeframe codes\n",
        "# The suffix added by your indicator script (the one we worked on before this verification script)\n",
        "OUTPUT_FILENAME_SUFFIX = \"with_indicators.csv\"\n",
        "\n",
        "EXPECTED_FILES_TO_VERIFY = []\n",
        "for pair in TARGET_PAIRS_FOR_VERIFICATION:\n",
        "    for tf in TIMEFRAMES_FOR_VERIFICATION:\n",
        "        # Filename format from your indicator processing script that adds indicators\n",
        "        filename = f\"{pair}_{tf}_{OUTPUT_FILENAME_SUFFIX}\"\n",
        "        EXPECTED_FILES_TO_VERIFY.append(filename)\n",
        "\n",
        "print(\"Expected files to verify (individual timeframe files with indicators):\")\n",
        "for f_name in EXPECTED_FILES_TO_VERIFY:\n",
        "    print(f\"- {f_name}\")\n",
        "\n",
        "# --- Main Verification Logic ---\n",
        "for file_to_verify in EXPECTED_FILES_TO_VERIFY:\n",
        "    full_file_path = os.path.join(GDRIVE_INDICATORS_OUTPUT_PATH, file_to_verify)\n",
        "    # Extract base coin symbol and timeframe for context in checks\n",
        "    parts = file_to_verify.split('_')\n",
        "    base_coin_symbol = parts[0] # e.g., BTC\n",
        "    timeframe_shortcode = parts[2] # e.g., 1d, 1h, 15m\n",
        "\n",
        "    print(f\"\\n\\n======================================================================\")\n",
        "    print(f\"--- Verifying File: {full_file_path} ---\")\n",
        "    print(f\"======================================================================\")\n",
        "\n",
        "    if not os.path.exists(full_file_path):\n",
        "        print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
        "        print(f\"ERROR: File not found at {full_file_path}\")\n",
        "        print(f\"Please ensure:\")\n",
        "        print(f\"1. The indicator processing script has run successfully and created this file.\")\n",
        "        print(f\"2. The path above is correct and your Google Drive is mounted.\")\n",
        "        print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
        "        continue # Skip to the next file\n",
        "    else:\n",
        "      try:\n",
        "          # Load the CSV. Index is 'datetime_utc'.\n",
        "          df = pd.read_csv(full_file_path, index_col='datetime_utc', parse_dates=True)\n",
        "          print(\"Successfully loaded the file.\")\n",
        "\n",
        "          if df.empty:\n",
        "              print(\"WARNING: The file is empty.\")\n",
        "          else:\n",
        "              print(f\"\\n1. Shape of the DataFrame (rows, columns): {df.shape}\")\n",
        "              if df.shape[0] == 0:\n",
        "                  print(\"WARNING: DataFrame has 0 rows.\")\n",
        "              if df.shape[1] == 0:\n",
        "                  print(\"WARNING: DataFrame has 0 columns.\")\n",
        "\n",
        "              print(\"\\n2. First 3 rows (transposed if many columns for readability):\")\n",
        "              try:\n",
        "                  pd.set_option('display.max_columns', None)\n",
        "                  if df.shape[1] > 15 : # If more than 15 columns, transpose\n",
        "                      print(df.head(3).T)\n",
        "                  else:\n",
        "                      print(df.head(3))\n",
        "              finally:\n",
        "                  pd.reset_option('display.max_columns')\n",
        "\n",
        "              print(\"\\n3. Index details:\")\n",
        "              print(f\"   Index name: {df.index.name}\")\n",
        "              print(f\"   Index data type: {df.index.dtype}\") # Should be datetime64[ns, UTC]\n",
        "              if not df.empty:\n",
        "                  print(f\"   Is index monotonic increasing? {df.index.is_monotonic_increasing}\")\n",
        "                  if not df.index.is_monotonic_increasing and len(df) > 1:\n",
        "                      print(f\"   WARNING: Index is not strictly monotonic increasing!\")\n",
        "                  print(f\"   Date range: {df.index.min()} to {df.index.max()}\")\n",
        "              print(f\"   Number of rows: {len(df)}\")\n",
        "              try: # Check frequency\n",
        "                  freq = pd.infer_freq(df.index)\n",
        "                  print(f\"   Inferred frequency of index: {freq}\")\n",
        "                  expected_freq_map = {'1d': 'D', '1h': 'H', '15m': '15T'} # 'T' or 'min' for minutes\n",
        "                  expected_tf_freq = expected_freq_map.get(timeframe_shortcode)\n",
        "                  if freq != expected_tf_freq and len(df) > 1:\n",
        "                      # Handle variations like 'BH' for business hour, '15min' vs '15T'\n",
        "                      is_freq_ok = False\n",
        "                      if timeframe_shortcode == '1h' and freq in ['H', 'BH']: is_freq_ok = True\n",
        "                      elif timeframe_shortcode == '15m' and freq in ['15T', '15min']: is_freq_ok = True\n",
        "                      elif timeframe_shortcode == '1d' and freq == 'D': is_freq_ok = True\n",
        "\n",
        "                      if not is_freq_ok:\n",
        "                          print(f\"   WARNING: Index frequency ({freq}) does not match expected for timeframe {timeframe_shortcode} (e.g., {expected_tf_freq}).\")\n",
        "              except Exception as e:\n",
        "                  print(f\"   Could not infer frequency: {e}\")\n",
        "\n",
        "\n",
        "              print(\"\\n4. Column Name Checks (No Prefixes Expected):\")\n",
        "              all_columns = df.columns.tolist()\n",
        "              print(f\"   Total columns found: {len(all_columns)}\")\n",
        "              # Check for essential OHLCV columns (should be uppercase after your processing script)\n",
        "              essential_ohlcv = ['Open', 'High', 'Low', 'Close', 'Volume', 'timestamp']\n",
        "              missing_essentials = []\n",
        "              for col_name in essential_ohlcv:\n",
        "                  if col_name not in df.columns:\n",
        "                      missing_essentials.append(col_name)\n",
        "              if missing_essentials:\n",
        "                  print(f\"     WARNING: Missing essential OHLCV or original timestamp columns: {missing_essentials}\")\n",
        "              else:\n",
        "                  print(f\"     All essential OHLCV + original timestamp columns found.\")\n",
        "\n",
        "              # Check for example indicator columns (these names depend on your master_indicator_configs)\n",
        "              # This is a simple check; a full check would parse master_indicator_configs\n",
        "              example_indicator_suffixes = ['_SMA_', '_EMA_', 'RSI_', 'BB_SMA_', 'MACD_', 'ATR_', 'Stoch_%K', 'MFI_', 'ROC_']\n",
        "              found_indicator_examples = []\n",
        "              for col in all_columns:\n",
        "                  if any(suffix in col for suffix in example_indicator_suffixes):\n",
        "                      found_indicator_examples.append(col)\n",
        "\n",
        "              if found_indicator_examples:\n",
        "                  print(f\"   Found example indicator columns (first few): {found_indicator_examples[:5]}\")\n",
        "                  if len(found_indicator_examples) == 0 : # Corrected from > 0\n",
        "                     print(f\"   WARNING: No example indicator columns found! Check indicator generation.\")\n",
        "              else:\n",
        "                  print(f\"   WARNING: No example indicator columns (based on suffixes) found! All columns: {all_columns}\")\n",
        "\n",
        "\n",
        "              print(\"\\n5. NaN Value Analysis (Summary):\")\n",
        "              total_cells = df.shape[0] * df.shape[1]\n",
        "              total_nan_cells = df.isnull().sum().sum()\n",
        "              if total_cells > 0 :\n",
        "                  percentage_nan = (total_nan_cells / total_cells) * 100\n",
        "                  print(f\"   Total NaN values: {total_nan_cells} (out of {total_cells} cells)\")\n",
        "                  print(f\"   Percentage of NaN values: {percentage_nan:.2f}%\")\n",
        "              else:\n",
        "                  print(f\"   DataFrame is empty, no NaN values to analyze.\")\n",
        "\n",
        "              nan_summary_per_col = df.isnull().sum()\n",
        "              cols_with_nans = nan_summary_per_col[nan_summary_per_col > 0].sort_values(ascending=False)\n",
        "              if not cols_with_nans.empty:\n",
        "                  print(\"   Top columns with NaN values and their counts (max 10 shown):\")\n",
        "                  print(cols_with_nans.head(10))\n",
        "                  if len(cols_with_nans) > 10:\n",
        "                      print(f\"   ... and {len(cols_with_nans) - 10} more columns with NaNs.\")\n",
        "                  print(\"   (Note: NaNs are expected at the start of indicator series due to lookback periods.)\")\n",
        "              elif not df.empty :\n",
        "                  print(\"   No NaN values found in any column.\")\n",
        "\n",
        "      except pd.errors.EmptyDataError:\n",
        "          print(f\"ERROR: The file '{full_file_path}' is empty and cannot be parsed by Pandas.\")\n",
        "      except Exception as e:\n",
        "          print(f\"ERROR: An error occurred while reading or processing '{full_file_path}': {e}\")\n",
        "\n",
        "print(\"\\n\\n--- Verification Script for Individual Timeframe Files Complete ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLSPTuqF8gGk",
        "outputId": "f434bee6-dbc9-4543-bfb9-3038c9e97be8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected files to verify (individual timeframe files with indicators):\n",
            "- BTC_USDT_1d_with_indicators.csv\n",
            "- BTC_USDT_1h_with_indicators.csv\n",
            "- BTC_USDT_15m_with_indicators.csv\n",
            "- ETH_USDT_1d_with_indicators.csv\n",
            "- ETH_USDT_1h_with_indicators.csv\n",
            "- ETH_USDT_15m_with_indicators.csv\n",
            "- SOL_USDT_1d_with_indicators.csv\n",
            "- SOL_USDT_1h_with_indicators.csv\n",
            "- SOL_USDT_15m_with_indicators.csv\n",
            "\n",
            "\n",
            "======================================================================\n",
            "--- Verifying File: /content/drive/MyDrive/CryptoDataCCXT/BTC_USDT_1d_with_indicators.csv ---\n",
            "======================================================================\n",
            "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
            "ERROR: File not found at /content/drive/MyDrive/CryptoDataCCXT/BTC_USDT_1d_with_indicators.csv\n",
            "Please ensure:\n",
            "1. The indicator processing script has run successfully and created this file.\n",
            "2. The path above is correct and your Google Drive is mounted.\n",
            "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
            "\n",
            "\n",
            "======================================================================\n",
            "--- Verifying File: /content/drive/MyDrive/CryptoDataCCXT/BTC_USDT_1h_with_indicators.csv ---\n",
            "======================================================================\n",
            "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
            "ERROR: File not found at /content/drive/MyDrive/CryptoDataCCXT/BTC_USDT_1h_with_indicators.csv\n",
            "Please ensure:\n",
            "1. The indicator processing script has run successfully and created this file.\n",
            "2. The path above is correct and your Google Drive is mounted.\n",
            "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
            "\n",
            "\n",
            "======================================================================\n",
            "--- Verifying File: /content/drive/MyDrive/CryptoDataCCXT/BTC_USDT_15m_with_indicators.csv ---\n",
            "======================================================================\n",
            "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
            "ERROR: File not found at /content/drive/MyDrive/CryptoDataCCXT/BTC_USDT_15m_with_indicators.csv\n",
            "Please ensure:\n",
            "1. The indicator processing script has run successfully and created this file.\n",
            "2. The path above is correct and your Google Drive is mounted.\n",
            "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
            "\n",
            "\n",
            "======================================================================\n",
            "--- Verifying File: /content/drive/MyDrive/CryptoDataCCXT/ETH_USDT_1d_with_indicators.csv ---\n",
            "======================================================================\n",
            "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
            "ERROR: File not found at /content/drive/MyDrive/CryptoDataCCXT/ETH_USDT_1d_with_indicators.csv\n",
            "Please ensure:\n",
            "1. The indicator processing script has run successfully and created this file.\n",
            "2. The path above is correct and your Google Drive is mounted.\n",
            "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
            "\n",
            "\n",
            "======================================================================\n",
            "--- Verifying File: /content/drive/MyDrive/CryptoDataCCXT/ETH_USDT_1h_with_indicators.csv ---\n",
            "======================================================================\n",
            "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
            "ERROR: File not found at /content/drive/MyDrive/CryptoDataCCXT/ETH_USDT_1h_with_indicators.csv\n",
            "Please ensure:\n",
            "1. The indicator processing script has run successfully and created this file.\n",
            "2. The path above is correct and your Google Drive is mounted.\n",
            "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
            "\n",
            "\n",
            "======================================================================\n",
            "--- Verifying File: /content/drive/MyDrive/CryptoDataCCXT/ETH_USDT_15m_with_indicators.csv ---\n",
            "======================================================================\n",
            "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
            "ERROR: File not found at /content/drive/MyDrive/CryptoDataCCXT/ETH_USDT_15m_with_indicators.csv\n",
            "Please ensure:\n",
            "1. The indicator processing script has run successfully and created this file.\n",
            "2. The path above is correct and your Google Drive is mounted.\n",
            "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
            "\n",
            "\n",
            "======================================================================\n",
            "--- Verifying File: /content/drive/MyDrive/CryptoDataCCXT/SOL_USDT_1d_with_indicators.csv ---\n",
            "======================================================================\n",
            "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
            "ERROR: File not found at /content/drive/MyDrive/CryptoDataCCXT/SOL_USDT_1d_with_indicators.csv\n",
            "Please ensure:\n",
            "1. The indicator processing script has run successfully and created this file.\n",
            "2. The path above is correct and your Google Drive is mounted.\n",
            "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
            "\n",
            "\n",
            "======================================================================\n",
            "--- Verifying File: /content/drive/MyDrive/CryptoDataCCXT/SOL_USDT_1h_with_indicators.csv ---\n",
            "======================================================================\n",
            "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
            "ERROR: File not found at /content/drive/MyDrive/CryptoDataCCXT/SOL_USDT_1h_with_indicators.csv\n",
            "Please ensure:\n",
            "1. The indicator processing script has run successfully and created this file.\n",
            "2. The path above is correct and your Google Drive is mounted.\n",
            "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
            "\n",
            "\n",
            "======================================================================\n",
            "--- Verifying File: /content/drive/MyDrive/CryptoDataCCXT/SOL_USDT_15m_with_indicators.csv ---\n",
            "======================================================================\n",
            "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
            "ERROR: File not found at /content/drive/MyDrive/CryptoDataCCXT/SOL_USDT_15m_with_indicators.csv\n",
            "Please ensure:\n",
            "1. The indicator processing script has run successfully and created this file.\n",
            "2. The path above is correct and your Google Drive is mounted.\n",
            "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
            "\n",
            "\n",
            "--- Verification Script for Individual Timeframe Files Complete ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def read_and_display_single_csv(file_path, num_rows_to_display=25):\n",
        "    \"\"\"\n",
        "    Reads a single CSV file and displays basic information about it.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The full path to the CSV file.\n",
        "        num_rows_to_display (int): The number of rows to display from the head.\n",
        "    \"\"\"\n",
        "    print(f\"--- Attempting to read file: {file_path} ---\")\n",
        "\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"ERROR: File not found at '{file_path}'.\")\n",
        "        print(\"Please ensure the file path is correct and the file exists.\")\n",
        "        return\n",
        "\n",
        "    if not os.path.isfile(file_path):\n",
        "        print(f\"ERROR: '{file_path}' is not a file.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # Attempt to read the CSV file.\n",
        "        # You might need to specify other parameters for pd.read_csv() depending on your file's format,\n",
        "        # such as:\n",
        "        # - sep=',' (or other delimiter like '\\t', ';')\n",
        "        # - header=0 (row number to use as column names)\n",
        "        # - index_col='your_index_column_name' (if you have a specific index column)\n",
        "        # - parse_dates=['your_date_column'] (to parse specific columns as dates)\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        if df.empty:\n",
        "            print(\"WARNING: The CSV file is empty (contains no data or only headers).\")\n",
        "        else:\n",
        "            print(\"\\nSuccessfully read the file.\")\n",
        "\n",
        "            print(f\"\\n1. First {num_rows_to_display} rows of the DataFrame:\")\n",
        "            print(df.head(num_rows_to_display))\n",
        "\n",
        "            print(f\"\\n2. Shape of the DataFrame (rows, columns): {df.shape}\")\n",
        "\n",
        "            print(\"\\n3. DataFrame Info (column data types and non-null values):\")\n",
        "            df.info()\n",
        "\n",
        "            print(\"\\n4. Basic descriptive statistics for numerical columns:\")\n",
        "            print(df.describe())\n",
        "\n",
        "    except pd.errors.EmptyDataError:\n",
        "        print(\"ERROR: The file is empty and could not be parsed by Pandas (no columns found).\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while reading or processing the file: {e}\")\n",
        "\n",
        "# --- Main Execution Example ---\n",
        "if __name__ == \"__main__\":\n",
        "    # **IMPORTANT**: Replace this with the actual path to YOUR file!\n",
        "    # Example for a file in the current directory:\n",
        "    # file_to_read = \"my_data.csv\"\n",
        "\n",
        "    # Example for a file in a Colab session (after uploading or generation):\n",
        "    # file_to_read = \"/content/BTC_USDT_1d_with_indicators.csv\"\n",
        "\n",
        "    # Example for a file on Google Drive (ensure Drive is mounted in Colab first):\n",
        "    # Make sure to run the drive mount cell in Colab:\n",
        "    # from google.colab import drive\n",
        "    # drive.mount('/content/drive')\n",
        "    # file_to_read = \"/content/drive/MyDrive/CryptoDataCCXT/with_indicators/BTC_USDT_1d_with_indicators.csv\"\n",
        "\n",
        "    # --- SET YOUR FILE PATH HERE ---\n",
        "    # Please update this path to point to the specific file you want to read.\n",
        "    # For instance, if you just ran the indicator script and want to check one of its outputs:\n",
        "    file_to_read = \"/content/drive/MyDrive/CryptoDataCCXT/with_indicators/BTC_USDT_1d_with_indicators.csv\"\n",
        "    # Or, if you are not in Colab and the file is in the same directory as the script:\n",
        "    # file_to_read = \"BTC_USDT_1d_with_indicators.csv\"\n",
        "\n",
        "\n",
        "    # Check if running in Colab and the path looks like a Drive path, then try to mount.\n",
        "    # This is a basic check; for more robust Colab/Drive handling, you might expand this.\n",
        "    if file_to_read.startswith(\"/content/drive/\"):\n",
        "        try:\n",
        "            from google.colab import drive\n",
        "            drive.mount('/content/drive', force_remount=True) # Using force_remount for convenience\n",
        "            print(\"Google Drive mounted (or re-mounted).\")\n",
        "        except ModuleNotFoundError:\n",
        "            print(\"Warning: Not in a Colab environment, but path looks like a Colab Drive path. Proceeding...\")\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not mount Google Drive: {e}. Ensure it's mounted if the path requires it.\")\n",
        "\n",
        "\n",
        "    read_and_display_single_csv(file_to_read)\n",
        "\n",
        "    print(\"\\n--- Script execution finished ---\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bE0jruzoHp7e",
        "outputId": "6edf05a1-2f5e-493b-aa2a-093c3c452a83"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Could not mount Google Drive: Mountpoint must not already contain files. Ensure it's mounted if the path requires it.\n",
            "--- Attempting to read file: /content/drive/MyDrive/CryptoDataCCXT/with_indicators/BTC_USDT_1d_with_indicators.csv ---\n",
            "ERROR: File not found at '/content/drive/MyDrive/CryptoDataCCXT/with_indicators/BTC_USDT_1d_with_indicators.csv'.\n",
            "Please ensure the file path is correct and the file exists.\n",
            "\n",
            "--- Script execution finished ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np # <<< Ensure numpy is imported for np.nan\n",
        "from google.colab import drive # For Google Drive access\n",
        "\n",
        "# --- Step 1: Mount Google Drive (Assumed to be run in a previous cell or handled)---\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error mounting Google Drive: {e}\")\n",
        "    raise RuntimeError(\"Google Drive could not be mounted. Script aborted.\")\n",
        "\n",
        "# --- Step 2: Ensure Indicator Functions and Master Config are Defined ---\n",
        "# IMPORTANT: Your functions (add_sma, add_ema, ..., add_basic_support_resistance)\n",
        "# AND your 'master_indicator_configs' dictionary MUST be defined in your\n",
        "# Colab notebook BEFORE this script cell is executed.\n",
        "\n",
        "# --- Configuration for the specific file to process ---\n",
        "BASE_OHLCV_PATH = \"/content/drive/MyDrive/CryptoDataCCXT/\" # Path to original CCXT data (without indicators)\n",
        "FILE_TO_PROCESS_PAIR = \"BTC_USDT\"\n",
        "FILE_TO_PROCESS_TIMEFRAME = \"1d\" # Example: \"1d\", \"1h\", or \"15m\"\n",
        "FILE_TO_PROCESS_DURATION_TAG = \"4years\"\n",
        "FILE_TO_PROCESS = os.path.join(BASE_OHLCV_PATH, f\"{FILE_TO_PROCESS_PAIR}_{FILE_TO_PROCESS_TIMEFRAME}_{FILE_TO_PROCESS_DURATION_TAG}.csv\")\n",
        "BASE_COIN_SYMBOL = FILE_TO_PROCESS_PAIR.split('_')[0] # e.g., \"BTC\" from \"BTC_USDT\"\n",
        "\n",
        "print(f\"\\n--- Processing File to Generate/Inspect Specific Indicator Data ---\")\n",
        "print(f\"File: {FILE_TO_PROCESS}\")\n",
        "print(f\"Base Coin for Config: {BASE_COIN_SYMBOL}\")\n",
        "\n",
        "if not os.path.exists(FILE_TO_PROCESS):\n",
        "    print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
        "    print(f\"ERROR: File not found at {FILE_TO_PROCESS}\")\n",
        "    print(f\"Please ensure the path and filename are correct, it's a BASE OHLCV file,\")\n",
        "    print(f\"and your Google Drive is mounted.\")\n",
        "    print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
        "else:\n",
        "    try:\n",
        "        # Load the base OHLCV CSV file.\n",
        "        df = pd.read_csv(FILE_TO_PROCESS)\n",
        "\n",
        "        # Standardize Datetime Index\n",
        "        if 'datetime_utc' in df.columns:\n",
        "            df['datetime_utc'] = pd.to_datetime(df['datetime_utc'])\n",
        "            df.set_index('datetime_utc', inplace=True)\n",
        "            print(\"Used 'datetime_utc' column as DatetimeIndex.\")\n",
        "        elif 'timestamp' in df.columns and not isinstance(df.index, pd.DatetimeIndex):\n",
        "            df['datetime_utc'] = pd.to_datetime(df['timestamp'], unit='ms', utc=True)\n",
        "            df.set_index('datetime_utc', inplace=True)\n",
        "            print(\"Used 'timestamp' (ms) column to create and set DatetimeIndex 'datetime_utc'.\")\n",
        "        else:\n",
        "            print(\"Warning: No 'datetime_utc' or 'timestamp' column found to set as DatetimeIndex. Ensure your CSV has one for time series analysis.\")\n",
        "\n",
        "\n",
        "        print(f\"Successfully loaded: {FILE_TO_PROCESS}\")\n",
        "\n",
        "        # Ensure OHLCV columns are uppercase for indicator functions\n",
        "        rename_map = {\n",
        "            'open': 'Open', 'high': 'High', 'low': 'Low', 'close': 'Close', 'volume': 'Volume'\n",
        "        }\n",
        "        cols_to_rename = {k: v for k, v in rename_map.items() if k in df.columns}\n",
        "        if cols_to_rename:\n",
        "            df.rename(columns=cols_to_rename, inplace=True)\n",
        "            print(f\"Renamed base OHLCV columns to uppercase: {list(cols_to_rename.values())}\")\n",
        "\n",
        "        print(f\"Columns after potential rename and index setting: {df.columns.tolist()}\")\n",
        "\n",
        "        # --- Apply/Re-apply All Indicators from Master Config ---\n",
        "        if BASE_COIN_SYMBOL in master_indicator_configs: # master_indicator_configs must be defined\n",
        "            current_config = master_indicator_configs[BASE_COIN_SYMBOL]\n",
        "\n",
        "            print(f\"\\nApplying all configured indicators for {BASE_COIN_SYMBOL}...\")\n",
        "            # Work on a copy if you want to preserve the original df in this cell's scope\n",
        "            # or if your indicator functions modify inplace in unexpected ways.\n",
        "            # Since your functions return df, direct assignment is also fine.\n",
        "            df_with_indicators = df.copy()\n",
        "\n",
        "            # --- Moving Averages ---\n",
        "            if \"sma_short\" in current_config:\n",
        "                params = current_config[\"sma_short\"]\n",
        "                col_name = f'{BASE_COIN_SYMBOL}_SMA_{params.get(\"window\")}' # .get(\"window\", default_val) if window might be missing\n",
        "                if col_name in df_with_indicators.columns: print(f\"Info: Column {col_name} exists. Re-calculating.\")\n",
        "                df_with_indicators = add_sma(df_with_indicators, **params, new_column_name=col_name) # Assumes add_sma and others are defined\n",
        "            if \"sma_long\" in current_config:\n",
        "                params = current_config[\"sma_long\"]\n",
        "                col_name = f'{BASE_COIN_SYMBOL}_SMA_{params.get(\"window\")}'\n",
        "                if col_name in df_with_indicators.columns: print(f\"Info: Column {col_name} exists. Re-calculating.\")\n",
        "                df_with_indicators = add_sma(df_with_indicators, **params, new_column_name=col_name)\n",
        "            if \"ema_short\" in current_config:\n",
        "                params = current_config[\"ema_short\"]\n",
        "                col_name = f'{BASE_COIN_SYMBOL}_EMA_{params.get(\"span\")}'\n",
        "                if col_name in df_with_indicators.columns: print(f\"Info: Column {col_name} exists. Re-calculating.\")\n",
        "                df_with_indicators = add_ema(df_with_indicators, **params, new_column_name=col_name)\n",
        "            if \"ema_long\" in current_config:\n",
        "                params = current_config[\"ema_long\"]\n",
        "                col_name = f'{BASE_COIN_SYMBOL}_EMA_{params.get(\"span\")}'\n",
        "                if col_name in df_with_indicators.columns: print(f\"Info: Column {col_name} exists. Re-calculating.\")\n",
        "                df_with_indicators = add_ema(df_with_indicators, **params, new_column_name=col_name)\n",
        "\n",
        "            # --- Other Non-Volume Dependent Indicators ---\n",
        "            if \"rsi\" in current_config:\n",
        "                params = current_config[\"rsi\"]\n",
        "                col_name = f'RSI_{params.get(\"window\", 14)}' # Defaulting for print, add_rsi handles its own name\n",
        "                if col_name in df_with_indicators.columns: print(f\"Info: Column {col_name} (or similar from add_rsi) exists. Re-calculating.\")\n",
        "                df_with_indicators = add_rsi(df_with_indicators, **params) # add_rsi creates its own default name if new_column_name not passed\n",
        "\n",
        "            if \"bollinger\" in current_config:\n",
        "                params = current_config[\"bollinger\"]\n",
        "                # add_bollinger_bands creates multiple columns (BB_SMA_*, BB_Upper_*, BB_Lower_*)\n",
        "                # We'll just check one for the print message\n",
        "                col_name_check = f'BB_SMA_{params.get(\"window\", 20)}'\n",
        "                if col_name_check in df_with_indicators.columns: print(f\"Info: Bollinger Band columns (e.g. {col_name_check}) may exist. Re-calculating.\")\n",
        "                df_with_indicators = add_bollinger_bands(df_with_indicators, **params)\n",
        "\n",
        "            if \"macd\" in current_config:\n",
        "                params = current_config[\"macd\"]\n",
        "                col_name_check = f'MACD_{params.get(\"short_window\",12)}_{params.get(\"long_window\",26)}'\n",
        "                if col_name_check in df_with_indicators.columns: print(f\"Info: MACD columns (e.g. {col_name_check}) may exist. Re-calculating.\")\n",
        "                df_with_indicators = add_macd(df_with_indicators, **params)\n",
        "\n",
        "            if \"atr\" in current_config:\n",
        "                params = current_config[\"atr\"]\n",
        "                col_name = f'ATR_{params.get(\"window\", 14)}'\n",
        "                if col_name in df_with_indicators.columns: print(f\"Info: Column {col_name} exists. Re-calculating.\")\n",
        "                df_with_indicators = add_atr(df_with_indicators, **params)\n",
        "\n",
        "            if \"stochastic\" in current_config:\n",
        "                params = current_config[\"stochastic\"]\n",
        "                col_name_check = f'Stoch_%K_{params.get(\"k_window\",14)}'\n",
        "                if col_name_check in df_with_indicators.columns: print(f\"Info: Stochastic columns (e.g. {col_name_check}) may exist. Re-calculating.\")\n",
        "                df_with_indicators = add_stochastic_oscillator(df_with_indicators, **params)\n",
        "\n",
        "            if \"roc\" in current_config:\n",
        "                params = current_config[\"roc\"]\n",
        "                col_name = f'ROC_{params.get(\"window\")}' # Assumes \"window\" is always in params for ROC\n",
        "                if col_name in df_with_indicators.columns: print(f\"Info: Column {col_name} exists. Re-calculating.\")\n",
        "                df_with_indicators = add_roc(df_with_indicators, **params)\n",
        "\n",
        "            if \"keltner\" in current_config:\n",
        "                params = current_config[\"keltner\"]\n",
        "                col_name_check = f'KC_Middle_{params.get(\"ema_window\",20)}'\n",
        "                if col_name_check in df_with_indicators.columns: print(f\"Info: Keltner Channel columns (e.g. {col_name_check}) may exist. Re-calculating.\")\n",
        "                df_with_indicators = add_keltner_channels(df_with_indicators, **params)\n",
        "\n",
        "            if \"support_resistance\" in current_config:\n",
        "                params = current_config[\"support_resistance\"]\n",
        "                col_name_check = f'Support_{params.get(\"window\",20)}'\n",
        "                if col_name_check in df_with_indicators.columns: print(f\"Info: Support/Resistance columns (e.g. {col_name_check}) may exist. Re-calculating.\")\n",
        "                df_with_indicators = add_basic_support_resistance(df_with_indicators, **params)\n",
        "\n",
        "            # --- Indicators REQUIRING Volume (MFI, Volume SMA) ---\n",
        "            if 'Volume' in df_with_indicators.columns:\n",
        "                print(f\"DEBUG: 'Volume' column available. Proceeding with volume-based indicators.\")\n",
        "                if \"mfi\" in current_config:\n",
        "                    params = current_config[\"mfi\"]\n",
        "                    col_name = f'MFI_{params.get(\"window\", 14)}'\n",
        "                    if col_name in df_with_indicators.columns: print(f\"Info: Column {col_name} exists. Re-calculating.\")\n",
        "                    df_with_indicators = add_mfi(df_with_indicators, **params)\n",
        "\n",
        "                if \"volume_sma\" in current_config:\n",
        "                    params = current_config[\"volume_sma\"]\n",
        "                    col_name = f'Volume_SMA_{params.get(\"window\", 20)}'\n",
        "                    if col_name in df_with_indicators.columns: print(f\"Info: Column {col_name} exists. Re-calculating.\")\n",
        "                    df_with_indicators = add_volume_sma(df_with_indicators, **params)\n",
        "            else:\n",
        "                print(f\"WARNING: 'Volume' column NOT found in DataFrame. Skipping MFI and Volume SMA.\")\n",
        "                # Add NaN columns if they were expected but volume was missing\n",
        "                if \"mfi\" in current_config:\n",
        "                    params = current_config[\"mfi\"]\n",
        "                    df_with_indicators[f'MFI_{params.get(\"window\", 14)}'] = np.nan\n",
        "                if \"volume_sma\" in current_config:\n",
        "                    params = current_config[\"volume_sma\"]\n",
        "                    df_with_indicators[f'Volume_SMA_{params.get(\"window\", 20)}'] = np.nan\n",
        "\n",
        "            print(\"All configured indicators applied/re-applied.\")\n",
        "\n",
        "            # --- Display and Inspect the Indicator Data ---\n",
        "            print(f\"\\n--- Specific Indicator Data for {BASE_COIN_SYMBOL} (from {FILE_TO_PROCESS_PAIR} {FILE_TO_PROCESS_TIMEFRAME}) ---\")\n",
        "            print(f\"\\n1. DataFrame Shape (rows, columns): {df_with_indicators.shape}\")\n",
        "\n",
        "            print(\"\\n2. First 5 rows (transposed for readability if many columns):\")\n",
        "            pd.set_option('display.max_columns', None)\n",
        "            if df_with_indicators.shape[1] > 10: print(df_with_indicators.head().T)\n",
        "            else: print(df_with_indicators.head())\n",
        "            pd.reset_option('display.max_columns')\n",
        "\n",
        "            print(\"\\n3. Last 5 rows (transposed for readability if many columns):\")\n",
        "            pd.set_option('display.max_columns', None)\n",
        "            if df_with_indicators.shape[1] > 10: print(df_with_indicators.tail().T)\n",
        "            else: print(df_with_indicators.tail())\n",
        "            pd.reset_option('display.max_columns')\n",
        "\n",
        "            print(\"\\n4. All Column Names after adding indicators:\")\n",
        "            print(df_with_indicators.columns.tolist())\n",
        "\n",
        "            print(\"\\n5. Info for DataFrame with indicators:\")\n",
        "            df_with_indicators.info()\n",
        "\n",
        "            # Optionally, save this newly processed DataFrame\n",
        "            output_filename_processed = os.path.join(BASE_OHLCV_PATH, \"with_indicators_calculated_now\", f\"{FILE_TO_PROCESS_PAIR}_{FILE_TO_PROCESS_TIMEFRAME}_calculated_indicators.csv\")\n",
        "            os.makedirs(os.path.dirname(output_filename_processed), exist_ok=True)\n",
        "            df_with_indicators.to_csv(output_filename_processed)\n",
        "            print(f\"\\nSaved DataFrame with calculated indicators to: {output_filename_processed}\")\n",
        "\n",
        "        else:\n",
        "            print(f\"ERROR: No configuration found for '{BASE_COIN_SYMBOL}' in master_indicator_configs.\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        # This is already handled by the outer check, but good for robustness\n",
        "        print(f\"ERROR: File not found during processing: {FILE_TO_PROCESS}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during data processing or indicator application: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc() # Print full traceback for debugging\n",
        "\n",
        "print(\"\\n--- Script to generate/inspect specific indicator data complete ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "uKVRKlp_N23u",
        "outputId": "e4cf2c19-f81f-4a40-f1da-05f25850d680"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error mounting Google Drive: Mountpoint must not already contain files\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Google Drive could not be mounted. Script aborted.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-0694e68a9c7f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Google Drive mounted successfully.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must not already contain files'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Mountpoint must not already contain files",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-0694e68a9c7f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error mounting Google Drive: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Google Drive could not be mounted. Script aborted.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# --- Step 2: Ensure Indicator Functions and Master Config are Defined ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Google Drive could not be mounted. Script aborted."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# --- Step 1: Mount Google Drive ---\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error mounting Google Drive: {e}\")\n",
        "    raise RuntimeError(\"Google Drive could not be mounted. Script aborted.\")\n",
        "\n",
        "# --- Step 2: Load Your Data with Indicators ---\n",
        "# This file should already contain your calculated SMAs\n",
        "GDRIVE_INDICATORS_PATH = \"/content/drive/MyDrive/CryptoDataCCXT/with_indicators/\"\n",
        "file_to_process = \"BTC_USDT_1d_with_indicators.csv\" # Using the daily BTC file\n",
        "full_file_path = os.path.join(GDRIVE_INDICATORS_PATH, file_to_process)\n",
        "\n",
        "print(f\"\\n--- Loading data for strategy: {full_file_path} ---\")\n",
        "\n",
        "if not os.path.exists(full_file_path):\n",
        "    print(f\"ERROR: File not found at {full_file_path}. Cannot proceed.\")\n",
        "else:\n",
        "    try:\n",
        "        df = pd.read_csv(full_file_path, index_col='datetime_utc', parse_dates=True)\n",
        "        print(\"Successfully loaded the data.\")\n",
        "        print(f\"Data shape: {df.shape}\")\n",
        "        print(\"First 5 rows:\")\n",
        "        print(df.head())\n",
        "\n",
        "        # --- Step 3: Define Indicator Columns for the Strategy ---\n",
        "        # These names should match the columns in your CSV file.\n",
        "        # These are based on your master_indicator_configs for BTC:\n",
        "        # \"BTC\": { \"sma_short\": {\"window\": 10, \"column\": \"Close\"},\n",
        "        #          \"sma_long\":  {\"window\": 50, \"column\": \"Close\"}, ... }\n",
        "        short_sma_col = 'BTC_SMA_10' # Adjust if your sma_short for BTC has a different window\n",
        "        long_sma_col = 'BTC_SMA_50'  # Adjust if your sma_long for BTC has a different window\n",
        "\n",
        "        # Check if required SMA columns exist\n",
        "        if not all(col in df.columns for col in [short_sma_col, long_sma_col]):\n",
        "            print(f\"ERROR: Required SMA columns ('{short_sma_col}', '{long_sma_col}') not found in the DataFrame.\")\n",
        "            print(f\"Available columns: {df.columns.tolist()}\")\n",
        "            # Exit or handle error appropriately\n",
        "        else:\n",
        "            print(f\"\\nUsing short SMA: {short_sma_col}, Long SMA: {long_sma_col}\")\n",
        "\n",
        "            # --- Step 4: Generate Trading Signals (SMA Crossover) ---\n",
        "            # Initialize signal column: 0 = Hold, 1 = Buy, -1 = Sell (to exit long)\n",
        "            df['signal'] = 0\n",
        "\n",
        "            # Generate buy signals\n",
        "            # Condition 1: Short SMA crosses above Long SMA\n",
        "            # (Short SMA > Long SMA in current period) AND (Short SMA < Long SMA in previous period)\n",
        "            df['buy_condition'] = (df[short_sma_col] > df[long_sma_col]) & \\\n",
        "                                  (df[short_sma_col].shift(1) < df[long_sma_col].shift(1))\n",
        "            df.loc[df['buy_condition'], 'signal'] = 1\n",
        "\n",
        "            # Generate sell signals (to exit a long position)\n",
        "            # Condition 1: Short SMA crosses below Long SMA\n",
        "            # (Short SMA < Long SMA in current period) AND (Short SMA > Long SMA in previous period)\n",
        "            df['sell_condition'] = (df[short_sma_col] < df[long_sma_col]) & \\\n",
        "                                   (df[short_sma_col].shift(1) > df[long_sma_col].shift(1))\n",
        "            df.loc[df['sell_condition'], 'signal'] = -1\n",
        "\n",
        "            # Remove temporary condition columns\n",
        "            df.drop(columns=['buy_condition', 'sell_condition'], inplace=True)\n",
        "\n",
        "            # --- Step 5: Display Results ---\n",
        "            print(\"\\n--- Strategy Signals Generated (SMA Crossover) ---\")\n",
        "\n",
        "            # Show rows where signals are generated\n",
        "            signals_df = df[df['signal'] != 0]\n",
        "            print(f\"Number of buy signals: {len(df[df['signal'] == 1])}\")\n",
        "            print(f\"Number of sell signals (exit long): {len(df[df['signal'] == -1])}\")\n",
        "\n",
        "            print(\"\\nFirst 10 signals generated:\")\n",
        "            print(signals_df[['Close', short_sma_col, long_sma_col, 'signal']].head(10))\n",
        "\n",
        "            print(\"\\nLast 10 signals generated:\")\n",
        "            print(signals_df[['Close', short_sma_col, long_sma_col, 'signal']].tail(10))\n",
        "\n",
        "            # You can save this DataFrame with signals for backtesting\n",
        "            output_filename = os.path.join(GDRIVE_INDICATORS_PATH, \"BTC_USDT_1d_sma_crossover_signals.csv\")\n",
        "            df.to_csv(output_filename)\n",
        "            print(f\"\\nSaved DataFrame with signals to: {output_filename}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "print(\"\\n--- Strategy signal generation script complete ---\")"
      ],
      "metadata": {
        "id": "QvTlq0-EU6r4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install backtesting"
      ],
      "metadata": {
        "id": "gFonfhzjVU5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "def list_all_files_in_drive(start_path):\n",
        "    \"\"\"\n",
        "    Lists all files recursively starting from the given path in Google Drive.\n",
        "\n",
        "    Args:\n",
        "        start_path (str): The starting directory path within your mounted Google Drive\n",
        "                          (e.g., \"/content/drive/MyDrive/\" or a subfolder like\n",
        "                          \"/content/drive/MyDrive/MyProjectFolder\").\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Listing all files starting from: {start_path} ---\")\n",
        "    file_count = 0\n",
        "    folder_count = 0\n",
        "\n",
        "    if not os.path.exists(start_path):\n",
        "        print(f\"ERROR: The path '{start_path}' does not exist. Please check the path.\")\n",
        "        return\n",
        "\n",
        "    if not os.path.isdir(start_path):\n",
        "        print(f\"ERROR: The path '{start_path}' is not a directory.\")\n",
        "        return\n",
        "\n",
        "    for root_directory, sub_directories, files_in_current_directory in os.walk(start_path):\n",
        "        folder_count += len(sub_directories) # Counts subdirectories encountered\n",
        "        for filename in files_in_current_directory:\n",
        "            file_path = os.path.join(root_directory, filename)\n",
        "            print(file_path)\n",
        "            file_count += 1\n",
        "\n",
        "    print(f\"\\n--- Scan Complete ---\")\n",
        "    print(f\"Total folders scanned (including subfolders): Approximately {folder_count} (os.walk yields them progressively)\")\n",
        "    print(f\"Total files found: {file_count}\")\n",
        "\n",
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Step 1: Mount Google Drive\n",
        "    try:\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        print(\"Google Drive mounted successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error mounting Google Drive: {e}\")\n",
        "        # If Drive doesn't mount, we can't proceed.\n",
        "        # You might want to exit or handle this error more gracefully.\n",
        "        # For this script, we'll let it try to proceed but it will likely fail if the path is Drive-based.\n",
        "\n",
        "    # Step 2: Define the starting path in your Google Drive\n",
        "    # To list ALL files in your \"My Drive\":\n",
        "    drive_start_path = \"/content/drive/MyDrive/CryptoDataCCXT/\"\n",
        "\n",
        "    # To list files in a SPECIFIC FOLDER within \"My Drive\":\n",
        "    # drive_start_path = \"/content/drive/MyDrive/YourSpecificFolder/AnotherSubFolder/\" # Example\n",
        "\n",
        "    # **IMPORTANT**: If you changed the name of \"My Drive\" (e.g., to \"My Files\"),\n",
        "    # adjust the path accordingly.\n",
        "    # Also, ensure the folder path is correct if you are not starting from the root of \"My Drive\".\n",
        "\n",
        "    # Check if the path exists after attempting to mount\n",
        "    if os.path.exists(drive_start_path):\n",
        "        list_all_files_in_drive(drive_start_path)\n",
        "    elif drive_start_path.startswith(\"/content/drive/\"): # If it looks like a drive path but doesn't exist\n",
        "        print(f\"\\nWARNING: The specified Google Drive path '{drive_start_path}' was not found after attempting to mount.\")\n",
        "        print(\"Please verify the path and ensure Google Drive is mounted correctly if you intended to use it.\")\n",
        "        print(\"If you see this after a successful mount message, the specific folder path might be incorrect.\")\n",
        "    else: # If it's not a drive path and doesn't exist\n",
        "         print(f\"\\nERROR: The specified path '{drive_start_path}' does not exist.\")"
      ],
      "metadata": {
        "id": "0c1BS_H75qgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "if data is not None:\n",
        "    plt.figure(figsize=(15, 7))\n",
        "    data['Close'].plot(label='Close Price', alpha=0.7)\n",
        "    data[short_sma_col_name].plot(label=short_sma_col_name)\n",
        "    data[long_sma_col_name].plot(label=long_sma_col_name)\n",
        "    plt.title(f'Price, {short_sma_col_name}, and {long_sma_col_name}')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "TzHwd1gjNgYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SmaCross(Strategy):\n",
        "    def init(self):\n",
        "        # Access the pre-calculated SMA series from the input DataFrame.\n",
        "        # 'self.data' provides an interface to the data, making columns accessible.\n",
        "        # The library will pass the correct series slices to functions like crossover.\n",
        "        self.sma1 = self.data.df[short_sma_col_name] # Short SMA\n",
        "        self.sma2 = self.data.df[long_sma_col_name] # Long SMA\n",
        "\n",
        "        # Optional: You can also use self.I to wrap existing series if you prefer,\n",
        "        # though direct access as above is fine for pre-calculated columns.\n",
        "        self.sma1 = self.I(lambda x: x, self.data.df[short_sma_col_name], name=\"ShortSMA\")\n",
        "        self.sma2 = self.I(lambda x: x, self.data.df[long_sma_col_name], name=\"LongSMA\")\n",
        "\n",
        "        print(f\"DEBUG init(): Short SMA series (first 5 values from data): \\n{self.sma1[:5]}\") # Use slicing for pandas series\n",
        "        print(f\"DEBUG init(): Long SMA series (first 5 values from data): \\n{self.sma2[:5]}\")\n",
        "\n",
        "\n",
        "    def next(self):\n",
        "        # The `crossover` function takes the two series.\n",
        "        # It implicitly looks at the current and previous values to determine a cross.\n",
        "\n",
        "        # If short SMA crosses above long SMA, and we're not already in a position, buy.\n",
        "        if crossover(self.sma1, self.sma2) and not self.position:\n",
        "            self.buy()\n",
        "            # print(f\"DEBUG: Buy signal on {self.data.index[-1]}\") # Uncomment for signal logging\n",
        "\n",
        "        # If short SMA crosses below long SMA, and we are in a position, sell/close.\n",
        "        elif crossover(self.sma2, self.sma1) and self.position:\n",
        "            self.position.close()\n",
        "            # print(f\"DEBUG: Sell signal on {self.data.index[-1]}\") # Uncomment for signal logging\n"
      ],
      "metadata": {
        "id": "fcRK9Y-kJnfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from google.colab import drive\n",
        "from backtesting import Backtest, Strategy\n",
        "from backtesting.lib import crossover # A utility for crossover detection\n",
        "\n",
        "# --- Mount Google Drive ---\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error mounting Google Drive: {e}\")\n",
        "    raise RuntimeError(\"Google Drive could not be mounted. Script aborted.\")\n",
        "\n",
        "# --- Load Your Data with Indicators ---\n",
        "GDRIVE_INDICATORS_PATH = \"/content/drive/MyDrive/CryptoDataCCXT/with_indicators/\"\n",
        "# Using the daily BTC file that already has indicators\n",
        "file_to_process = \"BTC_USDT_1d_with_indicators.csv\"\n",
        "full_file_path = os.path.join(GDRIVE_INDICATORS_PATH, file_to_process)\n",
        "\n",
        "print(f\"\\n--- Loading data for backtesting: {full_file_path} ---\")\n",
        "\n",
        "if not os.path.exists(full_file_path):\n",
        "    print(f\"ERROR: File not found at {full_file_path}. Cannot proceed with backtest.\")\n",
        "else:\n",
        "    try:\n",
        "        data = pd.read_csv(full_file_path, index_col='datetime_utc', parse_dates=True)\n",
        "        print(\"Successfully loaded the data.\")\n",
        "        print(f\"Data shape: {data.shape}\")\n",
        "\n",
        "        # Ensure essential columns are present (backtesting.py needs them uppercase)\n",
        "        required_cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
        "        if not all(col in data.columns for col in required_cols):\n",
        "            print(f\"ERROR: DataFrame is missing one or more required OHLCV columns: {required_cols}\")\n",
        "            print(f\"Available columns: {data.columns.tolist()}\")\n",
        "            # Consider exiting or raising an error if essential columns are missing\n",
        "            raise ValueError(\"DataFrame missing essential OHLCV columns for backtesting.\")\n",
        "\n",
        "        # --- Define Indicator Columns for the Strategy ---\n",
        "        # These names should match the columns in your CSV file.\n",
        "        # Based on your master_indicator_configs for BTC:\n",
        "        # \"BTC\": { \"sma_short\": {\"window\": 10}, \"sma_long\":  {\"window\": 50}, ... }\n",
        "        short_sma_col_name = 'BTC_SMA_10' # Adjust if your sma_short for BTC has a different window\n",
        "        long_sma_col_name = 'BTC_SMA_50'  # Adjust if your sma_long for BTC has a different window\n",
        "\n",
        "        if not all(col in data.columns for col in [short_sma_col_name, long_sma_col_name]):\n",
        "            print(f\"ERROR: DataFrame is missing SMA columns: '{short_sma_col_name}' or '{long_sma_col_name}'.\")\n",
        "            print(f\"Available columns: {data.columns.tolist()}\")\n",
        "            raise ValueError(\"DataFrame missing SMA columns for strategy.\")\n",
        "\n",
        "\n",
        "        # --- Run the Backtest ---\n",
        "        print(\"\\n--- Starting Backtest ---\")\n",
        "        # Initialize Backtest\n",
        "        # Cash: Initial capital\n",
        "        # Commission: Brokerage commission per trade (e.g., 0.001 for 0.1%)\n",
        "        # Exclusive_orders: If True, new orders cancel pending orders. Useful for some strategies.\n",
        "        bt = Backtest(data, SmaCross, cash=100000, commission=.002, exclusive_orders=True)\n",
        "\n",
        "        # Run the backtest\n",
        "        stats = bt.run()\n",
        "\n",
        "        print(\"\\n--- Backtest Statistics ---\")\n",
        "        print(stats)\n",
        "\n",
        "        # Print individual trades (optional, can be very long)\n",
        "        # print(\"\\n--- Trades ---\")\n",
        "        # print(stats['_trades']) # This DataFrame contains details of each trade\n",
        "\n",
        "        # --- Plot the Results ---\n",
        "        print(\"\\n--- Plotting Backtest Results (this will open in a new browser tab or display inline in some environments) ---\")\n",
        "        bt.plot()\n",
        "        # In Colab, the plot might try to open a new tab or might not display directly.\n",
        "        # If it doesn't display, you can save the plot to a file:\n",
        "        # bt.plot(filename='sma_cross_backtest.html', open_browser=False)\n",
        "        # print(\"Plot saved to sma_cross_backtest.html. You can download and open it.\")\n",
        "\n",
        "    except ValueError as ve:\n",
        "        print(f\"ValueError during data preparation or strategy definition: {ve}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during backtesting: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "print(\"\\n--- Backtesting script complete ---\")"
      ],
      "metadata": {
        "id": "4jQW5Cn05QFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "def calculate_max_drawdown(equity_series):\n",
        "    if equity_series.empty or len(equity_series) < 2:\n",
        "        return 0.0\n",
        "    running_max = equity_series.cummax()\n",
        "    drawdown = (equity_series - running_max) / running_max\n",
        "    max_drawdown = drawdown.min()\n",
        "    return max_drawdown if pd.notna(max_drawdown) else 0.0\n",
        "\n",
        "def simulate_buy_on_signal_sell_on_profit(df_input, initial_capital, short_sma_col_name, long_sma_col_name, commission_rate=0.001):\n",
        "    df = df_input.copy()\n",
        "\n",
        "    results = {\n",
        "        \"strategy_applied\": \"Buy on SMA Crossover, Sell ONLY for Profit (on reverse crossover)\",\n",
        "        \"trades\": [],\n",
        "        \"initial_capital\": initial_capital,\n",
        "        \"final_equity\": initial_capital,\n",
        "        \"return_percentage\": 0.0,\n",
        "        \"max_drawdown_percentage\": 0.0, # Will be calculated on overall equity\n",
        "        \"number_of_trades\": 0,\n",
        "        \"winning_trades\": 0,\n",
        "        \"status\": \"Simulation not run yet.\"\n",
        "    }\n",
        "\n",
        "    # Ensure required columns exist\n",
        "    required_cols = ['Open', 'Close', short_sma_col_name, long_sma_col_name]\n",
        "    if not all(col in df.columns for col in required_cols):\n",
        "        missing = [col for col in required_cols if col not in df.columns]\n",
        "        results[\"status\"] = f\"Error: Missing required columns: {missing}\"\n",
        "        print(results[\"status\"])\n",
        "        return results\n",
        "\n",
        "    # Generate SMA crossover signals\n",
        "    df['short_gt_long'] = df[short_sma_col_name] > df[long_sma_col_name]\n",
        "    df['prev_short_le_long'] = df[short_sma_col_name].shift(1) <= df[long_sma_col_name].shift(1)\n",
        "    df['buy_trigger_day'] = df['short_gt_long'] & df['prev_short_le_long']\n",
        "\n",
        "    df['short_lt_long'] = df[short_sma_col_name] < df[long_sma_col_name]\n",
        "    df['prev_short_ge_long'] = df[short_sma_col_name].shift(1) >= df[long_sma_col_name].shift(1)\n",
        "    df['potential_sell_trigger_day'] = df['short_lt_long'] & df['prev_short_ge_long']\n",
        "\n",
        "    current_capital = initial_capital\n",
        "    position_open = False\n",
        "    entry_price = 0.0\n",
        "    shares_held = 0.0\n",
        "    equity_over_time = pd.Series(index=df.index, dtype=float)\n",
        "    equity_over_time.iloc[0] = initial_capital # Start with initial capital\n",
        "\n",
        "    print(\"\\nSimulating trades...\")\n",
        "    for i in range(1, len(df)): # Start from the second day to allow for .shift(1)\n",
        "        current_date = df.index[i]\n",
        "        equity_over_time[current_date] = equity_over_time[df.index[i-1]] # Carry forward equity initially\n",
        "\n",
        "        if position_open:\n",
        "            # Update equity based on current price of shares held\n",
        "            equity_over_time[current_date] = shares_held * df['Close'].iloc[i] + (current_capital - (shares_held * entry_price * (1+commission_rate) if i == df.index.get_loc(entry_date)+1 else 0) ) # More precise would be cash + asset value\n",
        "            equity_over_time[current_date] = shares_held * df['Close'].iloc[i] # Value of asset holding\n",
        "            # If we also had cash, it would be equity_over_time[current_date] += cash_balance\n",
        "            # For simplicity in this strategy, assuming all capital is deployed or is in asset.\n",
        "            # Let's track equity as (shares_held * current_close_price) if in position, else cash.\n",
        "            # If not using margin, cash = current_capital - (shares_bought * entry_price_with_commission)\n",
        "            # This part gets complex for precise equity curve if not all capital is used.\n",
        "            # For simplicity, let's use a common backtesting approach:\n",
        "            # If in position, equity = shares * current_price. If not, equity = cash.\n",
        "\n",
        "        # Check for Potential Sell Signal\n",
        "        if position_open and df['potential_sell_trigger_day'].iloc[i]:\n",
        "            # Exit only if profitable (current open price > entry price)\n",
        "            # Sell on next day's open\n",
        "            potential_exit_price = df['Open'].iloc[i] # Sell on the open of the day signal occurred\n",
        "\n",
        "            # Check profitability (more accurately, current price vs entry price)\n",
        "            # For this example, let's check if current Close is profitable to simplify\n",
        "            # A real exit would use next bar's Open.\n",
        "            # For this logic: if a sell signal appears, and current *value* > entry *value*, then sell.\n",
        "            current_close_price = df['Close'].iloc[i] # Price at which sell signal is confirmed\n",
        "\n",
        "            # More precise: if we were to sell at next open, would it be profitable?\n",
        "            # For simplicity, let's assume we check profit at the *close* of the signal day,\n",
        "            # and if profitable, sell at that close.\n",
        "            if current_close_price > entry_price: # Profitable before commission\n",
        "                sell_value = shares_held * current_close_price\n",
        "                sell_value_after_commission = sell_value * (1 - commission_rate)\n",
        "\n",
        "                profit = sell_value_after_commission - (shares_held * entry_price * (1+commission_rate)) # This is not right\n",
        "                # Initial cost = shares_held * entry_price (ignoring commission impact on share count for now)\n",
        "                initial_cost_of_shares = shares_held * entry_price # The price paid per share\n",
        "\n",
        "                if current_close_price * (1 - commission_rate) > entry_price_with_commission: # Sell if net price > net entry price\n",
        "                    print(f\"{current_date.strftime('%Y-%m-%d')}: Potential Sell Signal. Current Close ({current_close_price:.2f}) > Entry ({entry_price:.2f}). Selling.\")\n",
        "                    current_capital = shares_held * current_close_price * (1 - commission_rate)\n",
        "                    results[\"trades\"].append({\n",
        "                        \"entry_date\": entry_date,\n",
        "                        \"entry_price\": entry_price,\n",
        "                        \"exit_date\": current_date,\n",
        "                        \"exit_price\": current_close_price,\n",
        "                        \"profit_pct\": ((current_close_price * (1-commission_rate)) / (entry_price_with_commission) - 1) * 100\n",
        "                    })\n",
        "                    if current_close_price > entry_price_with_commission : results[\"winning_trades\"] += 1\n",
        "                    position_open = False\n",
        "                    shares_held = 0.0\n",
        "                    entry_price = 0.0\n",
        "                    results[\"number_of_trades\"] += 1\n",
        "                else:\n",
        "                    print(f\"{current_date.strftime('%Y-%m-%d')}: Potential Sell Signal. But Current Close ({current_close_price:.2f}) not profitable vs Entry ({entry_price_with_commission:.2f} incl. commission). Holding.\")\n",
        "            else:\n",
        "                print(f\"{current_date.strftime('%Y-%m-%d')}: Potential Sell Signal. But Current Close ({current_close_price:.2f}) <= Entry ({entry_price:.2f}). Holding due to loss.\")\n",
        "\n",
        "        # Check for Buy Signal (only if not already in a position)\n",
        "        # Entry on next day's open\n",
        "        if not position_open and df['buy_trigger_day'].iloc[i-1]: # Signal on previous day's close\n",
        "            entry_date = current_date # Entry on current day's open\n",
        "            entry_price = df['Open'].iloc[i]\n",
        "            entry_price_with_commission = entry_price * (1 + commission_rate) # Effective entry price\n",
        "\n",
        "            shares_held = current_capital / entry_price_with_commission\n",
        "            # current_capital -= shares_held * entry_price_with_commission # This assumes all capital deployed\n",
        "            # For tracking equity if all in:\n",
        "            equity_over_time[current_date] = shares_held * df['Close'].iloc[i]\n",
        "\n",
        "            print(f\"{entry_date.strftime('%Y-%m-%d')}: Buy Signal. Entering long at {entry_price:.2f}. Shares: {shares_held:.4f}\")\n",
        "            position_open = True\n",
        "\n",
        "        # Update equity for the day\n",
        "        if position_open:\n",
        "            equity_over_time[current_date] = shares_held * df['Close'].iloc[i]\n",
        "        else:\n",
        "            equity_over_time[current_date] = current_capital # Cash if not in position\n",
        "\n",
        "    # If position is still open at the end, liquidate it\n",
        "    if position_open:\n",
        "        final_close_price = df['Close'].iloc[-1]\n",
        "        print(f\"End of data. Position still open. Liquidating at {df.index[-1].strftime('%Y-%m-%d')} Close: {final_close_price:.2f}\")\n",
        "\n",
        "        # Only sell if profitable at the very end\n",
        "        if final_close_price * (1-commission_rate) > entry_price_with_commission:\n",
        "            current_capital = shares_held * final_close_price * (1 - commission_rate)\n",
        "            results[\"trades\"].append({\n",
        "                \"entry_date\": entry_date,\n",
        "                \"entry_price\": entry_price,\n",
        "                \"exit_date\": df.index[-1],\n",
        "                \"exit_price\": final_close_price,\n",
        "                \"profit_pct\": ((final_close_price * (1-commission_rate)) / (entry_price_with_commission) - 1) * 100\n",
        "            })\n",
        "            if final_close_price > entry_price_with_commission: results[\"winning_trades\"] += 1\n",
        "            results[\"number_of_trades\"] += 1\n",
        "            print(\"  Sold at profit at end.\")\n",
        "        else:\n",
        "            # As per strategy \"never sell when there is a loss\", we might just value the holding.\n",
        "            # However, for backtesting, we usually liquidate. Let's assume if it's a loss at the end,\n",
        "            # for the purpose of this strategy, we are still \"holding\" that loss on paper.\n",
        "            # The final equity will reflect the value of shares.\n",
        "            current_capital = shares_held * final_close_price # Value of holding, no sell commission yet if not sold\n",
        "            print(f\"  Position held at a loss at end. Value: {current_capital:.2f} (Entry was {entry_price_with_commission:.2f} net)\")\n",
        "            # If we wanted to record this \"unrealized loss\" trade for stats:\n",
        "            # results[\"trades\"].append({ ... profit_pct would be negative ... })\n",
        "            # results[\"number_of_trades\"] += 1\n",
        "            # For this strategy, let's say an \"exit\" only happens on profit. So this doesn't count as a closed trade if at loss.\n",
        "\n",
        "    results[\"final_equity\"] = equity_over_time.iloc[-1]\n",
        "    results[\"return_percentage\"] = ((results[\"final_equity\"] - initial_capital) / initial_capital) * 100\n",
        "    results[\"max_drawdown_percentage\"] = calculate_max_drawdown(equity_over_time) * 100\n",
        "    results[\"status\"] = \"Simulation complete.\"\n",
        "\n",
        "    if not results[\"trades\"]:\n",
        "        results[\"status\"] += \" No profitable sell opportunities occurred based on SMA crossover signals.\"\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        print(\"Google Drive mounted successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error mounting Google Drive: {e}\")\n",
        "        raise RuntimeError(\"Google Drive could not be mounted. Script aborted.\")\n",
        "\n",
        "    GDRIVE_INDICATORS_PATH = \"/content/drive/MyDrive/CryptoDataCCXT/with_indicators/\"\n",
        "    file_to_process = \"BTC_USDT_1d_with_indicators.csv\"\n",
        "    full_file_path = os.path.join(GDRIVE_INDICATORS_PATH, file_to_process)\n",
        "\n",
        "    SHORT_SMA_COL_NAME = 'BTC_SMA_10'\n",
        "    LONG_SMA_COL_NAME = 'BTC_SMA_50'\n",
        "    INITIAL_CAPITAL = 100000.0\n",
        "    COMMISSION_RATE = 0.001\n",
        "\n",
        "    print(f\"\\n--- Simulating 'Buy on SMA Crossover, Sell ONLY for Profit' for: {file_to_process} ---\")\n",
        "\n",
        "    if not os.path.exists(full_file_path):\n",
        "        print(f\"ERROR: File not found at {full_file_path}. Cannot proceed.\")\n",
        "    else:\n",
        "        try:\n",
        "            df_loaded = pd.read_csv(full_file_path, index_col='datetime_utc', parse_dates=True)\n",
        "            print(f\"Successfully loaded data. Shape: {df_loaded.shape}\")\n",
        "\n",
        "            if df_loaded.empty:\n",
        "                print(\"ERROR: Loaded DataFrame is empty.\")\n",
        "            else:\n",
        "                strategy_performance = simulate_buy_on_signal_sell_on_profit(\n",
        "                    df_loaded,\n",
        "                    INITIAL_CAPITAL,\n",
        "                    SHORT_SMA_COL_NAME,\n",
        "                    LONG_SMA_COL_NAME,\n",
        "                    COMMISSION_RATE\n",
        "                )\n",
        "\n",
        "                print(\"\\n--- Strategy Performance ---\")\n",
        "                for key, value in strategy_performance.items():\n",
        "                    if key == \"trades\":\n",
        "                        print(f\"  {key.replace('_', ' ').title()}:\")\n",
        "                        if value:\n",
        "                            for i, trade in enumerate(value):\n",
        "                                print(f\"    Trade {i+1}: Entry {trade['entry_date'].strftime('%Y-%m-%d')} @ {trade['entry_price']:.2f} | Exit {trade['exit_date'].strftime('%Y-%m-%d')} @ {trade['exit_price']:.2f} | Profit: {trade['profit_pct']:.2f}%\")\n",
        "                        else:\n",
        "                            print(\"    No trades executed that met profit criteria.\")\n",
        "                    elif isinstance(value, float) and (\"percentage\" in key or \"equity\" in key or \"price\" in key or \"capital\" in key):\n",
        "                        print(f\"  {key.replace('_', ' ').title()}: {value:,.2f}\")\n",
        "                    else:\n",
        "                        print(f\"  {key.replace('_', ' ').title()}: {value}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during the simulation: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "    print(\"\\n--- Script complete ---\")"
      ],
      "metadata": {
        "id": "mQ143pB4jdta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "def calculate_max_drawdown(equity_series):\n",
        "    \"\"\"\n",
        "    Calculates the maximum drawdown from an equity series.\n",
        "    Args:\n",
        "        equity_series (pd.Series): A pandas Series representing the equity over time.\n",
        "    Returns:\n",
        "        float: The maximum drawdown as a negative percentage (e.g., -0.25 for -25%).\n",
        "               Returns 0.0 if no drawdown or insufficient data.\n",
        "    \"\"\"\n",
        "    if equity_series.empty or len(equity_series) < 2:\n",
        "        return 0.0\n",
        "    running_max = equity_series.cummax()\n",
        "    drawdown = (equity_series - running_max) / running_max\n",
        "    max_drawdown = drawdown.min()\n",
        "    return max_drawdown if pd.notna(max_drawdown) else 0.0\n",
        "\n",
        "def simulate_daily_dca_and_hold(df_input, daily_investment_amount, commission_rate=0.001):\n",
        "    \"\"\"\n",
        "    Simulates a daily Dollar-Cost Averaging (DCA) and Hold strategy.\n",
        "    \"\"\"\n",
        "    df = df_input.copy()\n",
        "\n",
        "    results = {\n",
        "        \"strategy_applied\": \"Daily Dollar-Cost Averaging and Hold\",\n",
        "        \"daily_investment_amount\": daily_investment_amount,\n",
        "        \"total_days\": len(df),\n",
        "        \"total_cash_invested\": 0.0,\n",
        "        \"total_units_held_final\": 0.0,\n",
        "        \"average_cost_per_unit\": 0.0,\n",
        "        \"final_portfolio_value_before_sell_commission\": 0.0,\n",
        "        \"final_portfolio_value_after_sell_commission\": 0.0,\n",
        "        \"net_profit\": 0.0,\n",
        "        \"return_on_investment_percentage\": 0.0,\n",
        "        \"max_drawdown_percentage_on_portfolio_value\": 0.0,\n",
        "        \"status\": \"Simulation not run yet.\"\n",
        "    }\n",
        "\n",
        "    required_cols = ['Open', 'Close']\n",
        "    if not all(col in df.columns for col in required_cols):\n",
        "        missing = [col for col in required_cols if col not in df.columns]\n",
        "        results[\"status\"] = f\"Error: Missing required columns: {missing}\"\n",
        "        print(results[\"status\"])\n",
        "        return results\n",
        "\n",
        "    if df.empty:\n",
        "        results[\"status\"] = \"Error: DataFrame is empty.\"\n",
        "        print(results[\"status\"])\n",
        "        return results\n",
        "\n",
        "    total_units_accumulated = 0.0\n",
        "    total_cash_invested = 0.0\n",
        "    portfolio_value_over_time = pd.Series(index=df.index, dtype=float)\n",
        "\n",
        "    print(\"\\nSimulating daily DCA purchases...\")\n",
        "    for i in range(len(df)):\n",
        "        current_date = df.index[i]\n",
        "        purchase_price = df['Open'].iloc[i] # Buy at Open\n",
        "\n",
        "        # Calculate units bought today\n",
        "        cash_for_purchase_after_commission = daily_investment_amount * (1 - commission_rate)\n",
        "        units_bought_today = cash_for_purchase_after_commission / purchase_price\n",
        "\n",
        "        total_units_accumulated += units_bought_today\n",
        "        total_cash_invested += daily_investment_amount # This is the actual cash outlaid\n",
        "\n",
        "        # Portfolio value at the end of this day (using Close price)\n",
        "        current_portfolio_value = total_units_accumulated * df['Close'].iloc[i]\n",
        "        portfolio_value_over_time[current_date] = current_portfolio_value\n",
        "\n",
        "        if (i + 1) % 365 == 0: # Print progress yearly\n",
        "            print(f\"Year { (i+1)//365 }: Invested ${total_cash_invested:,.2f}, Holding {total_units_accumulated:.4f} units, Value ${current_portfolio_value:,.2f}\")\n",
        "\n",
        "    results[\"total_cash_invested\"] = total_cash_invested\n",
        "    results[\"total_units_held_final\"] = total_units_accumulated\n",
        "\n",
        "    if total_units_accumulated > 0:\n",
        "        results[\"average_cost_per_unit\"] = total_cash_invested / total_units_accumulated\n",
        "\n",
        "    # --- Final Liquidation at the end of the period ---\n",
        "    final_close_price = df['Close'].iloc[-1]\n",
        "    results[\"final_portfolio_value_before_sell_commission\"] = total_units_accumulated * final_close_price\n",
        "    results[\"final_portfolio_value_after_sell_commission\"] = results[\"final_portfolio_value_before_sell_commission\"] * (1 - commission_rate)\n",
        "\n",
        "    results[\"net_profit\"] = results[\"final_portfolio_value_after_sell_commission\"] - total_cash_invested\n",
        "\n",
        "    if total_cash_invested > 0:\n",
        "        results[\"return_on_investment_percentage\"] = (results[\"net_profit\"] / total_cash_invested) * 100\n",
        "\n",
        "    if not portfolio_value_over_time.empty:\n",
        "        results[\"max_drawdown_percentage_on_portfolio_value\"] = calculate_max_drawdown(portfolio_value_over_time) * 100\n",
        "\n",
        "    results[\"status\"] = f\"Simulation complete. Held until {df.index[-1].strftime('%Y-%m-%d')}.\"\n",
        "    print(results[\"status\"])\n",
        "    return results\n",
        "\n",
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        print(\"Google Drive mounted successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error mounting Google Drive: {e}\")\n",
        "        raise RuntimeError(\"Google Drive could not be mounted. Script aborted.\")\n",
        "\n",
        "    GDRIVE_INDICATORS_PATH = \"/content/drive/MyDrive/CryptoDataCCXT/with_indicators/\"\n",
        "    # Using the file that already has indicators, but DCA only needs OHLC.\n",
        "    # We could also use the base CCXT output file (e.g., BTC_USDT_1d_4years.csv)\n",
        "    # This file should at least contain 'Open' and 'Close' columns with a DatetimeIndex.\n",
        "    file_to_process = \"BTC_USDT_1d_with_indicators.csv\"\n",
        "    full_file_path = os.path.join(GDRIVE_INDICATORS_PATH, file_to_process)\n",
        "\n",
        "    DAILY_INVESTMENT = 68.50 # To get approx $100k invested over ~4 years\n",
        "    COMMISSION_RATE = 0.001 # 0.1%\n",
        "\n",
        "    print(f\"\\n--- Simulating 'Daily DCA and Hold' for: {file_to_process} ---\")\n",
        "    print(f\"Daily Investment: ${DAILY_INVESTMENT:.2f}\")\n",
        "\n",
        "    if not os.path.exists(full_file_path):\n",
        "        print(f\"ERROR: File not found at {full_file_path}. Cannot proceed.\")\n",
        "    else:\n",
        "        try:\n",
        "            df_loaded = pd.read_csv(full_file_path, index_col='datetime_utc', parse_dates=True)\n",
        "            # If using the _with_indicators.csv, Open and Close are already uppercase.\n",
        "            # If using base CCXT output, ensure 'Open' and 'Close' columns exist or rename them.\n",
        "            # For this simulation, we assume 'Open' and 'Close' columns are present as needed.\n",
        "\n",
        "            print(f\"Successfully loaded data. Shape: {df_loaded.shape}. Data from {df_loaded.index.min()} to {df_loaded.index.max()}\")\n",
        "\n",
        "            if df_loaded.empty:\n",
        "                print(\"ERROR: Loaded DataFrame is empty.\")\n",
        "            else:\n",
        "                # Ensure 'Open' and 'Close' columns are available (they are in uppercase in _with_indicators files)\n",
        "                if not {'Open', 'Close'}.issubset(df_loaded.columns):\n",
        "                    # Attempt to rename if lowercase ohlc are present from a base file\n",
        "                    rename_map_dca = {'open': 'Open', 'close': 'Close'}\n",
        "                    cols_to_rename_dca = {k: v for k, v in rename_map_dca.items() if k in df_loaded.columns}\n",
        "                    if 'Open' not in df_loaded.columns and 'open' in cols_to_rename_dca : df_loaded.rename(columns={'open':'Open'}, inplace=True)\n",
        "                    if 'Close' not in df_loaded.columns and 'close' in cols_to_rename_dca : df_loaded.rename(columns={'close':'Close'}, inplace=True)\n",
        "\n",
        "                    if not {'Open', 'Close'}.issubset(df_loaded.columns):\n",
        "                        print(f\"ERROR: DataFrame must contain 'Open' and 'Close' columns. Found: {df_loaded.columns.tolist()}\")\n",
        "                        raise ValueError(\"Missing essential OHLC columns for DCA simulation.\")\n",
        "\n",
        "\n",
        "                dca_performance = simulate_daily_dca_and_hold(\n",
        "                    df_loaded,\n",
        "                    DAILY_INVESTMENT,\n",
        "                    COMMISSION_RATE\n",
        "                )\n",
        "\n",
        "                print(\"\\n--- Daily DCA and Hold Strategy Performance ---\")\n",
        "                for key, value in dca_performance.items():\n",
        "                    if isinstance(value, float) and (\"percentage\" in key or \"value\" in key or \"amount\" in key or \"cost\" in key or \"profit\" in key or \"invested\" in key):\n",
        "                        print(f\"  {key.replace('_', ' ').title()}: {value:,.2f}\")\n",
        "                    else:\n",
        "                        print(f\"  {key.replace('_', ' ').title()}: {value}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during the simulation: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "    print(\"\\n--- Script complete ---\")"
      ],
      "metadata": {
        "id": "Lv4bRd3zkm2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# --- Assume your indicator functions (add_sma, etc.) are defined ---\n",
        "# --- Assume your master_indicator_configs is defined ---\n",
        "\n",
        "def calculate_max_drawdown(equity_series):\n",
        "    if equity_series.empty or len(equity_series) < 2: return 0.0\n",
        "    running_max = equity_series.cummax()\n",
        "    drawdown = (equity_series - running_max) / running_max\n",
        "    max_drawdown = drawdown.min()\n",
        "    return max_drawdown if pd.notna(max_drawdown) else 0.0\n",
        "\n",
        "def simulate_signal_triggered_daily_dca(\n",
        "    df_input,\n",
        "    daily_allocation,\n",
        "    short_sma_col_name, # For SMA Crossover signal\n",
        "    long_sma_col_name,  # For SMA Crossover signal\n",
        "    commission_rate=0.001,\n",
        "    deploy_all_accumulated_cash_on_signal=True # If True, invests all waiting cash on signal\n",
        "):\n",
        "    df = df_input.copy()\n",
        "    results = {\n",
        "        \"strategy_applied\": \"Signal-Triggered Daily DCA (SMA Crossover Entry, Hold)\",\n",
        "        \"daily_allocation\": daily_allocation,\n",
        "        \"total_days_processed\": len(df),\n",
        "        \"total_cash_allocated_over_period\": len(df) * daily_allocation,\n",
        "        \"total_cash_actually_invested\": 0.0,\n",
        "        \"remaining_cash_at_end_if_not_all_invested\": 0.0,\n",
        "        \"total_units_held_final\": 0.0,\n",
        "        \"average_cost_per_unit_of_invested_cash\": 0.0,\n",
        "        \"final_portfolio_value_before_sell_commission\": 0.0,\n",
        "        \"final_portfolio_value_after_sell_commission\": 0.0,\n",
        "        \"net_profit_on_invested_cash\": 0.0,\n",
        "        \"return_on_invested_cash_percentage\": 0.0,\n",
        "        \"return_on_total_allocated_cash_percentage\": 0.0,\n",
        "        \"max_drawdown_percentage_on_portfolio_value\": 0.0,\n",
        "        \"number_of_buy_transactions\": 0,\n",
        "        \"buy_days\": [],\n",
        "        \"status\": \"Simulation not run yet.\"\n",
        "    }\n",
        "\n",
        "    required_cols = ['Open', 'Close', short_sma_col_name, long_sma_col_name]\n",
        "    if not all(col in df.columns for col in required_cols):\n",
        "        missing = [col for col in required_cols if col not in df.columns]\n",
        "        results[\"status\"] = f\"Error: Missing required columns: {missing}\"\n",
        "        print(results[\"status\"])\n",
        "        return results\n",
        "\n",
        "    # --- Generate Entry Signals (SMA Crossover) ---\n",
        "    df['short_gt_long'] = df[short_sma_col_name] > df[long_sma_col_name]\n",
        "    df['prev_short_le_long'] = df[short_sma_col_name].shift(1) <= df[long_sma_col_name].shift(1)\n",
        "    df['buy_signal_on_prev_close'] = df['short_gt_long'] & df['prev_short_le_long']\n",
        "    # We act on signal_on_prev_close by buying at current day's Open\n",
        "\n",
        "    accumulated_cash_waiting = 0.0\n",
        "    total_units_held = 0.0\n",
        "    total_cash_invested_into_asset = 0.0 # Tracks only cash that actually bought the asset\n",
        "\n",
        "    portfolio_value_over_time = pd.Series(index=df.index, dtype=float)\n",
        "    portfolio_value_over_time.iloc[0] = 0 # Start with 0 asset value, cash is separate\n",
        "\n",
        "    print(\"\\nSimulating Signal-Triggered Daily DCA...\")\n",
        "    for i in range(len(df)):\n",
        "        current_date = df.index[i]\n",
        "        accumulated_cash_waiting += daily_allocation # Add today's allocation\n",
        "\n",
        "        buy_signal_active_today = df['buy_signal_on_prev_close'].iloc[i] if i > 0 else False # Signal based on previous day's close\n",
        "\n",
        "        if buy_signal_active_today:\n",
        "            cash_to_invest_this_time = 0\n",
        "            if deploy_all_accumulated_cash_on_signal:\n",
        "                cash_to_invest_this_time = accumulated_cash_waiting\n",
        "            else: # Only invest current day's allocation on signal\n",
        "                cash_to_invest_this_time = daily_allocation\n",
        "                # The rest of accumulated_cash_waiting remains if this logic is chosen\n",
        "                # This part needs refinement if not deploying all. Let's stick to deploy_all_accumulated_cash_on_signal=True for now.\n",
        "\n",
        "            if cash_to_invest_this_time > 0:\n",
        "                purchase_price = df['Open'].iloc[i]\n",
        "                purchase_amount_after_commission = cash_to_invest_this_time * (1 - commission_rate)\n",
        "                units_bought_today = purchase_amount_after_commission / purchase_price\n",
        "\n",
        "                total_units_held += units_bought_today\n",
        "                total_cash_invested_into_asset += cash_to_invest_this_time # The amount taken from accumulated_cash_waiting\n",
        "                accumulated_cash_waiting -= cash_to_invest_this_time # Deduct invested amount\n",
        "\n",
        "                results[\"number_of_buy_transactions\"] += 1\n",
        "                results[\"buy_days\"].append(current_date)\n",
        "                print(f\"{current_date.strftime('%Y-%m-%d')}: BUY SIGNAL. Invested ${cash_to_invest_this_time:,.2f} at ${purchase_price:,.2f}. Units: {units_bought_today:.6f}. Total units: {total_units_held:.6f}\")\n",
        "\n",
        "        # Portfolio value at the end of this day\n",
        "        current_portfolio_value = total_units_held * df['Close'].iloc[i]\n",
        "        portfolio_value_over_time[current_date] = current_portfolio_value + accumulated_cash_waiting # Total net worth\n",
        "\n",
        "    results[\"total_cash_actually_invested\"] = total_cash_invested_into_asset\n",
        "    results[\"remaining_cash_at_end_if_not_all_invested\"] = accumulated_cash_waiting\n",
        "    results[\"total_units_held_final\"] = total_units_held\n",
        "\n",
        "    if total_units_held > 0 and total_cash_invested_into_asset > 0:\n",
        "        results[\"average_cost_per_unit_of_invested_cash\"] = total_cash_invested_into_asset / total_units_held\n",
        "\n",
        "    final_asset_value = total_units_held * df['Close'].iloc[-1]\n",
        "    results[\"final_portfolio_value_before_sell_commission\"] = final_asset_value + accumulated_cash_waiting\n",
        "\n",
        "    # Simulate selling all held assets at the end\n",
        "    final_asset_value_after_sell_commission = final_asset_value * (1 - commission_rate)\n",
        "    results[\"final_portfolio_value_after_sell_commission\"] = final_asset_value_after_sell_commission + accumulated_cash_waiting\n",
        "\n",
        "    results[\"net_profit_on_invested_cash\"] = (final_asset_value_after_sell_commission - total_cash_invested_into_asset)\n",
        "\n",
        "    if total_cash_invested_into_asset > 0:\n",
        "        results[\"return_on_invested_cash_percentage\"] = (results[\"net_profit_on_invested_cash\"] / total_cash_invested_into_asset) * 100\n",
        "\n",
        "    # ROI on total allocated cash (includes cash never invested if signals were rare)\n",
        "    net_profit_overall = results[\"final_portfolio_value_after_sell_commission\"] - results[\"total_cash_allocated_over_period\"]\n",
        "    if results[\"total_cash_allocated_over_period\"] > 0:\n",
        "        results[\"return_on_total_allocated_cash_percentage\"] = (net_profit_overall / results[\"total_cash_allocated_over_period\"]) * 100\n",
        "\n",
        "    if not portfolio_value_over_time.empty: # portfolio_value_over_time tracks asset value + waiting cash\n",
        "        results[\"max_drawdown_percentage_on_portfolio_value\"] = calculate_max_drawdown(portfolio_value_over_time) * 100\n",
        "\n",
        "    results[\"status\"] = f\"Simulation complete. Held until {df.index[-1].strftime('%Y-%m-%d')}.\"\n",
        "    print(results[\"status\"])\n",
        "    return results\n",
        "\n",
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        print(\"Google Drive mounted successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error mounting Google Drive: {e}\")\n",
        "        raise RuntimeError(\"Google Drive could not be mounted. Script aborted.\")\n",
        "\n",
        "    GDRIVE_INDICATORS_PATH = \"/content/drive/MyDrive/CryptoDataCCXT/with_indicators/\"\n",
        "    file_to_process = \"BTC_USDT_1d_with_indicators.csv\"\n",
        "    full_file_path = os.path.join(GDRIVE_INDICATORS_PATH, file_to_process)\n",
        "\n",
        "    DAILY_ALLOCATION = 200.00\n",
        "    SHORT_SMA_COL_NAME = 'BTC_SMA_10' # From your BTC master_config\n",
        "    LONG_SMA_COL_NAME = 'BTC_SMA_50'  # From your BTC master_config\n",
        "    COMMISSION_RATE = 0.001\n",
        "\n",
        "    print(f\"\\n--- Simulating 'Signal-Triggered Daily DCA' for: {file_to_process} ---\")\n",
        "    print(f\"Daily Allocation: ${DAILY_ALLOCATION:.2f}\")\n",
        "\n",
        "    if not os.path.exists(full_file_path):\n",
        "        print(f\"ERROR: File not found at {full_file_path}. Cannot proceed.\")\n",
        "    else:\n",
        "        try:\n",
        "            df_loaded = pd.read_csv(full_file_path, index_col='datetime_utc', parse_dates=True)\n",
        "            print(f\"Successfully loaded data. Shape: {df_loaded.shape}. Data from {df_loaded.index.min()} to {df_loaded.index.max()}\")\n",
        "\n",
        "            if df_loaded.empty:\n",
        "                print(\"ERROR: Loaded DataFrame is empty.\")\n",
        "            else:\n",
        "                # Ensure necessary columns exist\n",
        "                required_for_sim = ['Open', 'Close', SHORT_SMA_COL_NAME, LONG_SMA_COL_NAME]\n",
        "                if not all(col in df_loaded.columns for col in required_for_sim):\n",
        "                    raise ValueError(f\"DataFrame missing one or more required columns for simulation: {required_for_sim}. Found: {df_loaded.columns.tolist()}\")\n",
        "\n",
        "                dca_signal_performance = simulate_signal_triggered_daily_dca(\n",
        "                    df_loaded,\n",
        "                    DAILY_ALLOCATION,\n",
        "                    SHORT_SMA_COL_NAME,\n",
        "                    LONG_SMA_COL_NAME,\n",
        "                    COMMISSION_RATE,\n",
        "                    deploy_all_accumulated_cash_on_signal=True # Key parameter\n",
        "                )\n",
        "\n",
        "                print(\"\\n--- Signal-Triggered Daily DCA Performance ---\")\n",
        "                for key, value in dca_signal_performance.items():\n",
        "                    if key == \"buy_days\":\n",
        "                        print(f\"  {key.replace('_', ' ').title()}: {len(value)} days. First 5: {[d.strftime('%Y-%m-%d') for d in value[:5]]}\")\n",
        "                    elif isinstance(value, float) and (\"percentage\" in key or \"value\" in key or \"amount\" in key or \"cost\" in key or \"profit\" in key or \"invested\" in key or \"allocation\" in key):\n",
        "                        print(f\"  {key.replace('_', ' ').title()}: {value:,.2f}\")\n",
        "                    else:\n",
        "                        print(f\"  {key.replace('_', ' ').title()}: {value}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during the simulation: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "    print(\"\\n--- Script complete ---\")"
      ],
      "metadata": {
        "id": "0prXDf-Bma1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "def calculate_max_drawdown(equity_series):\n",
        "    if equity_series.empty or len(equity_series) < 2: return 0.0\n",
        "    running_max = equity_series.cummax()\n",
        "    drawdown = (equity_series - running_max) / running_max\n",
        "    max_drawdown = drawdown.min()\n",
        "    return max_drawdown if pd.notna(max_drawdown) else 0.0\n",
        "\n",
        "def simulate_dca_with_signal_logging(\n",
        "    df_input,\n",
        "    daily_investment_amount,\n",
        "    short_sma_col_name, # For SMA Crossover signal logging\n",
        "    long_sma_col_name,  # For SMA Crossover signal logging\n",
        "    commission_rate=0.001\n",
        "):\n",
        "    df = df_input.copy()\n",
        "    results = {\n",
        "        \"strategy_applied\": \"Daily DCA with SMA Crossover Signal Logging\",\n",
        "        \"daily_investment_amount\": daily_investment_amount,\n",
        "        \"total_days\": len(df),\n",
        "        \"total_cash_invested\": 0.0,\n",
        "        \"total_units_held_final\": 0.0,\n",
        "        \"average_cost_per_unit\": 0.0,\n",
        "        \"final_portfolio_value_before_sell_commission\": 0.0,\n",
        "        \"final_portfolio_value_after_sell_commission\": 0.0,\n",
        "        \"net_profit\": 0.0,\n",
        "        \"return_on_investment_percentage\": 0.0,\n",
        "        \"max_drawdown_percentage_on_portfolio_value\": 0.0,\n",
        "        \"days_with_buy_signal_when_investing\": 0,\n",
        "        \"signal_days_details\": [],\n",
        "        \"status\": \"Simulation not run yet.\"\n",
        "    }\n",
        "\n",
        "    required_cols = ['Open', 'Close', short_sma_col_name, long_sma_col_name]\n",
        "    if not all(col in df.columns for col in required_cols):\n",
        "        missing = [col for col in required_cols if col not in df.columns]\n",
        "        results[\"status\"] = f\"Error: Missing required columns: {missing}\"\n",
        "        print(results[\"status\"])\n",
        "        return results\n",
        "\n",
        "    if df.empty:\n",
        "        results[\"status\"] = \"Error: DataFrame is empty.\"\n",
        "        print(results[\"status\"])\n",
        "        return results\n",
        "\n",
        "    # --- Generate Entry Signals (for logging purposes) ---\n",
        "    df['short_gt_long'] = df[short_sma_col_name] > df[long_sma_col_name]\n",
        "    df['prev_short_le_long'] = df[short_sma_col_name].shift(1) <= df[long_sma_col_name].shift(1)\n",
        "    df['buy_signal_on_prev_close'] = df['short_gt_long'] & df['prev_short_le_long']\n",
        "\n",
        "    total_units_accumulated = 0.0\n",
        "    total_cash_invested = 0.0\n",
        "    portfolio_value_over_time = pd.Series(index=df.index, dtype=float)\n",
        "    portfolio_value_over_time.iloc[0] = 0 # Start with 0 asset value\n",
        "\n",
        "    print(\"\\nSimulating Daily DCA with signal logging...\")\n",
        "    for i in range(len(df)):\n",
        "        current_date = df.index[i]\n",
        "        purchase_price = df['Open'].iloc[i] # Buy at Open\n",
        "\n",
        "        cash_for_purchase_after_commission = daily_investment_amount * (1 - commission_rate)\n",
        "        units_bought_today = cash_for_purchase_after_commission / purchase_price\n",
        "\n",
        "        total_units_accumulated += units_bought_today\n",
        "        total_cash_invested += daily_investment_amount\n",
        "\n",
        "        current_portfolio_value = total_units_accumulated * df['Close'].iloc[i]\n",
        "        portfolio_value_over_time[current_date] = current_portfolio_value\n",
        "\n",
        "        # Log if a buy signal was active when this DCA purchase was made\n",
        "        signal_active_for_this_entry = df['buy_signal_on_prev_close'].iloc[i] if i > 0 else False\n",
        "        if signal_active_for_this_entry:\n",
        "            results[\"days_with_buy_signal_when_investing\"] += 1\n",
        "            results[\"signal_days_details\"].append({\n",
        "                \"date\": current_date,\n",
        "                \"price_bought\": purchase_price,\n",
        "                \"units_bought\": units_bought_today\n",
        "            })\n",
        "            # print(f\"{current_date.strftime('%Y-%m-%d')}: DCA Purchase. BUY SIGNAL was active. Price: ${purchase_price:,.2f}\")\n",
        "\n",
        "        if (i + 1) % 365 == 0: # Print progress yearly\n",
        "            print(f\"Year { (i+1)//365 }: Invested ${total_cash_invested:,.2f}, Holding {total_units_accumulated:.4f} units, Value ${current_portfolio_value:,.2f}. Signal days so far: {results['days_with_buy_signal_when_investing']}\")\n",
        "\n",
        "    results[\"total_cash_invested\"] = total_cash_invested\n",
        "    results[\"total_units_held_final\"] = total_units_accumulated\n",
        "\n",
        "    if total_units_accumulated > 0:\n",
        "        results[\"average_cost_per_unit\"] = total_cash_invested / total_units_accumulated\n",
        "\n",
        "    final_close_price = df['Close'].iloc[-1]\n",
        "    results[\"final_portfolio_value_before_sell_commission\"] = total_units_accumulated * final_close_price\n",
        "    results[\"final_portfolio_value_after_sell_commission\"] = results[\"final_portfolio_value_before_sell_commission\"] * (1 - commission_rate)\n",
        "\n",
        "    results[\"net_profit\"] = results[\"final_portfolio_value_after_sell_commission\"] - total_cash_invested\n",
        "\n",
        "    if total_cash_invested > 0:\n",
        "        results[\"return_on_investment_percentage\"] = (results[\"net_profit\"] / total_cash_invested) * 100\n",
        "\n",
        "    if not portfolio_value_over_time.empty:\n",
        "        results[\"max_drawdown_percentage_on_portfolio_value\"] = calculate_max_drawdown(portfolio_value_over_time) * 100\n",
        "\n",
        "    results[\"status\"] = f\"Simulation complete. Held until {df.index[-1].strftime('%Y-%m-%d')}.\"\n",
        "    print(results[\"status\"])\n",
        "    return results\n",
        "\n",
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        print(\"Google Drive mounted successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error mounting Google Drive: {e}\")\n",
        "        raise RuntimeError(\"Google Drive could not be mounted. Script aborted.\")\n",
        "\n",
        "    GDRIVE_INDICATORS_PATH = \"/content/drive/MyDrive/CryptoDataCCXT/with_indicators/\"\n",
        "    file_to_process = \"BTC_USDT_1d_with_indicators.csv\"\n",
        "    full_file_path = os.path.join(GDRIVE_INDICATORS_PATH, file_to_process)\n",
        "\n",
        "    DAILY_INVESTMENT_OPT_B = 200.00\n",
        "    SHORT_SMA_COL_NAME = 'BTC_SMA_10'\n",
        "    LONG_SMA_COL_NAME = 'BTC_SMA_50'\n",
        "    COMMISSION_RATE = 0.001\n",
        "\n",
        "    print(f\"\\n--- Simulating Option B: 'Daily DCA ($200) with SMA Crossover Signal Logging' for: {file_to_process} ---\")\n",
        "\n",
        "    if not os.path.exists(full_file_path):\n",
        "        print(f\"ERROR: File not found at {full_file_path}. Cannot proceed.\")\n",
        "    else:\n",
        "        try:\n",
        "            df_loaded = pd.read_csv(full_file_path, index_col='datetime_utc', parse_dates=True)\n",
        "            print(f\"Successfully loaded data. Shape: {df_loaded.shape}. Data from {df_loaded.index.min()} to {df_loaded.index.max()}\")\n",
        "\n",
        "            if df_loaded.empty:\n",
        "                print(\"ERROR: Loaded DataFrame is empty.\")\n",
        "            else:\n",
        "                if not {'Open', 'Close', SHORT_SMA_COL_NAME, LONG_SMA_COL_NAME}.issubset(df_loaded.columns):\n",
        "                    raise ValueError(f\"DataFrame missing one or more required columns. Needed: Open, Close, {SHORT_SMA_COL_NAME}, {LONG_SMA_COL_NAME}. Found: {df_loaded.columns.tolist()}\")\n",
        "\n",
        "                dca_option_b_performance = simulate_dca_with_signal_logging(\n",
        "                    df_loaded,\n",
        "                    DAILY_INVESTMENT_OPT_B,\n",
        "                    SHORT_SMA_COL_NAME,\n",
        "                    LONG_SMA_COL_NAME,\n",
        "                    COMMISSION_RATE\n",
        "                )\n",
        "\n",
        "                print(\"\\n--- Option B: Daily DCA with Signal Logging Performance ---\")\n",
        "                for key, value in dca_option_b_performance.items():\n",
        "                    if key == \"signal_days_details\":\n",
        "                        print(f\"  {key.replace('_', ' ').title()}: First 5 signal-coincident purchases:\")\n",
        "                        for item in value[:5]:\n",
        "                            print(f\"    Date: {item['date'].strftime('%Y-%m-%d')}, Price: {item['price_bought']:.2f}, Units: {item['units_bought']:.6f}\")\n",
        "                    elif isinstance(value, float) and (\"percentage\" in key or \"value\" in key or \"amount\" in key or \"cost\" in key or \"profit\" in key or \"invested\" in key):\n",
        "                        print(f\"  {key.replace('_', ' ').title()}: {value:,.2f}\")\n",
        "                    else:\n",
        "                        print(f\"  {key.replace('_', ' ').title()}: {value}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during the simulation: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "    print(\"\\n--- Script complete ---\")"
      ],
      "metadata": {
        "id": "DT09uIFrmsqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# --- Assume your indicator functions (add_sma, etc.) are defined ---\n",
        "# --- Assume your master_indicator_configs is defined ---\n",
        "\n",
        "def calculate_max_drawdown(equity_series):\n",
        "    if equity_series.empty or len(equity_series) < 2: return 0.0\n",
        "    running_max = equity_series.cummax()\n",
        "    drawdown = (equity_series - running_max) / running_max\n",
        "    max_drawdown = drawdown.min()\n",
        "    return max_drawdown if pd.notna(max_drawdown) else 0.0\n",
        "\n",
        "def simulate_option_c_dca(\n",
        "    df_primary_asset, # DataFrame for BTC with its indicators\n",
        "    daily_allocation,\n",
        "    primary_asset_symbol, # e.g., \"BTC\"\n",
        "    short_sma_col_name,\n",
        "    long_sma_col_name,\n",
        "    commission_rate_primary=0.001,\n",
        "    # For simplicity, let's assume uninvested cash doesn't earn interest in this example,\n",
        "    # but one could add a small daily interest accrual to 'cash_pool'.\n",
        "    # alternative_asset_daily_interest_rate=0.00005 # e.g., ~1.8% APR / 365\n",
        "    deploy_all_accumulated_cash_on_signal=True\n",
        "):\n",
        "    df = df_primary_asset.copy()\n",
        "    results = {\n",
        "        \"strategy_applied\": \"Option C: Daily Allocation, Buy Primary on Signal, Else Hold Cash\",\n",
        "        \"daily_allocation\": daily_allocation,\n",
        "        \"total_days_processed\": len(df),\n",
        "        \"total_cash_allocated_over_period\": len(df) * daily_allocation,\n",
        "        \"total_cash_invested_in_primary\": 0.0,\n",
        "        \"cash_pool_at_end\": 0.0,\n",
        "        \"total_primary_asset_units_final\": 0.0,\n",
        "        \"average_cost_per_primary_unit\": 0.0,\n",
        "        \"final_portfolio_value_before_sell_commission\": 0.0, # Value of primary asset + cash pool\n",
        "        \"final_portfolio_value_after_sell_commission\": 0.0,\n",
        "        \"net_profit_overall\": 0.0,\n",
        "        \"return_on_total_allocated_cash_percentage\": 0.0,\n",
        "        \"max_drawdown_percentage_on_portfolio_value\": 0.0,\n",
        "        \"number_of_primary_asset_buys\": 0,\n",
        "        \"buy_days_primary_asset\": [],\n",
        "        \"status\": \"Simulation not run yet.\"\n",
        "    }\n",
        "\n",
        "    required_cols = ['Open', 'Close', short_sma_col_name, long_sma_col_name]\n",
        "    if not all(col in df.columns for col in required_cols):\n",
        "        missing = [col for col in required_cols if col not in df.columns]\n",
        "        results[\"status\"] = f\"Error: Missing required columns in primary asset DataFrame: {missing}\"\n",
        "        print(results[\"status\"])\n",
        "        return results\n",
        "\n",
        "    # --- Generate Entry Signals for Primary Asset (e.g., BTC SMA Crossover) ---\n",
        "    df['short_gt_long'] = df[short_sma_col_name] > df[long_sma_col_name]\n",
        "    df['prev_short_le_long'] = df[short_sma_col_name].shift(1) <= df[long_sma_col_name].shift(1)\n",
        "    df['buy_signal_on_prev_close_primary'] = df['short_gt_long'] & df['prev_short_le_long']\n",
        "\n",
        "    cash_pool = 0.0 # Cash waiting to be invested in primary asset\n",
        "    primary_asset_units_held = 0.0\n",
        "    total_cash_invested_in_primary_asset = 0.0\n",
        "\n",
        "    portfolio_value_over_time = pd.Series(index=df.index, dtype=float)\n",
        "    portfolio_value_over_time.iloc[0] = daily_allocation # First day's allocation becomes initial cash\n",
        "\n",
        "    print(\"\\nSimulating Option C: Signal-Triggered DCA for Primary, Cash Accumulation Else...\")\n",
        "    for i in range(len(df)):\n",
        "        current_date = df.index[i]\n",
        "        cash_pool += daily_allocation # Add today's allocation to the pool\n",
        "\n",
        "        # If not the first day, carry forward previous day's asset value for portfolio tracking before today's action\n",
        "        current_primary_asset_value = primary_asset_units_held * df['Close'].iloc[i-1] if i > 0 else 0\n",
        "        portfolio_value_over_time[current_date] = current_primary_asset_value + cash_pool\n",
        "\n",
        "\n",
        "        buy_signal_active_today_primary = df['buy_signal_on_prev_close_primary'].iloc[i] if i > 0 else False\n",
        "\n",
        "        if buy_signal_active_today_primary:\n",
        "            cash_to_invest_this_time = 0\n",
        "            if deploy_all_accumulated_cash_on_signal:\n",
        "                cash_to_invest_this_time = cash_pool # Invest all available cash\n",
        "            else: # Only invest current day's allocation (more complex cash pool management)\n",
        "                cash_to_invest_this_time = min(daily_allocation, cash_pool)\n",
        "\n",
        "            if cash_to_invest_this_time > 1: # Check if there's meaningful cash to invest\n",
        "                purchase_price_primary = df['Open'].iloc[i]\n",
        "                purchase_amount_after_commission = cash_to_invest_this_time * (1 - commission_rate_primary)\n",
        "                units_bought_today_primary = purchase_amount_after_commission / purchase_price_primary\n",
        "\n",
        "                primary_asset_units_held += units_bought_today_primary\n",
        "                total_cash_invested_in_primary_asset += cash_to_invest_this_time\n",
        "                cash_pool -= cash_to_invest_this_time # Deduct invested amount\n",
        "\n",
        "                results[\"number_of_primary_asset_buys\"] += 1\n",
        "                results[\"buy_days_primary_asset\"].append(current_date)\n",
        "                print(f\"{current_date.strftime('%Y-%m-%d')}: PRIMARY ASSET ({primary_asset_symbol}) BUY SIGNAL. Invested ${cash_to_invest_this_time:,.2f} at ${purchase_price_primary:,.2f}. Units: {units_bought_today_primary:.6f}. Cash pool: ${cash_pool:,.2f}\")\n",
        "\n",
        "        # Update portfolio value at end of day\n",
        "        current_primary_asset_value = primary_asset_units_held * df['Close'].iloc[i]\n",
        "        portfolio_value_over_time[current_date] = current_primary_asset_value + cash_pool\n",
        "\n",
        "        if (i + 1) % 365 == 0:\n",
        "             print(f\"Year {(i+1)//365}: Primary Units: {primary_asset_units_held:.4f}, Cash Pool: ${cash_pool:,.2f}, Portfolio Value: ${portfolio_value_over_time[current_date]:,.2f}\")\n",
        "\n",
        "\n",
        "    results[\"total_cash_actually_invested_in_primary\"] = total_cash_invested_in_primary_asset\n",
        "    results[\"cash_pool_at_end\"] = cash_pool\n",
        "    results[\"total_primary_asset_units_final\"] = primary_asset_units_held\n",
        "\n",
        "    if primary_asset_units_held > 0 and total_cash_invested_in_primary_asset > 0:\n",
        "        results[\"average_cost_per_primary_unit\"] = total_cash_invested_in_primary_asset / primary_asset_units_held\n",
        "\n",
        "    # Final portfolio value = value of primary asset held + remaining cash pool\n",
        "    final_primary_asset_value = primary_asset_units_held * df['Close'].iloc[-1]\n",
        "    results[\"final_portfolio_value_before_sell_commission\"] = final_primary_asset_value + cash_pool\n",
        "\n",
        "    # Simulate selling all held primary assets at the end\n",
        "    final_primary_asset_value_after_sell_commission = final_primary_asset_value * (1 - commission_rate_primary)\n",
        "    results[\"final_portfolio_value_after_sell_commission\"] = final_primary_asset_value_after_sell_commission + cash_pool # Add back cash pool\n",
        "\n",
        "    # Net profit based on total cash allocated over the period\n",
        "    results[\"net_profit_overall\"] = results[\"final_portfolio_value_after_sell_commission\"] - results[\"total_cash_allocated_over_period\"]\n",
        "\n",
        "    if results[\"total_cash_allocated_over_period\"] > 0:\n",
        "        results[\"return_on_total_allocated_cash_percentage\"] = (results[\"net_profit_overall\"] / results[\"total_cash_allocated_over_period\"]) * 100\n",
        "\n",
        "    if not portfolio_value_over_time.empty:\n",
        "        results[\"max_drawdown_percentage_on_portfolio_value\"] = calculate_max_drawdown(portfolio_value_over_time) * 100\n",
        "\n",
        "    results[\"status\"] = f\"Simulation complete. Held until {df.index[-1].strftime('%Y-%m-%d')}.\"\n",
        "    print(results[\"status\"])\n",
        "    return results\n",
        "\n",
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        print(\"Google Drive mounted successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error mounting Google Drive: {e}\")\n",
        "        raise RuntimeError(\"Google Drive could not be mounted. Script aborted.\")\n",
        "\n",
        "    GDRIVE_INDICATORS_PATH = \"/content/drive/MyDrive/CryptoDataCCXT/with_indicators/\"\n",
        "    file_to_process_btc = \"BTC_USDT_1d_with_indicators.csv\" # Primary asset data\n",
        "    full_file_path_btc = os.path.join(GDRIVE_INDICATORS_PATH, file_to_process_btc)\n",
        "\n",
        "    DAILY_ALLOCATION_OPT_C = 200.00\n",
        "    # For BTC (Primary Asset)\n",
        "    PRIMARY_ASSET_SYMBOL = \"BTC\"\n",
        "    BTC_SHORT_SMA_COL_NAME = 'BTC_SMA_10'\n",
        "    BTC_LONG_SMA_COL_NAME = 'BTC_SMA_50'\n",
        "    COMMISSION_RATE = 0.001\n",
        "\n",
        "    print(f\"\\n--- Simulating Option C for Primary Asset: {PRIMARY_ASSET_SYMBOL} from file: {file_to_process_btc} ---\")\n",
        "    print(f\"Daily Allocation: ${DAILY_ALLOCATION_OPT_C:.2f}\")\n",
        "\n",
        "    if not os.path.exists(full_file_path_btc):\n",
        "        print(f\"ERROR: Primary asset data file not found at {full_file_path_btc}. Cannot proceed.\")\n",
        "    else:\n",
        "        try:\n",
        "            df_btc_loaded = pd.read_csv(full_file_path_btc, index_col='datetime_utc', parse_dates=True)\n",
        "            print(f\"Successfully loaded BTC data. Shape: {df_btc_loaded.shape}. Data from {df_btc_loaded.index.min()} to {df_btc_loaded.index.max()}\")\n",
        "\n",
        "            if df_btc_loaded.empty:\n",
        "                print(\"ERROR: Loaded BTC DataFrame is empty.\")\n",
        "            else:\n",
        "                required_for_sim_btc = ['Open', 'Close', BTC_SHORT_SMA_COL_NAME, BTC_LONG_SMA_COL_NAME]\n",
        "                if not all(col in df_btc_loaded.columns for col in required_for_sim_btc):\n",
        "                    raise ValueError(f\"BTC DataFrame missing one or more required columns for simulation: {required_for_sim_btc}. Found: {df_btc_loaded.columns.tolist()}\")\n",
        "\n",
        "                option_c_performance = simulate_option_c_dca(\n",
        "                    df_btc_loaded,\n",
        "                    DAILY_ALLOCATION_OPT_C,\n",
        "                    PRIMARY_ASSET_SYMBOL,\n",
        "                    BTC_SHORT_SMA_COL_NAME,\n",
        "                    BTC_LONG_SMA_COL_NAME,\n",
        "                    COMMISSION_RATE,\n",
        "                    deploy_all_accumulated_cash_on_signal=True\n",
        "                )\n",
        "\n",
        "                print(\"\\n--- Option C: Signal-Triggered DCA for Primary, Cash Accumulation Else ---\")\n",
        "                for key, value in option_c_performance.items():\n",
        "                    if key == \"buy_days_primary_asset\":\n",
        "                        print(f\"  {key.replace('_', ' ').title()}: {len(value)} days. First 5: {[d.strftime('%Y-%m-%d') for d in value[:5]]}\")\n",
        "                    elif isinstance(value, float) and any(sub in key for sub in [\"percentage\", \"value\", \"amount\", \"cost\", \"profit\", \"invested\", \"allocation\", \"pool\"]):\n",
        "                        print(f\"  {key.replace('_', ' ').title()}: {value:,.2f}\")\n",
        "                    else:\n",
        "                        print(f\"  {key.replace('_', ' ').title()}: {value}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during the simulation: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "    print(\"\\n--- Script complete ---\")"
      ],
      "metadata": {
        "id": "axWEGV2EnJU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# --- Step 1: Mount Google Drive ---\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error mounting Google Drive: {e}\")\n",
        "    raise RuntimeError(\"Google Drive could not be mounted. Script aborted.\")\n",
        "\n",
        "# --- Step 2: Define Master Indicator Configs (or ensure it's loaded from a previous cell) ---\n",
        "# IMPORTANT: This script assumes 'master_indicator_configs' is already defined in your notebook.\n",
        "# For this example, we'll use a placeholder to dynamically get column names.\n",
        "# Replace this with your actual master_indicator_configs dictionary.\n",
        "master_indicator_configs = {\n",
        "    \"BTC\": {\n",
        "        \"ema_short\": {\"span\": 12, \"column\": \"Close\"}, # Example: Fast EMA\n",
        "        \"ema_long\":  {\"span\": 26, \"column\": \"Close\"}, # Example: Slow EMA\n",
        "        \"rsi\":       {\"window\": 14, \"column\": \"Close\"},\n",
        "        # Add other indicators if your strategy needs them directly for signals\n",
        "    },\n",
        "    # Add ETH, SOL configs if you adapt this script for them\n",
        "}\n",
        "\n",
        "\n",
        "# --- Step 3: Configuration for the Hourly Strategy ---\n",
        "GDRIVE_INDICATORS_PATH = \"/content/drive/MyDrive/CryptoDataCCXT/with_indicators/\"\n",
        "PAIR_SYMBOL = \"BTC_USDT\"\n",
        "TIMEFRAME = \"1h\" # Change to \"15m\" to adapt for 15-minute data\n",
        "FILE_TO_PROCESS = f\"{PAIR_SYMBOL}_{TIMEFRAME}_with_indicators.csv\"\n",
        "full_file_path = os.path.join(GDRIVE_INDICATORS_PATH, FILE_TO_PROCESS)\n",
        "\n",
        "BASE_COIN_SYMBOL = PAIR_SYMBOL.split('_')[0] # e.g., \"BTC\"\n",
        "\n",
        "# Dynamically get indicator column names based on master_indicator_configs for BTC\n",
        "# This makes the script more robust if you change parameters in your config\n",
        "try:\n",
        "    btc_config = master_indicator_configs[BASE_COIN_SYMBOL]\n",
        "    FAST_EMA_COL = f\"{BASE_COIN_SYMBOL}_EMA_{btc_config['ema_short']['span']}\"\n",
        "    SLOW_EMA_COL = f\"{BASE_COIN_SYMBOL}_EMA_{btc_config['ema_long']['span']}\"\n",
        "    RSI_COL = f\"RSI_{btc_config['rsi']['window']}\" # Assuming your add_rsi names it like RSI_WINDOW\n",
        "except KeyError as e:\n",
        "    print(f\"ERROR: Key {e} not found in master_indicator_configs for {BASE_COIN_SYMBOL}.\")\n",
        "    print(\"Please ensure master_indicator_configs is correctly defined and includes ema_short, ema_long, and rsi for BTC.\")\n",
        "    # Fallback to common defaults if config is missing for some reason (not recommended for production)\n",
        "    # FAST_EMA_COL = f\"{BASE_COIN_SYMBOL}_EMA_12\"\n",
        "    # SLOW_EMA_COL = f\"{BASE_COIN_SYMBOL}_EMA_26\"\n",
        "    # RSI_COL = f\"RSI_14\"\n",
        "    # print(f\"Warning: Using default column names: {FAST_EMA_COL}, {SLOW_EMA_COL}, {RSI_COL}\")\n",
        "    raise # Stop execution if config is missing\n",
        "\n",
        "RSI_THRESHOLD = 50 # RSI must be above this for a buy signal\n",
        "\n",
        "print(f\"\\n--- Developing Hourly Strategy for: {full_file_path} ---\")\n",
        "print(f\"Using Fast EMA: {FAST_EMA_COL}, Slow EMA: {SLOW_EMA_COL}, RSI: {RSI_COL} (threshold > {RSI_THRESHOLD})\")\n",
        "\n",
        "if not os.path.exists(full_file_path):\n",
        "    print(f\"ERROR: File not found at {full_file_path}. Cannot proceed.\")\n",
        "else:\n",
        "    try:\n",
        "        df = pd.read_csv(full_file_path, index_col='datetime_utc', parse_dates=True)\n",
        "        print(f\"\\nSuccessfully loaded data. Shape: {df.shape}\")\n",
        "        print(\"First 3 rows of loaded data:\")\n",
        "        pd.set_option('display.max_columns', None)\n",
        "        print(df.head(3).T)\n",
        "        pd.reset_option('display.max_columns')\n",
        "\n",
        "\n",
        "        # --- Step 4: Ensure Required Indicator Columns Exist ---\n",
        "        required_indicator_cols = [FAST_EMA_COL, SLOW_EMA_COL, RSI_COL, 'Close'] # Need 'Close' for context\n",
        "        missing_cols = [col for col in required_indicator_cols if col not in df.columns]\n",
        "        if missing_cols:\n",
        "            print(f\"\\nERROR: DataFrame is missing one or more required indicator columns for the strategy: {missing_cols}\")\n",
        "            print(f\"Available columns: {df.columns.tolist()}\")\n",
        "            # Terminate or handle error appropriately\n",
        "            raise ValueError(\"Missing required indicator columns for strategy.\")\n",
        "        else:\n",
        "            print(f\"\\nAll required indicator columns found: {required_indicator_cols}\")\n",
        "\n",
        "            # --- Step 5: Generate Trading Signals ---\n",
        "            df['signal'] = 0 # 0: Hold, 1: Buy (Enter Long), -1: Sell (Exit Long)\n",
        "\n",
        "            # Condition for Fast EMA > Slow EMA\n",
        "            df['fast_ema_gt_slow_ema'] = df[FAST_EMA_COL] > df[SLOW_EMA_COL]\n",
        "            # Condition for RSI > Threshold\n",
        "            df['rsi_gt_threshold'] = df[RSI_COL] > RSI_THRESHOLD\n",
        "\n",
        "            # Buy Signal: Fast EMA just crossed above Slow EMA AND RSI is above threshold\n",
        "            df['buy_trigger'] = (df['fast_ema_gt_slow_ema'] &\n",
        "                                 (df['fast_ema_gt_slow_ema'].shift(1) == False) &\n",
        "                                 df['rsi_gt_threshold'])\n",
        "            df.loc[df['buy_trigger'], 'signal'] = 1\n",
        "\n",
        "            # Sell Signal (Exit Long): Fast EMA just crossed below Slow EMA\n",
        "            df['sell_trigger'] = ((df['fast_ema_gt_slow_ema'] == False) &\n",
        "                                  (df['fast_ema_gt_slow_ema'].shift(1)))\n",
        "            df.loc[df['sell_trigger'], 'signal'] = -1\n",
        "\n",
        "            # Clean up temporary columns\n",
        "            df.drop(columns=['fast_ema_gt_slow_ema', 'rsi_gt_threshold', 'buy_trigger', 'sell_trigger'], inplace=True)\n",
        "\n",
        "            # --- Step 6: Display Signal Results ---\n",
        "            print(\"\\n--- Hourly Strategy Signals Generated ---\")\n",
        "            signals_generated_df = df[df['signal'] != 0]\n",
        "            print(f\"Number of buy signals: {len(df[df['signal'] == 1])}\")\n",
        "            print(f\"Number of sell signals (exit long): {len(df[df['signal'] == -1])}\")\n",
        "\n",
        "            if not signals_generated_df.empty:\n",
        "                print(\"\\nFirst 10 signals (Buy=1, Sell=-1):\")\n",
        "                print(signals_generated_df[['Close', FAST_EMA_COL, SLOW_EMA_COL, RSI_COL, 'signal']].head(10))\n",
        "                print(\"\\nLast 10 signals:\")\n",
        "                print(signals_generated_df[['Close', FAST_EMA_COL, SLOW_EMA_COL, RSI_COL, 'signal']].tail(10))\n",
        "            else:\n",
        "                print(\"No trade signals were generated by this strategy.\")\n",
        "\n",
        "            # For a full backtest, you would now simulate trades based on these signals,\n",
        "            # applying position sizing, commissions, stop-losses, and take-profits.\n",
        "            print(\"\\nReminder: This script generates signals. Full backtesting would simulate trades and P&L.\")\n",
        "            print(\"Consider adding ATR-based stop-losses and take-profits for a more complete hourly strategy.\")\n",
        "\n",
        "    except ValueError as ve:\n",
        "        print(ve) # Print specific ValueError from column check\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "print(\"\\n--- Hourly strategy development script segment complete ---\")"
      ],
      "metadata": {
        "id": "MYJhwTWhqcSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FjGl0yfBraZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PASTE YOUR master_indicator_configs DICTIONARY HERE\n",
        "master_indicator_configs = {\n",
        "    \"BTC\": {\n",
        "        \"sma_short\":    {\"window\": 10, \"column\": \"Close\"},\n",
        "        \"sma_long\":     {\"window\": 50, \"column\": \"Close\"},\n",
        "        \"ema_short\":    {\"span\": 12, \"column\": \"Close\"},\n",
        "        \"ema_long\":     {\"span\": 26, \"column\": \"Close\"},\n",
        "        \"rsi\":          {\"window\": 14, \"column\": \"Close\"},\n",
        "        \"bollinger\":    {\"window\": 20, \"num_std_dev\": 2, \"column\": \"Close\"},\n",
        "        \"macd\":         {\"short_window\": 12, \"long_window\": 26, \"signal_window\": 9, \"column\": \"Close\"},\n",
        "        \"atr\":          {\"window\": 14}, # Assumes High, Low, Close columns\n",
        "        \"stochastic\":   {\"k_window\": 14, \"d_window\": 3}, # Assumes High, Low, Close\n",
        "        \"mfi\":          {\"window\": 14}, # Assumes High, Low, Close, Volume\n",
        "        \"roc\":          {\"window\": 10, \"column\": \"Close\"},\n",
        "        \"keltner\":      {\"ema_window\": 20, \"atr_window\": 10, \"atr_multiplier\": 2}, # Assumes High, Low, Close\n",
        "        \"volume_sma\":   {\"window\": 20}, # Assumes Volume column\n",
        "        \"support_resistance\": {\"window\": 15} # Assumes High, Low columns\n",
        "    },\n",
        "    \"ETH\": { # Ensure ETH config is complete if you process ETH\n",
        "        \"sma_short\":    {\"window\": 12, \"column\": \"Close\"},\n",
        "        \"sma_long\":     {\"window\": 55, \"column\": \"Close\"},\n",
        "        \"ema_short\":    {\"span\": 10, \"column\": \"Close\"},\n",
        "        \"ema_long\":     {\"span\": 30, \"column\": \"Close\"},\n",
        "        \"rsi\":          {\"window\": 14, \"column\": \"Close\"},\n",
        "        \"bollinger\":    {\"window\": 20, \"num_std_dev\": 2.1, \"column\": \"Close\"},\n",
        "        \"macd\":         {\"short_window\": 12, \"long_window\": 26, \"signal_window\": 9, \"column\": \"Close\"},\n",
        "        \"atr\":          {\"window\": 10},\n",
        "        \"stochastic\":   {\"k_window\": 10, \"d_window\": 3},\n",
        "        \"mfi\":          {\"window\": 12},\n",
        "        \"roc\":          {\"window\": 12, \"column\": \"Close\"},\n",
        "        \"keltner\":      {\"ema_window\": 22, \"atr_window\": 11, \"atr_multiplier\": 2},\n",
        "        \"volume_sma\":   {\"window\": 25},\n",
        "        \"support_resistance\": {\"window\": 20}\n",
        "    },\n",
        "    \"SOL\": { # Ensure SOL config is complete\n",
        "        \"sma_short\":    {\"window\": 9, \"column\": \"Close\"},\n",
        "        \"sma_long\":     {\"window\": 45, \"column\": \"Close\"},\n",
        "        \"ema_short\":    {\"span\": 12, \"column\": \"Close\"},\n",
        "        \"ema_long\":     {\"span\": 26, \"column\": \"Close\"},\n",
        "        \"rsi\":          {\"window\": 14, \"column\": \"Close\"},\n",
        "        \"bollinger\":    {\"window\": 20, \"num_std_dev\": 2, \"column\": \"Close\"},\n",
        "        \"macd\":         {\"short_window\": 12, \"long_window\": 26, \"signal_window\": 9, \"column\": \"Close\"},\n",
        "        \"atr\":          {\"window\": 14},\n",
        "        \"stochastic\":   {\"k_window\": 14, \"d_window\": 3},\n",
        "        \"mfi\":          {\"window\": 14},\n",
        "        \"roc\":          {\"window\": 10, \"column\": \"Close\"},\n",
        "        \"keltner\":      {\"ema_window\": 20, \"atr_window\": 10, \"atr_multiplier\": 1.8},\n",
        "        \"volume_sma\":   {\"window\": 20},\n",
        "        \"support_resistance\": {\"window\": 15}\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "jDMBfJJfvPna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IpZFguX2vPbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PASTE ALL 13 add_... INDICATOR FUNCTION DEFINITIONS HERE\n",
        "# (add_sma, add_ema, add_rsi, add_bollinger_bands, add_macd, add_atr,\n",
        "#  add_stochastic_oscillator, add_mfi, add_roc, add_keltner_channels,\n",
        "#  add_volume_sma, add_basic_support_resistance)\n",
        "# ... ensure numpy is imported within them or globally (import numpy as np)\n",
        "import numpy as np # Make sure numpy is imported if your functions use it directly\n",
        "import pandas as pd # Should already be imported, but good practice\n",
        "\n",
        "def add_sma(df, window=20, column='Close', new_column_name=None):\n",
        "    if new_column_name is None: new_column_name = f'SMA_{window}'\n",
        "    df[new_column_name] = df[column].rolling(window=window, min_periods=1).mean()\n",
        "    return df\n",
        "\n",
        "def add_ema(df, span=20, column='Close', new_column_name=None):\n",
        "    if new_column_name is None: new_column_name = f'EMA_{span}'\n",
        "    df[new_column_name] = df[column].ewm(span=span, adjust=False, min_periods=1).mean()\n",
        "    return df\n",
        "\n",
        "def add_rsi(df, window=14, column='Close', new_column_name=None):\n",
        "    if new_column_name is None: new_column_name = f'RSI_{window}'\n",
        "    delta = df[column].diff(1)\n",
        "    gain = delta.where(delta > 0, 0).fillna(0)\n",
        "    loss = -delta.where(delta < 0, 0).fillna(0)\n",
        "    avg_gain = gain.ewm(alpha=1/window, adjust=False, min_periods=1).mean()\n",
        "    avg_loss = loss.ewm(alpha=1/window, adjust=False, min_periods=1).mean()\n",
        "    rs = avg_gain / avg_loss\n",
        "    rs = rs.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "    rsi = np.where(avg_loss == 0, 100, rsi)\n",
        "    rsi = np.where(avg_gain == 0, 0, rsi) # Handles all losses case\n",
        "    df[new_column_name] = rsi\n",
        "    return df\n",
        "\n",
        "def add_bollinger_bands(df, window=20, num_std_dev=2, column='Close', sma_col_name=None, upper_band_col_name=None, lower_band_col_name=None):\n",
        "    if sma_col_name is None: sma_col_name = f'BB_SMA_{window}'\n",
        "    if upper_band_col_name is None: upper_band_col_name = f'BB_Upper_{window}'\n",
        "    if lower_band_col_name is None: lower_band_col_name = f'BB_Lower_{window}'\n",
        "    df[sma_col_name] = df[column].rolling(window=window, min_periods=1).mean()\n",
        "    std_dev = df[column].rolling(window=window, min_periods=1).std(ddof=0) # ddof=0 for population std dev if preferred for BB\n",
        "    df[upper_band_col_name] = df[sma_col_name] + (std_dev * num_std_dev)\n",
        "    df[lower_band_col_name] = df[sma_col_name] - (std_dev * num_std_dev)\n",
        "    return df\n",
        "\n",
        "def add_macd(df, short_window=12, long_window=26, signal_window=9, column='Close', macd_line_col_name=None, signal_line_col_name=None, macd_hist_col_name=None):\n",
        "    if macd_line_col_name is None: macd_line_col_name = f'MACD_{short_window}_{long_window}'\n",
        "    if signal_line_col_name is None: signal_line_col_name = f'MACD_Signal_{signal_window}'\n",
        "    if macd_hist_col_name is None: macd_hist_col_name = f'MACD_Hist_{signal_window}'\n",
        "    ema_short = df[column].ewm(span=short_window, adjust=False, min_periods=1).mean()\n",
        "    ema_long = df[column].ewm(span=long_window, adjust=False, min_periods=1).mean()\n",
        "    df[macd_line_col_name] = ema_short - ema_long\n",
        "    df[signal_line_col_name] = df[macd_line_col_name].ewm(span=signal_window, adjust=False, min_periods=1).mean()\n",
        "    df[macd_hist_col_name] = df[macd_line_col_name] - df[signal_line_col_name]\n",
        "    return df\n",
        "\n",
        "def add_atr(df, window=14, high_col='High', low_col='Low', close_col='Close', atr_col_name=None):\n",
        "    if atr_col_name is None: atr_col_name = f'ATR_{window}'\n",
        "    # Ensure columns exist before trying to use them\n",
        "    if not all(col in df.columns for col in [high_col, low_col, close_col]):\n",
        "        print(f\"ATR calc: Missing one or more required columns: {high_col}, {low_col}, {close_col}\")\n",
        "        df[atr_col_name] = np.nan\n",
        "        return df\n",
        "\n",
        "    df['_prev_close'] = df[close_col].shift(1)\n",
        "    df['_h_minus_l'] = df[high_col] - df[low_col]\n",
        "    df['_h_minus_pc'] = abs(df[high_col] - df['_prev_close'])\n",
        "    df['_l_minus_pc'] = abs(df[low_col] - df['_prev_close'])\n",
        "    # Ensure NaNs from shift(1) don't break max() if it's the first row, fill with 0 or handle\n",
        "    df['_true_range'] = df[['_h_minus_l', '_h_minus_pc', '_l_minus_pc']].max(axis=1)\n",
        "    df[atr_col_name] = df['_true_range'].ewm(alpha=1/window, adjust=False, min_periods=1).mean()\n",
        "    df.drop(columns=['_prev_close', '_h_minus_l', '_h_minus_pc', '_l_minus_pc', '_true_range'], inplace=True, errors='ignore')\n",
        "    return df\n",
        "\n",
        "def add_stochastic_oscillator(df, k_window=14, d_window=3,\n",
        "                              high_col='High', low_col='Low', close_col='Close',\n",
        "                              stoch_k_col_name=None, stoch_d_col_name=None):\n",
        "    if stoch_k_col_name is None: stoch_k_col_name = f'Stoch_%K_{k_window}'\n",
        "    if stoch_d_col_name is None: stoch_d_col_name = f'Stoch_%D_{d_window}'\n",
        "    if not all(col in df.columns for col in [high_col, low_col, close_col]):\n",
        "        print(f\"Stochastic calc: Missing one or more required columns: {high_col}, {low_col}, {close_col}\")\n",
        "        df[stoch_k_col_name] = np.nan\n",
        "        df[stoch_d_col_name] = np.nan\n",
        "        return df\n",
        "\n",
        "    lowest_low = df[low_col].rolling(window=k_window, min_periods=1).min()\n",
        "    highest_high = df[high_col].rolling(window=k_window, min_periods=1).max()\n",
        "    denominator = highest_high - lowest_low\n",
        "    df[stoch_k_col_name] = ((df[close_col] - lowest_low) / (denominator + 1e-9)) * 100 # Epsilon for stability\n",
        "    df[stoch_k_col_name] = df[stoch_k_col_name].replace([np.inf, -np.inf], np.nan)\n",
        "    df[stoch_k_col_name] = df[stoch_k_col_name].ffill().bfill() # Address NaNs before clip\n",
        "    df[stoch_k_col_name] = df[stoch_k_col_name].clip(0, 100)\n",
        "    df[stoch_d_col_name] = df[stoch_k_col_name].rolling(window=d_window, min_periods=1).mean()\n",
        "    return df\n",
        "\n",
        "def add_mfi(df, window=14, high_col='High', low_col='Low', close_col='Close', volume_col='Volume', mfi_col_name=None):\n",
        "    if mfi_col_name is None: mfi_col_name = f'MFI_{window}'\n",
        "    if not all(col in df.columns for col in [high_col, low_col, close_col, volume_col]):\n",
        "        print(f\"MFI calc: Missing one or more required columns: {high_col}, {low_col}, {close_col}, {volume_col}\")\n",
        "        df[mfi_col_name] = np.nan\n",
        "        return df\n",
        "\n",
        "    df['_typical_price'] = (df[high_col] + df[low_col] + df[close_col]) / 3\n",
        "    df['_raw_money_flow'] = df['_typical_price'] * df[volume_col]\n",
        "    price_diff = df['_typical_price'].diff(1)\n",
        "    df['_positive_money_flow'] = df['_raw_money_flow'].where(price_diff > 0, 0).fillna(0)\n",
        "    df['_negative_money_flow'] = df['_raw_money_flow'].where(price_diff < 0, 0).fillna(0)\n",
        "    positive_mf_sum = df['_positive_money_flow'].rolling(window=window, min_periods=1).sum()\n",
        "    negative_mf_sum = df['_negative_money_flow'].rolling(window=window, min_periods=1).sum()\n",
        "    money_flow_ratio = positive_mf_sum / (negative_mf_sum + 1e-9)\n",
        "    mfi = 100 - (100 / (1 + money_flow_ratio))\n",
        "    mfi = mfi.replace([np.inf, -np.inf], 100) # If negative_mf_sum was 0, MFR is inf, MFI is 100\n",
        "    mfi = np.where((positive_mf_sum == 0) & (negative_mf_sum == 0), 50, mfi) # Neutral if no flow\n",
        "    mfi = np.where((positive_mf_sum > 0) & (negative_mf_sum == 0), 100, mfi) # All positive flow\n",
        "    mfi = np.where((positive_mf_sum == 0) & (negative_mf_sum > 0), 0, mfi)   # All negative flow\n",
        "    df[mfi_col_name] = mfi\n",
        "    df.drop(columns=['_typical_price', '_raw_money_flow', '_positive_money_flow', '_negative_money_flow'], inplace=True, errors='ignore')\n",
        "    return df\n",
        "\n",
        "def add_roc(df, window=12, column='Close', new_column_name=None):\n",
        "    if new_column_name is None: new_column_name = f'ROC_{window}'\n",
        "    df[new_column_name] = df[column].pct_change(periods=window) * 100\n",
        "    return df\n",
        "\n",
        "def add_keltner_channels(df, ema_window=20, atr_window=10, atr_multiplier=2, high_col='High', low_col='Low', close_col='Close', middle_col_name=None, upper_col_name=None, lower_col_name=None):\n",
        "    if middle_col_name is None: middle_col_name = f'KC_Middle_{ema_window}'\n",
        "    if upper_col_name is None: upper_col_name = f'KC_Upper_{ema_window}'\n",
        "    if lower_col_name is None: lower_col_name = f'KC_Lower_{ema_window}'\n",
        "\n",
        "    if not all(col in df.columns for col in [high_col, low_col, close_col]):\n",
        "        print(f\"Keltner calc: Missing one or more required columns: {high_col}, {low_col}, {close_col}\")\n",
        "        df[middle_col_name] = np.nan\n",
        "        df[upper_col_name] = np.nan\n",
        "        df[lower_col_name] = np.nan\n",
        "        return df\n",
        "\n",
        "    df[middle_col_name] = df[close_col].ewm(span=ema_window, adjust=False, min_periods=1).mean()\n",
        "\n",
        "    # Use a temporary ATR column name that is unlikely to clash\n",
        "    temp_atr_col_for_kc = f'_temp_atr_kc_{atr_window}'\n",
        "    # Make a copy for ATR calculation to avoid modifying df if add_atr drops columns from original\n",
        "    df_for_atr = df[[high_col, low_col, close_col]].copy()\n",
        "    df_for_atr = add_atr(df_for_atr, window=atr_window, high_col=high_col, low_col=low_col, close_col=close_col, atr_col_name=temp_atr_col_for_kc)\n",
        "\n",
        "    if temp_atr_col_for_kc in df_for_atr.columns:\n",
        "        df[upper_col_name] = df[middle_col_name] + (df_for_atr[temp_atr_col_for_kc] * atr_multiplier)\n",
        "        df[lower_col_name] = df[middle_col_name] - (df_for_atr[temp_atr_col_for_kc] * atr_multiplier)\n",
        "    else:\n",
        "        print(f\"Keltner calc: Failed to calculate ATR ({temp_atr_col_for_kc}). Setting bands to NaN.\")\n",
        "        df[upper_col_name] = np.nan\n",
        "        df[lower_col_name] = np.nan\n",
        "    return df\n",
        "\n",
        "def add_volume_sma(df, window=20, volume_col='Volume', new_column_name=None):\n",
        "    if new_column_name is None: new_column_name = f'Volume_SMA_{window}'\n",
        "    if volume_col not in df.columns:\n",
        "        print(f\"Volume SMA calc: Missing volume column: {volume_col}\")\n",
        "        df[new_column_name] = np.nan\n",
        "        return df\n",
        "    df[new_column_name] = df[volume_col].rolling(window=window, min_periods=1).mean()\n",
        "    return df\n",
        "\n",
        "def add_basic_support_resistance(df, window=20, high_col='High', low_col='Low', support_col_name=None, resistance_col_name=None):\n",
        "    if support_col_name is None: support_col_name = f'Support_{window}'\n",
        "    if resistance_col_name is None: resistance_col_name = f'Resistance_{window}'\n",
        "    if not all(col in df.columns for col in [high_col, low_col]):\n",
        "        print(f\"Support/Resistance calc: Missing one or more required columns: {high_col}, {low_col}\")\n",
        "        df[support_col_name] = np.nan\n",
        "        df[resistance_col_name] = np.nan\n",
        "        return df\n",
        "\n",
        "    df[resistance_col_name] = df[high_col].rolling(window=window, min_periods=1).max().shift(1)\n",
        "    df[support_col_name] = df[low_col].rolling(window=window, min_periods=1).min().shift(1)\n",
        "    return df"
      ],
      "metadata": {
        "id": "vNlphc1VvUdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main script to generate indicators for 1h and 15m BTC data\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np # Already imported with functions, but good practice here too\n",
        "from google.colab import drive\n",
        "\n",
        "# --- Step 1: Mount Google Drive ---\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error mounting Google Drive: {e}\")\n",
        "    raise RuntimeError(\"Google Drive could not be mounted. Script aborted.\")\n",
        "\n",
        "# --- Step 2: Configuration ---\n",
        "BASE_OHLCV_PATH = \"/content/drive/MyDrive/CryptoDataCCXT/\" # Path to original CCXT data (lowercase columns)\n",
        "# Let's save to a new subfolder to keep things organized\n",
        "OUTPUT_INDICATORS_PATH = \"/content/drive/MyDrive/CryptoDataCCXT/with_indicators_h_m15_custom/\"\n",
        "if not os.path.exists(OUTPUT_INDICATORS_PATH):\n",
        "    os.makedirs(OUTPUT_INDICATORS_PATH)\n",
        "    print(f\"Created output directory: {OUTPUT_INDICATORS_PATH}\")\n",
        "\n",
        "PAIR_SYMBOL_TO_PROCESS = \"BTC_USDT\"\n",
        "TIMEFRAMES_TO_PROCESS = [\"1h\", \"15m\"]\n",
        "DURATION_TAG = \"4years\" # From your base filenames\n",
        "\n",
        "BASE_COIN_FOR_CONFIG = PAIR_SYMBOL_TO_PROCESS.split('_')[0] # e.g., \"BTC\"\n",
        "\n",
        "# --- Step 3: Loop through timeframes and process ---\n",
        "for timeframe in TIMEFRAMES_TO_PROCESS:\n",
        "    input_filename = f\"{PAIR_SYMBOL_TO_PROCESS}_{timeframe}_{DURATION_TAG}.csv\"\n",
        "    full_input_path = os.path.join(BASE_OHLCV_PATH, input_filename)\n",
        "\n",
        "    # Define a clear output filename\n",
        "    output_filename = f\"{PAIR_SYMBOL_TO_PROCESS}_{timeframe}_all_custom_indicators.csv\"\n",
        "    full_output_path = os.path.join(OUTPUT_INDICATORS_PATH, output_filename)\n",
        "\n",
        "    print(f\"\\n\\n======================================================================\")\n",
        "    print(f\"Processing: {full_input_path}\")\n",
        "    print(f\"Output will be: {full_output_path}\")\n",
        "    print(f\"======================================================================\")\n",
        "\n",
        "    if not os.path.exists(full_input_path):\n",
        "        print(f\"ERROR: Input file not found: {full_input_path}. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        # Load the base OHLCV CSV file\n",
        "        df = pd.read_csv(full_input_path)\n",
        "        print(f\"  Successfully loaded base data. Shape: {df.shape}\")\n",
        "\n",
        "        # Standardize Datetime Index\n",
        "        if 'datetime_utc' in df.columns:\n",
        "            df['datetime_utc'] = pd.to_datetime(df['datetime_utc'])\n",
        "            df.set_index('datetime_utc', inplace=True)\n",
        "            print(\"  Used 'datetime_utc' column as DatetimeIndex.\")\n",
        "        elif 'timestamp' in df.columns and not isinstance(df.index, pd.DatetimeIndex):\n",
        "            df['datetime_utc'] = pd.to_datetime(df['timestamp'], unit='ms', utc=True)\n",
        "            df.set_index('datetime_utc', inplace=True)\n",
        "            print(\"  Used 'timestamp' (ms) column to create and set DatetimeIndex 'datetime_utc'.\")\n",
        "        else:\n",
        "            print(\"  Warning: No 'datetime_utc' or 'timestamp' column found to set as DatetimeIndex.\")\n",
        "            # If index is not datetime, many indicators might fail or behave unexpectedly.\n",
        "            # Depending on your base files, you might need to handle this.\n",
        "            # For now, we assume one of them exists.\n",
        "\n",
        "        # Ensure OHLCV columns are uppercase for indicator functions\n",
        "        rename_map = {\n",
        "            'open': 'Open', 'high': 'High', 'low': 'Low', 'close': 'Close', 'volume': 'Volume'\n",
        "        }\n",
        "        # Only rename if lowercase versions exist and uppercase don't already\n",
        "        cols_to_rename_now = {}\n",
        "        for Lc, Uc in rename_map.items():\n",
        "            if Lc in df.columns and Uc not in df.columns:\n",
        "                cols_to_rename_now[Lc] = Uc\n",
        "            elif Lc in df.columns and Uc in df.columns and Lc != Uc: # Both exist, prefer Uc, warn\n",
        "                print(f\"  Warning: Both '{Lc}' and '{Uc}' exist. Using '{Uc}'.\")\n",
        "\n",
        "\n",
        "        if cols_to_rename_now:\n",
        "            df.rename(columns=cols_to_rename_now, inplace=True)\n",
        "            print(f\"  Renamed base OHLCV columns to uppercase: {list(cols_to_rename_now.values())}\")\n",
        "\n",
        "        # Check for essential columns after rename\n",
        "        essential_cols = ['Open', 'High', 'Low', 'Close', 'Volume'] # Volume might be optional for some indicators\n",
        "        missing_essentials = [col for col in essential_cols if col not in df.columns]\n",
        "        if any(col in missing_essentials for col in ['Open', 'High', 'Low', 'Close']): # Core OHLC are critical\n",
        "            print(f\"  ERROR: Missing critical OHLC columns after renaming: {missing_essentials}. Skipping indicator calculation for this file.\")\n",
        "            continue\n",
        "        if 'Volume' not in df.columns:\n",
        "            print(\"  Warning: 'Volume' column not found. Volume-based indicators will have NaNs.\")\n",
        "\n",
        "\n",
        "        # --- Apply All Configured Indicators for BTC ---\n",
        "        if BASE_COIN_FOR_CONFIG in master_indicator_configs:\n",
        "            current_config = master_indicator_configs[BASE_COIN_FOR_CONFIG]\n",
        "            print(f\"\\n  Applying all configured indicators for {BASE_COIN_FOR_CONFIG} on {timeframe} data...\")\n",
        "\n",
        "            # Make a copy to add indicators to, preserving original loaded df for this iteration\n",
        "            df_with_indicators = df.copy()\n",
        "\n",
        "            # Moving Averages\n",
        "            if \"sma_short\" in current_config:\n",
        "                params = current_config[\"sma_short\"]\n",
        "                df_with_indicators = add_sma(df_with_indicators, **params, new_column_name=f'{BASE_COIN_FOR_CONFIG}_SMA_{params.get(\"window\")}')\n",
        "            if \"sma_long\" in current_config:\n",
        "                params = current_config[\"sma_long\"]\n",
        "                df_with_indicators = add_sma(df_with_indicators, **params, new_column_name=f'{BASE_COIN_FOR_CONFIG}_SMA_{params.get(\"window\")}')\n",
        "            if \"ema_short\" in current_config:\n",
        "                params = current_config[\"ema_short\"]\n",
        "                df_with_indicators = add_ema(df_with_indicators, **params, new_column_name=f'{BASE_COIN_FOR_CONFIG}_EMA_{params.get(\"span\")}')\n",
        "            if \"ema_long\" in current_config:\n",
        "                params = current_config[\"ema_long\"]\n",
        "                df_with_indicators = add_ema(df_with_indicators, **params, new_column_name=f'{BASE_COIN_FOR_CONFIG}_EMA_{params.get(\"span\")}')\n",
        "\n",
        "            # Other Non-Volume Dependent Indicators\n",
        "            if \"rsi\" in current_config: df_with_indicators = add_rsi(df_with_indicators, **current_config[\"rsi\"])\n",
        "            if \"bollinger\" in current_config: df_with_indicators = add_bollinger_bands(df_with_indicators, **current_config[\"bollinger\"])\n",
        "            if \"macd\" in current_config: df_with_indicators = add_macd(df_with_indicators, **current_config[\"macd\"])\n",
        "            if \"atr\" in current_config: df_with_indicators = add_atr(df_with_indicators, **current_config[\"atr\"])\n",
        "            if \"stochastic\" in current_config: df_with_indicators = add_stochastic_oscillator(df_with_indicators, **current_config[\"stochastic\"])\n",
        "            if \"roc\" in current_config: df_with_indicators = add_roc(df_with_indicators, **current_config[\"roc\"])\n",
        "            if \"keltner\" in current_config: df_with_indicators = add_keltner_channels(df_with_indicators, **current_config[\"keltner\"])\n",
        "            if \"support_resistance\" in current_config: df_with_indicators = add_basic_support_resistance(df_with_indicators, **current_config[\"support_resistance\"])\n",
        "\n",
        "            # Indicators REQUIRING Volume\n",
        "            if 'Volume' in df_with_indicators.columns:\n",
        "                print(f\"  DEBUG: 'Volume' column available. Proceeding with volume-based indicators.\")\n",
        "                if \"mfi\" in current_config:\n",
        "                    df_with_indicators = add_mfi(df_with_indicators, **current_config[\"mfi\"])\n",
        "                if \"volume_sma\" in current_config:\n",
        "                    df_with_indicators = add_volume_sma(df_with_indicators, **current_config[\"volume_sma\"])\n",
        "            else:\n",
        "                print(f\"  WARNING: 'Volume' column NOT found in DataFrame. Skipping MFI and Volume SMA, or they will have NaNs if functions add NaN columns.\")\n",
        "                # Ensure NaN columns are added if functions don't handle missing input columns by adding NaNs themselves\n",
        "                if \"mfi\" in current_config: df_with_indicators[f'MFI_{current_config[\"mfi\"].get(\"window\",14)}'] = np.nan\n",
        "                if \"volume_sma\" in current_config: df_with_indicators[f'Volume_SMA_{current_config[\"volume_sma\"].get(\"window\",20)}'] = np.nan\n",
        "\n",
        "            print(\"  All configured indicators applied.\")\n",
        "\n",
        "            # --- Save the DataFrame with Indicators ---\n",
        "            df_with_indicators.to_csv(full_output_path)\n",
        "            print(f\"  Successfully saved DataFrame with indicators to: {full_output_path}\")\n",
        "            print(f\"  New file shape: {df_with_indicators.shape}\")\n",
        "            print(f\"  New file columns (first 10): {df_with_indicators.columns.tolist()[:10]}\")\n",
        "\n",
        "        else:\n",
        "            print(f\"  ERROR: No configuration found for '{BASE_COIN_FOR_CONFIG}' in master_indicator_configs. Cannot apply indicators.\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        # This should be caught by the earlier check, but as a safeguard\n",
        "        print(f\"  ERROR: File not found during processing: {full_input_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  An error occurred while processing {full_input_path}: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "print(\"\\n\\n--- All 1h and 15m BTC Indicator Generation Complete ---\")"
      ],
      "metadata": {
        "id": "TrlCOSKpvXz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from google.colab import drive # For Google Drive access\n",
        "\n",
        "# Ensure backtesting library is installed\n",
        "# !pip install backtesting\n",
        "\n",
        "from backtesting import Backtest, Strategy\n",
        "from backtesting.lib import crossover # For detecting crossovers\n",
        "\n",
        "# --- Step 1: Mount Google Drive ---\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error mounting Google Drive: {e}\")\n",
        "    raise RuntimeError(\"Google Drive could not be mounted. Script aborted.\")\n",
        "\n",
        "# --- Step 2: Define Your Strategy Class (Generates Signals Internally) ---\n",
        "# This class structure is the same, the position_size_pct will be passed from bt.run()\n",
        "class EmaRsiAtrDynamicSizeStrategy(Strategy):\n",
        "    # --- Strategy Parameters (will be set via bt.run()) ---\n",
        "    atr_multiplier_sl = 1.5\n",
        "    atr_multiplier_tp = 3.0\n",
        "    rsi_buy_threshold = 50\n",
        "    position_size_pct = 0.10 # Default if not passed, but we will pass it\n",
        "\n",
        "    fast_ema_col_name = 'DEFAULT_FAST_EMA'\n",
        "    slow_ema_col_name = 'DEFAULT_SLOW_EMA'\n",
        "    rsi_col_name = 'DEFAULT_RSI'\n",
        "    atr_col_name = 'DEFAULT_ATR'\n",
        "\n",
        "    def init(self):\n",
        "        if self.fast_ema_col_name not in self.data.df.columns: raise ValueError(f\"Fast EMA column '{self.fast_ema_col_name}' not found. Cols: {self.data.df.columns.tolist()}\")\n",
        "        if self.slow_ema_col_name not in self.data.df.columns: raise ValueError(f\"Slow EMA column '{self.slow_ema_col_name}' not found. Cols: {self.data.df.columns.tolist()}\")\n",
        "        if self.rsi_col_name not in self.data.df.columns: raise ValueError(f\"RSI column '{self.rsi_col_name}' not found. Cols: {self.data.df.columns.tolist()}\")\n",
        "        if self.atr_col_name not in self.data.df.columns: raise ValueError(f\"ATR column '{self.atr_col_name}' not found. Cols: {self.data.df.columns.tolist()}\")\n",
        "\n",
        "        self.fast_ema = self.I(lambda x: x, self.data.df[self.fast_ema_col_name], name=\"FastEMA\")\n",
        "        self.slow_ema = self.I(lambda x: x, self.data.df[self.slow_ema_col_name], name=\"SlowEMA\")\n",
        "        self.rsi = self.I(lambda x: x, self.data.df[self.rsi_col_name], name=\"RSI\")\n",
        "        self.atr = self.I(lambda x: x, self.data.df[self.atr_col_name], name=\"ATR\")\n",
        "\n",
        "    def next(self):\n",
        "        if len(self.data.Close) < 2: # Ensure there's at least one previous bar for signal calculation\n",
        "            return\n",
        "\n",
        "        # Signals are based on the PREVIOUS bar's close (i-1, or self.data.Close[-2] if current bar is -1)\n",
        "        # Trade execution at the CURRENT bar's open (i, or self.data.Open[-1] if current bar is -1)\n",
        "\n",
        "        # Using -2 to access data from the bar that just closed, decision for open of bar -1 (current processing bar)\n",
        "        # backtesting.py calls next() for each bar. self.data.Close[-1] is the current bar's close for which signal forms.\n",
        "        # Trade is then for next bar.\n",
        "        # crossover() uses current and previous values internally.\n",
        "\n",
        "        price_for_sl_tp_calc = self.data.Close[-1] # Price of the bar where signal is confirmed (its close)\n",
        "        current_atr_for_sl_tp = self.atr[-1]      # ATR of the bar where signal is confirmed\n",
        "        entry_price_next_bar_open = self.data.Open[-1] # This is actually current bar's open\n",
        "                                                     # In backtesting.py, self.buy() places order for *next* bar.\n",
        "                                                     # So, decision based on Close[-1], ATR[-1]. Order for Open[current_bar_idx+1].\n",
        "                                                     # For sizing, we can use Close[-1] as proxy for next open.\n",
        "\n",
        "        # --- Entry Conditions (Long Only) ---\n",
        "        fast_ema_crossed_above_slow_ema = crossover(self.fast_ema, self.slow_ema)\n",
        "        rsi_is_bullish = self.rsi[-1] > self.rsi_buy_threshold\n",
        "\n",
        "        if fast_ema_crossed_above_slow_ema and rsi_is_bullish and not self.position:\n",
        "            if pd.notna(current_atr_for_sl_tp) and current_atr_for_sl_tp > 0:\n",
        "\n",
        "                # For SL/TP, anchor to the estimated entry price (next bar's open, proxied by current close)\n",
        "                sl_price = price_for_sl_tp_calc - (current_atr_for_sl_tp * self.atr_multiplier_sl)\n",
        "                tp_price = price_for_sl_tp_calc + (current_atr_for_sl_tp * self.atr_multiplier_tp)\n",
        "\n",
        "                # Calculate size based on equity and estimated entry price\n",
        "                # The trade will actually happen at the next bar's open.\n",
        "                # Using current close as proxy for next open for sizing.\n",
        "                # self.equity is the current cash + value of open positions.\n",
        "                # If not in position, self.equity is effectively cash.\n",
        "                cash_to_invest = self.equity * self.position_size_pct\n",
        "                size_in_units = cash_to_invest / price_for_sl_tp_calc # Using current close as proxy for next open\n",
        "\n",
        "                if size_in_units > 1e-8: # Avoid extremely small or zero size\n",
        "                    self.buy(size=size_in_units, sl=sl_price, tp=tp_price)\n",
        "            else:\n",
        "                if not (pd.notna(current_atr_for_sl_tp) and current_atr_for_sl_tp > 0):\n",
        "                     print(f\"Warning: ATR is invalid ({current_atr_for_sl_tp:.2f}) on {self.data.index[-1]}. Cannot set SL/TP. Skipping buy signal.\")\n",
        "\n",
        "        # --- Exit Conditions (Long Only) based on EMA cross ---\n",
        "        fast_ema_crossed_below_slow_ema = crossover(self.slow_ema, self.fast_ema)\n",
        "\n",
        "        if fast_ema_crossed_below_slow_ema and self.position:\n",
        "            self.position.close()\n",
        "\n",
        "# --- Step 3: Master Indicator Configuration (Placeholder - ENSURE THIS IS DEFINED IN YOUR NOTEBOOK) ---\n",
        "# This dictionary is crucial for dynamically getting the correct column names.\n",
        "master_indicator_configs = {\n",
        "    \"BTC\": {\n",
        "        \"ema_short\": {\"span\": 12, \"column\": \"Close\"},\n",
        "        \"ema_long\":  {\"span\": 26, \"column\": \"Close\"},\n",
        "        \"rsi\":       {\"window\": 14, \"column\": \"Close\"},\n",
        "        \"atr\":       {\"window\": 14},\n",
        "    },\n",
        "    \"ETH\": {\n",
        "        \"ema_short\": {\"span\": 12, \"column\": \"Close\"}, \"ema_long\":  {\"span\": 26, \"column\": \"Close\"},\n",
        "        \"rsi\":       {\"window\": 14, \"column\": \"Close\"}, \"atr\":       {\"window\": 14},\n",
        "    },\n",
        "    \"SOL\": {\n",
        "        \"ema_short\": {\"span\": 12, \"column\": \"Close\"}, \"ema_long\":  {\"span\": 26, \"column\": \"Close\"},\n",
        "        \"rsi\":       {\"window\": 14, \"column\": \"Close\"}, \"atr\":       {\"window\": 14},\n",
        "    }\n",
        "}\n",
        "# --- End of Master Indicator Configuration Placeholder ---\n",
        "\n",
        "\n",
        "# --- Step 4: Main Configuration & Data Loading ---\n",
        "GDRIVE_INDICATORS_PATH = \"/content/drive/MyDrive/CryptoDataCCXT/with_indicators_h_m15_custom/\"\n",
        "\n",
        "PAIR_SYMBOL = \"BTC_USDT\"\n",
        "TIMEFRAME = \"1h\" # Keeping 1h as per your last output. Change to \"15m\" if needed.\n",
        "\n",
        "FILENAME_SUFFIX = \"all_custom_indicators.csv\"\n",
        "file_to_backtest = f\"{PAIR_SYMBOL}_{TIMEFRAME}_{FILENAME_SUFFIX}\"\n",
        "full_file_path = os.path.join(GDRIVE_INDICATORS_PATH, file_to_backtest)\n",
        "\n",
        "BASE_COIN_SYMBOL = PAIR_SYMBOL.split('_')[0]\n",
        "\n",
        "try:\n",
        "    coin_config = master_indicator_configs[BASE_COIN_SYMBOL]\n",
        "    if 'ema_short' not in coin_config or 'span' not in coin_config['ema_short']: raise KeyError(\"Config missing 'ema_short' or its 'span'\")\n",
        "    if 'ema_long' not in coin_config or 'span' not in coin_config['ema_long']: raise KeyError(\"Config missing 'ema_long' or its 'span'\")\n",
        "    if 'rsi' not in coin_config or 'window' not in coin_config['rsi']: raise KeyError(\"Config missing 'rsi' or its 'window'\")\n",
        "    if 'atr' not in coin_config or 'window' not in coin_config['atr']: raise KeyError(\"Config missing 'atr' or its 'window'\")\n",
        "\n",
        "    ACTUAL_FAST_EMA_COL_NAME = f\"{BASE_COIN_SYMBOL}_EMA_{coin_config['ema_short']['span']}\"\n",
        "    ACTUAL_SLOW_EMA_COL_NAME = f\"{BASE_COIN_SYMBOL}_EMA_{coin_config['ema_long']['span']}\"\n",
        "    ACTUAL_RSI_COL_NAME = f\"RSI_{coin_config['rsi']['window']}\"\n",
        "    ACTUAL_ATR_COL_NAME = f\"ATR_{coin_config['atr']['window']}\"\n",
        "except KeyError as e:\n",
        "    print(f\"CRITICAL ERROR with master_indicator_configs for {BASE_COIN_SYMBOL}: {e}.\")\n",
        "    raise\n",
        "\n",
        "INITIAL_CAPITAL = 100000.0\n",
        "COMMISSION_RATE_DECIMAL = 0.001\n",
        "ATR_SL_MULTIPLIER_PARAM = 1.5\n",
        "ATR_TP_MULTIPLIER_PARAM = 3.0\n",
        "RSI_BUY_LEVEL_PARAM = 50\n",
        "# --- KEY CHANGE: Reduce Position Size ---\n",
        "POSITION_SIZE_PERCENT_PARAM = 0.10 # Using 10% of equity per trade\n",
        "\n",
        "print(f\"\\n--- Backtesting Strategy for: {full_file_path} using backtesting.py ---\")\n",
        "print(f\"Strategy uses: Fast EMA ({ACTUAL_FAST_EMA_COL_NAME}), Slow EMA ({ACTUAL_SLOW_EMA_COL_NAME}), RSI ({ACTUAL_RSI_COL_NAME}), ATR ({ACTUAL_ATR_COL_NAME})\")\n",
        "print(f\"Position Size per trade: {POSITION_SIZE_PERCENT_PARAM*100}% of current equity.\")\n",
        "\n",
        "if not os.path.exists(full_file_path):\n",
        "    print(f\"ERROR: File not found at {full_file_path}. Cannot proceed.\")\n",
        "else:\n",
        "    try:\n",
        "        df_loaded = pd.read_csv(full_file_path, index_col='datetime_utc', parse_dates=True)\n",
        "        print(f\"Successfully loaded data. Shape: {df_loaded.shape}. Data from {df_loaded.index.min()} to {df_loaded.index.max()}\")\n",
        "\n",
        "        if df_loaded.empty:\n",
        "            print(\"ERROR: Loaded DataFrame is empty.\")\n",
        "        else:\n",
        "            required_ohlc_cols = ['Open', 'High', 'Low', 'Close']\n",
        "            required_strategy_indicator_cols = [ACTUAL_FAST_EMA_COL_NAME, ACTUAL_SLOW_EMA_COL_NAME, ACTUAL_RSI_COL_NAME, ACTUAL_ATR_COL_NAME]\n",
        "            all_needed_cols = required_ohlc_cols + required_strategy_indicator_cols\n",
        "            missing_cols_in_df = [col for col in all_needed_cols if col not in df_loaded.columns]\n",
        "\n",
        "            if missing_cols_in_df:\n",
        "                 raise ValueError(f\"DataFrame is missing one or more required columns for the strategy: {missing_cols_in_df}. Available columns: {df_loaded.columns.tolist()}\")\n",
        "\n",
        "            bt = Backtest(\n",
        "                df_loaded, EmaRsiAtrDynamicSizeStrategy,\n",
        "                cash=INITIAL_CAPITAL, commission=COMMISSION_RATE_DECIMAL,\n",
        "                exclusive_orders=True, trade_on_close=False\n",
        "            )\n",
        "\n",
        "            stats = bt.run(\n",
        "                atr_multiplier_sl=ATR_SL_MULTIPLIER_PARAM,\n",
        "                atr_multiplier_tp=ATR_TP_MULTIPLIER_PARAM,\n",
        "                rsi_buy_threshold=RSI_BUY_LEVEL_PARAM,\n",
        "                position_size_pct=POSITION_SIZE_PERCENT_PARAM, # Pass the updated position size\n",
        "                fast_ema_col_name=ACTUAL_FAST_EMA_COL_NAME,\n",
        "                slow_ema_col_name=ACTUAL_SLOW_EMA_COL_NAME,\n",
        "                rsi_col_name=ACTUAL_RSI_COL_NAME,\n",
        "                atr_col_name=ACTUAL_ATR_COL_NAME\n",
        "            )\n",
        "\n",
        "            print(\"\\n--- Backtest Performance Metrics (from backtesting.py) ---\")\n",
        "            print(stats)\n",
        "\n",
        "            print(f\"\\nSelected Stats:\")\n",
        "            stat_keys_to_print = ['Return [%]', 'Max. Drawdown [%]', 'Win Rate [%]', '# Trades', 'Profit Factor', 'Sharpe Ratio', 'Sortino Ratio']\n",
        "            for key in stat_keys_to_print:\n",
        "                value = stats.get(key, 'N/A')\n",
        "                if isinstance(value, float):\n",
        "                    print(f\"  {key}: {value:.2f}\")\n",
        "                else:\n",
        "                    print(f\"  {key}: {value}\")\n",
        "\n",
        "            plot_filename = os.path.join(GDRIVE_INDICATORS_PATH, f\"{PAIR_SYMBOL}_{TIMEFRAME}_ema_rsi_atr_backtest_plot_pos_size_updated.html\")\n",
        "            try:\n",
        "                bt.plot(filename=plot_filename, open_browser=False, superimpose=True)\n",
        "                print(f\"Backtest plot saved to: {plot_filename}\")\n",
        "            except Exception as plot_error:\n",
        "                print(f\"Could not generate plot: {plot_error}.\")\n",
        "\n",
        "    except ValueError as ve:\n",
        "        print(f\"ValueError during setup or backtest: {ve}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during the backtest: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "print(\"\\n--- Backtesting script with backtesting.py complete ---\")\n"
      ],
      "metadata": {
        "id": "8QpDMtYcvnkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "master_indicator_configs = {\n",
        "    \"BTC\": {\n",
        "        # Parameters for OptimizableEmaRsiAtrStrategy\n",
        "        \"ema_short_len\":      {\"default\": 12, \"optimize\": range(9, 25, 3)},   # Test 9, 12, 15, 18, 21, 24\n",
        "        \"ema_long_len\":       {\"default\": 26, \"optimize\": range(25, 55, 5)},  # Test 25, 30, ..., 50\n",
        "        \"rsi_len\":            {\"default\": 14, \"optimize\": range(7, 22, 3)},    # Test 7, 10, 13, 16, 19\n",
        "        \"rsi_buy_threshold\":  {\"default\": 50, \"optimize\": range(40, 61, 5)},   # Test 40, 45, 50, 55, 60\n",
        "        \"atr_len\":            {\"default\": 14, \"optimize\": range(10, 21, 4)},   # Test 10, 14, 18\n",
        "        \"atr_multiplier_sl\":  {\"default\": 1.5, \"optimize\": np.arange(1.0, 2.6, 0.5)}, # Test 1.0, 1.5, 2.0, 2.5\n",
        "        \"atr_multiplier_tp\":  {\"default\": 3.0, \"optimize\": np.arange(2.0, 5.1, 1.0)}, # Test 2.0, 3.0, 4.0, 5.0\n",
        "        \"use_trend_filter\":   {\"default\": True, \"optimize\": [True, False]},\n",
        "        \"long_sma_trend_len\": {\"default\": 100, \"optimize\": range(50, 151, 25)}, # Test 50, 75, 100, 125, 150\n",
        "\n",
        "        # You can keep your original indicator configs here too if used by other scripts\n",
        "        # \"sma_short\":    {\"window\": 10, \"column\": \"Close\"},\n",
        "        # ... etc.\n",
        "    },\n",
        "    \"ETH\": {\n",
        "        # Define similar \"param_name\": {\"default\": X, \"optimize\": Y} for ETH\n",
        "        \"ema_short_len\":      {\"default\": 12, \"optimize\": range(9, 25, 3)},\n",
        "        \"ema_long_len\":       {\"default\": 26, \"optimize\": range(25, 55, 5)},\n",
        "        # ... fill out for all optimizable parameters for ETH\n",
        "        \"rsi_len\": {\"default\": 14, \"optimize\": [14]}, # Example: fix RSI window for ETH\n",
        "        \"rsi_buy_threshold\": {\"default\": 50, \"optimize\": [50, 55]},\n",
        "        \"atr_len\": {\"default\": 14, \"optimize\": [14]},\n",
        "        \"atr_multiplier_sl\": {\"default\": 1.5, \"optimize\": [1.5, 2.0]},\n",
        "        \"atr_multiplier_tp\": {\"default\": 3.0, \"optimize\": [3.0, 4.0]},\n",
        "        \"use_trend_filter\":  {\"default\": True, \"optimize\": [True]},\n",
        "        \"long_sma_trend_len\": {\"default\": 100, \"optimize\": [100, 120]},\n",
        "    },\n",
        "    \"SOL\": {\n",
        "        # Define similar \"param_name\": {\"default\": X, \"optimize\": Y} for SOL\n",
        "        \"ema_short_len\":      {\"default\": 12, \"optimize\": range(9, 25, 3)},\n",
        "        \"ema_long_len\":       {\"default\": 26, \"optimize\": range(25, 55, 5)},\n",
        "        # ... fill out for all optimizable parameters for SOL\n",
        "        \"rsi_len\": {\"default\": 14, \"optimize\": [14]},\n",
        "        \"rsi_buy_threshold\": {\"default\": 50, \"optimize\": [50]},\n",
        "        \"atr_len\": {\"default\": 14, \"optimize\": [14]},\n",
        "        \"atr_multiplier_sl\": {\"default\": 1.5, \"optimize\": [1.5]},\n",
        "        \"atr_multiplier_tp\": {\"default\": 3.0, \"optimize\": [3.0]},\n",
        "        \"use_trend_filter\":  {\"default\": True, \"optimize\": [True, False]},\n",
        "        \"long_sma_trend_len\": {\"default\": 100, \"optimize\": [100]},\n",
        "    }\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "id": "N-TO4lq8zy3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Ensure backtesting library is installed\n",
        "# !pip install backtesting\n",
        "\n",
        "from backtesting import Backtest, Strategy\n",
        "from backtesting.lib import crossover\n",
        "\n",
        "# --- Helper functions for Indicators (same as before) ---\n",
        "def ema_func(series, span):\n",
        "    return series.ewm(span=span, adjust=False, min_periods=1).mean()\n",
        "\n",
        "def sma_func(series, window):\n",
        "    return series.rolling(window=window, min_periods=1).mean()\n",
        "\n",
        "def rsi_func(series, window):\n",
        "    if window == 0: return pd.Series(50., index=series.index) # Handle invalid window\n",
        "    delta = series.diff(1)\n",
        "    gain = delta.where(delta > 0, 0).fillna(0)\n",
        "    loss = -delta.where(delta < 0, 0).fillna(0)\n",
        "    # Ensure min_periods is at least 1, and ideally >= window for stable start\n",
        "    min_p = max(1, int(window))\n",
        "    avg_gain = gain.ewm(alpha=1/window, adjust=False, min_periods=min_p).mean()\n",
        "    avg_loss = loss.ewm(alpha=1/window, adjust=False, min_periods=min_p).mean()\n",
        "    rs = avg_gain / (avg_loss + 1e-9)\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "    rsi = np.where(avg_loss < 1e-9, 100, rsi) # If avg_loss is effectively 0\n",
        "    rsi = np.where(avg_gain < 1e-9, 0, rsi)   # If avg_gain is effectively 0\n",
        "    return pd.Series(rsi, index=series.index).fillna(50) # Fill initial NaNs with neutral 50\n",
        "\n",
        "def atr_func(high_series, low_series, close_series, window):\n",
        "    if window == 0: return pd.Series(0., index=close_series.index) # Handle invalid window\n",
        "    prev_close = close_series.shift(1)\n",
        "    tr1 = abs(high_series - low_series)\n",
        "    tr2 = abs(high_series - prev_close)\n",
        "    tr3 = abs(low_series - prev_close)\n",
        "    true_range = pd.DataFrame({'tr1': tr1, 'tr2': tr2, 'tr3': tr3}).max(axis=1)\n",
        "    # Ensure min_periods is at least 1, and ideally >= window\n",
        "    min_p = max(1, int(window))\n",
        "    atr = true_range.ewm(alpha=1/window, adjust=False, min_periods=min_p).mean()\n",
        "    return atr.fillna(0) # Fill initial NaNs with 0\n",
        "\n",
        "\n",
        "# --- Step 1: Mount Google Drive ---\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error mounting Google Drive: {e}\")\n",
        "    raise RuntimeError(\"Google Drive could not be mounted. Script aborted.\")\n",
        "\n",
        "# --- Step 2: Define Your Optimizable Strategy Class ---\n",
        "class OptimizableEmaRsiAtrStrategy(Strategy):\n",
        "    # Parameters will be set by bt.optimize() using values from master_indicator_configs\n",
        "    # Class variables here act as defaults if a param is not optimized OR if running bt.run() directly\n",
        "    ema_short_len = 12\n",
        "    ema_long_len = 26\n",
        "    rsi_len = 14\n",
        "    rsi_buy_threshold = 50\n",
        "    atr_len = 14\n",
        "    atr_multiplier_sl = 1.5\n",
        "    atr_multiplier_tp = 3.0\n",
        "    use_trend_filter = True\n",
        "    long_sma_trend_len = 100\n",
        "    position_size_pct = 0.10 # Fixed for this optimization, can be optimized separately\n",
        "\n",
        "    def init(self):\n",
        "        # Ensure parameters are integers where appropriate (for window/span/length)\n",
        "        self.p_ema_short_len = int(self.ema_short_len)\n",
        "        self.p_ema_long_len = int(self.ema_long_len)\n",
        "        self.p_rsi_len = int(self.rsi_len)\n",
        "        self.p_atr_len = int(self.atr_len)\n",
        "        self.p_long_sma_trend_len = int(self.long_sma_trend_len)\n",
        "\n",
        "        self.fast_ema = self.I(ema_func, self.data.Close, span=self.p_ema_short_len, name=\"FastEMA\")\n",
        "        self.slow_ema = self.I(ema_func, self.data.Close, span=self.p_ema_long_len, name=\"SlowEMA\")\n",
        "        self.rsi_calc = self.I(rsi_func, self.data.Close, window=self.p_rsi_len, name=\"RSI\")\n",
        "        self.atr_calc = self.I(atr_func, self.data.High, self.data.Low, self.data.Close, window=self.p_atr_len, name=\"ATR\")\n",
        "\n",
        "        if self.use_trend_filter:\n",
        "            self.long_sma_trend = self.I(sma_func, self.data.Close, window=self.p_long_sma_trend_len, name=\"TrendSMA\")\n",
        "        else:\n",
        "            self.long_sma_trend = None\n",
        "\n",
        "    def next(self):\n",
        "        # Ensure enough data for all indicators based on their actual lengths\n",
        "        min_bars_needed = max(self.p_ema_long_len, self.p_rsi_len, self.p_atr_len, (self.p_long_sma_trend_len if self.use_trend_filter else 0)) + 2\n",
        "        if len(self.data.Close) < min_bars_needed:\n",
        "            return\n",
        "\n",
        "        price_for_sl_tp_calc = self.data.Close[-1]\n",
        "        current_atr_for_sl_tp = self.atr_calc[-1]\n",
        "        entry_price_this_bar_open = self.data.Open[-1] # This is for current bar's open if trade_on_close=False\n",
        "\n",
        "        trend_is_up = True\n",
        "        if self.use_trend_filter and self.long_sma_trend is not None:\n",
        "            trend_is_up = self.data.Close[-1] > self.long_sma_trend[-1]\n",
        "\n",
        "        fast_ema_crossed_above_slow_ema = crossover(self.fast_ema, self.slow_ema)\n",
        "        rsi_is_bullish = self.rsi_calc[-1] > self.rsi_buy_threshold\n",
        "\n",
        "        if trend_is_up and fast_ema_crossed_above_slow_ema and rsi_is_bullish and not self.position:\n",
        "            if pd.notna(current_atr_for_sl_tp) and current_atr_for_sl_tp > 0 and pd.notna(price_for_sl_tp_calc) and price_for_sl_tp_calc > 0 : # Use price_for_sl_tp_calc also for sizing as proxy\n",
        "                sl_price = price_for_sl_tp_calc - (current_atr_for_sl_tp * self.atr_multiplier_sl)\n",
        "                tp_price = price_for_sl_tp_calc + (current_atr_for_sl_tp * self.atr_multiplier_tp)\n",
        "\n",
        "                cash_to_invest = self.equity * self.position_size_pct\n",
        "                size_in_units = cash_to_invest / price_for_sl_tp_calc # Using Close of signal bar as proxy for next open\n",
        "\n",
        "                if size_in_units > 1e-8:\n",
        "                    self.buy(size=size_in_units, sl=sl_price, tp=tp_price)\n",
        "            else:\n",
        "                # Minimal logging during optimization, can be enabled for debugging a single run\n",
        "                # print(f\"Trace: ATR or Price invalid. ATR: {current_atr_for_sl_tp}, Price: {price_for_sl_tp_calc}. Buy skipped on {self.data.index[-1]}.\")\n",
        "                pass\n",
        "\n",
        "\n",
        "        fast_ema_crossed_below_slow_ema = crossover(self.slow_ema, self.fast_ema)\n",
        "        if fast_ema_crossed_below_slow_ema and self.position:\n",
        "            self.position.close()\n",
        "\n",
        "# --- Step 3: Master Indicator Configuration ( ENSURE THIS IS DEFINED IN YOUR NOTEBOOK ) ---\n",
        "# Paste your 'master_indicator_configs' dictionary here, structured for optimization.\n",
        "# Example structure provided in the explanation above.\n",
        "master_indicator_configs = {\n",
        "    \"BTC\": {\n",
        "        \"ema_short_len\":      {\"default\": 12, \"optimize\": range(10, 25, 3)}, # e.g., 10, 13, 16, 19, 22\n",
        "        \"ema_long_len\":       {\"default\": 26, \"optimize\": range(25, 51, 5)},  # e.g., 25, 30, ... 50\n",
        "        \"rsi_len\":            {\"default\": 14, \"optimize\": range(9, 20, 3)},   # e.g., 9, 12, 15, 18\n",
        "        \"rsi_buy_threshold\":  {\"default\": 50, \"optimize\": [45, 50, 55]},     # Test specific values\n",
        "        \"atr_len\":            {\"default\": 14, \"optimize\": range(10, 21, 5)},   # e.g., 10, 15, 20\n",
        "        \"atr_multiplier_sl\":  {\"default\": 1.5, \"optimize\": np.arange(1.0, 2.6, 0.5)},\n",
        "        \"atr_multiplier_tp\":  {\"default\": 3.0, \"optimize\": np.arange(2.0, 4.1, 0.5)},\n",
        "        \"use_trend_filter\":   {\"default\": True, \"optimize\": [True, False]},\n",
        "        \"long_sma_trend_len\": {\"default\": 100, \"optimize\": range(50, 151, 25)},\n",
        "        # position_size_pct is fixed for this optimization run\n",
        "    },\n",
        "    # Define ETH, SOL similarly if you want to optimize for them later\n",
        "}\n",
        "# --- End of Master Indicator Configuration ---\n",
        "\n",
        "\n",
        "# --- Step 4: Main Configuration & Data Loading for Optimization ---\n",
        "GDRIVE_BASE_DATA_PATH = \"/content/drive/MyDrive/CryptoDataCCXT/\"\n",
        "OUTPUT_OPTIMIZATION_PATH = \"/content/drive/MyDrive/CryptoDataCCXT/optimization_results/\"\n",
        "if not os.path.exists(OUTPUT_OPTIMIZATION_PATH):\n",
        "    os.makedirs(OUTPUT_OPTIMIZATION_PATH)\n",
        "\n",
        "PAIR_SYMBOL = \"BTC_USDT\"\n",
        "TIMEFRAME = \"1h\" # Change to \"15m\" if desired\n",
        "DURATION_TAG = \"4years\"\n",
        "\n",
        "file_to_backtest = f\"{PAIR_SYMBOL}_{TIMEFRAME}_{DURATION_TAG}.csv\" # Using BASE OHLCV file\n",
        "full_file_path = os.path.join(GDRIVE_BASE_DATA_PATH, file_to_backtest)\n",
        "\n",
        "INITIAL_CAPITAL = 100000.0\n",
        "COMMISSION_RATE_DECIMAL = 0.001\n",
        "FIXED_POSITION_SIZE_PCT = 0.10 # Keeping this fixed during this optimization round\n",
        "\n",
        "print(f\"\\n--- Optimizing Strategy for: {full_file_path} using backtesting.py ---\")\n",
        "print(f\"Fixed Position Size per trade: {FIXED_POSITION_SIZE_PCT*100}% of current equity.\")\n",
        "\n",
        "if not os.path.exists(full_file_path):\n",
        "    print(f\"ERROR: File not found at {full_file_path}. Cannot proceed.\")\n",
        "else:\n",
        "    try:\n",
        "        df_loaded_raw = pd.read_csv(full_file_path)\n",
        "\n",
        "        if 'datetime_utc' in df_loaded_raw.columns:\n",
        "            df_loaded_raw['datetime_utc'] = pd.to_datetime(df_loaded_raw['datetime_utc'])\n",
        "            df_loaded_raw.set_index('datetime_utc', inplace=True)\n",
        "        elif 'timestamp' in df_loaded_raw.columns:\n",
        "            df_loaded_raw['datetime_utc'] = pd.to_datetime(df_loaded_raw['timestamp'], unit='ms', utc=True)\n",
        "            df_loaded_raw.set_index('datetime_utc', inplace=True)\n",
        "        else:\n",
        "            raise ValueError(\"DataFrame must have 'datetime_utc' or 'timestamp' column for index.\")\n",
        "\n",
        "        rename_map = {'open': 'Open', 'high': 'High', 'low': 'Low', 'close': 'Close', 'volume': 'Volume'}\n",
        "        df_prepared = df_loaded_raw.rename(columns={k:v for k,v in rename_map.items() if k in df_loaded_raw.columns})\n",
        "\n",
        "        required_ohlcv_cols = ['Open', 'High', 'Low', 'Close']\n",
        "        if not all(col in df_prepared.columns for col in required_ohlcv_cols):\n",
        "            raise ValueError(f\"Prepared DataFrame must contain 'Open', 'High', 'Low', 'Close'. Found: {df_prepared.columns.tolist()}\")\n",
        "        if 'Volume' not in df_prepared.columns: # MFI uses Volume, ensure it's available if MFI part of strategy\n",
        "            print(\"Warning: 'Volume' column not found in input data. Volume-based indicators will not work if strategy depends on them.\")\n",
        "\n",
        "\n",
        "        print(f\"Successfully loaded and prepared data. Shape: {df_prepared.shape}. Data from {df_prepared.index.min()} to {df_prepared.index.max()}\")\n",
        "\n",
        "        if df_prepared.empty:\n",
        "            print(\"ERROR: Loaded DataFrame is empty.\")\n",
        "        else:\n",
        "            # --- Step 5: Prepare Optimization Grid from Master Config ---\n",
        "            BASE_COIN_FOR_CONFIG = PAIR_SYMBOL.split('_')[0]\n",
        "            if BASE_COIN_FOR_CONFIG not in master_indicator_configs:\n",
        "                raise ValueError(f\"No configuration found for {BASE_COIN_FOR_CONFIG} in master_indicator_configs.\")\n",
        "\n",
        "            coin_opt_config = master_indicator_configs[BASE_COIN_FOR_CONFIG]\n",
        "            optimization_grid = {}\n",
        "            strategy_fixed_params = {\"position_size_pct\": FIXED_POSITION_SIZE_PCT}\n",
        "\n",
        "            # Parameters that the strategy class defines and that we want to optimize\n",
        "            strategy_params_to_optimize = [\n",
        "                \"ema_short_len\", \"ema_long_len\", \"rsi_len\", \"rsi_buy_threshold\",\n",
        "                \"atr_len\", \"atr_multiplier_sl\", \"atr_multiplier_tp\",\n",
        "                \"use_trend_filter\", \"long_sma_trend_len\"\n",
        "            ]\n",
        "\n",
        "            for param_name in strategy_params_to_optimize:\n",
        "                if param_name in coin_opt_config and \"optimize\" in coin_opt_config[param_name]:\n",
        "                    optimization_grid[param_name] = coin_opt_config[param_name][\"optimize\"]\n",
        "                elif param_name in coin_opt_config and \"default\" in coin_opt_config[param_name]:\n",
        "                    # If no \"optimize\" range, pass the \"default\" as a fixed parameter for this run\n",
        "                    strategy_fixed_params[param_name] = coin_opt_config[param_name][\"default\"]\n",
        "                    print(f\"Using default value for {param_name}: {strategy_fixed_params[param_name]}\")\n",
        "                else:\n",
        "                    # Use class default if not in config, bt.optimize will pass it if in grid\n",
        "                    # or strategy class default will be used.\n",
        "                    print(f\"Parameter {param_name} not found in coin_opt_config or no 'optimize'/'default' key. Strategy class default will be used if not optimized.\")\n",
        "\n",
        "\n",
        "            if not optimization_grid:\n",
        "                raise ValueError(\"Optimization grid is empty. Check 'master_indicator_configs' for 'optimize' keys.\")\n",
        "\n",
        "            print(\"\\nOptimization Grid:\")\n",
        "            for k, v in optimization_grid.items():\n",
        "                print(f\"  {k}: {list(v) if isinstance(v, (range, type(np.arange(1)))) else v}\")\n",
        "            print(\"\\nFixed Parameters for Strategy (not optimized in this run):\")\n",
        "            for k, v in strategy_fixed_params.items():\n",
        "                print(f\"  {k}: {v}\")\n",
        "\n",
        "\n",
        "            # --- Run Optimization ---\n",
        "            bt = Backtest(\n",
        "                df_prepared, OptimizableEmaRsiAtrStrategy,\n",
        "                cash=INITIAL_CAPITAL, commission=COMMISSION_RATE_DECIMAL,\n",
        "                exclusive_orders=True, trade_on_close=False\n",
        "            )\n",
        "\n",
        "            print(\"\\nStarting optimization (this may take some time depending on ranges and data length)...\")\n",
        "\n",
        "            optimization_stats = bt.optimize(\n",
        "                **optimization_grid, # Spread the optimization grid\n",
        "                **strategy_fixed_params, # Spread fixed parameters\n",
        "                maximize='SQN',\n",
        "                # maximize='Sharpe Ratio',\n",
        "                # maximize='Sortino Ratio',\n",
        "                # maximize='Equity Final [$]',\n",
        "                constraint=lambda params: params.ema_short_len < params.ema_long_len and \\\n",
        "                                           params.atr_multiplier_sl > 0 and \\\n",
        "                                           params.atr_multiplier_tp > params.atr_multiplier_sl, # More constraints\n",
        "                max_tries=200 # Adjust as needed. If combinations are few, grid search is used.\n",
        "                              # If many, random search up to max_tries.\n",
        "            )\n",
        "\n",
        "            print(\"\\n--- Optimization Results (Best Strategy Stats) ---\")\n",
        "            print(optimization_stats)\n",
        "\n",
        "            print(\"\\n--- Best Strategy Parameters Found ---\")\n",
        "            best_params = optimization_stats._strategy # This is an instance of the strategy with best params\n",
        "            print(f\"  EMA Short Length: {best_params.ema_short_len}\")\n",
        "            print(f\"  EMA Long Length: {best_params.ema_long_len}\")\n",
        "            print(f\"  RSI Length: {best_params.rsi_len}\")\n",
        "            print(f\"  RSI Buy Threshold: {best_params.rsi_buy_threshold}\")\n",
        "            print(f\"  ATR Length: {best_params.atr_len}\")\n",
        "            print(f\"  ATR SL Multiplier: {best_params.atr_multiplier_sl}\")\n",
        "            print(f\"  ATR TP Multiplier: {best_params.atr_multiplier_tp}\")\n",
        "            print(f\"  Use Trend Filter: {best_params.use_trend_filter}\")\n",
        "            print(f\"  Long SMA Trend Length: {best_params.long_sma_trend_len}\")\n",
        "\n",
        "            # Plotting the best strategy\n",
        "            plot_filename = os.path.join(OUTPUT_OPTIMIZATION_PATH, f\"{PAIR_SYMBOL}_{TIMEFRAME}_optimized_plot.html\")\n",
        "            try:\n",
        "                bt.plot(filename=plot_filename, open_browser=False, superimpose=True, plot_equity=True, plot_drawdown=True)\n",
        "                print(f\"Plot for BEST optimized strategy saved to: {plot_filename}\")\n",
        "            except Exception as plot_error:\n",
        "                print(f\"Could not generate plot for optimized strategy: {plot_error}.\")\n",
        "\n",
        "    except ValueError as ve:\n",
        "        print(f\"ValueError during setup or optimization: {ve}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "print(\"\\n--- Optimization script with backtesting.py complete ---\")"
      ],
      "metadata": {
        "id": "kijfukxv0ILs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}